{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "g9kxq01msMg9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\alex\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (0.17.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (3.3.2)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (1.19.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (50.3.1.post20201107)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\alex\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alex\\anaconda3\\lib\\site-packages (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from h5py) (1.19.2)\n",
      "Requirement already satisfied: six in c:\\users\\alex\\anaconda3\\lib\\site-packages (from h5py) (1.15.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\alex\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (0.35.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (3.14.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (3.3.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (50.3.1.post20201107)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (0.11.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (0.4.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.32.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.19.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.26.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\alex\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.26.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "### BEWARE:Tensorflow is stochastic - this means the model will not be replicated exactly. \n",
    "### Use GA_Load_Model for reproduction\n",
    "\n",
    "!pip install mlxtend\n",
    "\n",
    "!pip install h5py pyyaml\n",
    "\n",
    "!pip install tensorboard\n",
    "\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tLftnb5sBe7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "#Load packages\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iBsKz40OAGDU"
   },
   "outputs": [],
   "source": [
    "### Packages necessary for model construction \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks\n",
    "import datetime \n",
    "import statistics\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FrEcBaVWVVkb"
   },
   "outputs": [],
   "source": [
    "#Read the Data\n",
    "\n",
    "UN_Data = pd.read_csv('GA_Query_CleanLIWC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 4369,
     "status": "ok",
     "timestamp": 1611639237251,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "FNUqnWTFS4y2",
    "outputId": "cf54ea17-a224-4830-8f6b-554ee9bafaf0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Policy Passed</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20075.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>822.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17021.0</td>\n",
       "      <td>98.45</td>\n",
       "      <td>...</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9289.0</td>\n",
       "      <td>98.94</td>\n",
       "      <td>...</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4059.0</td>\n",
       "      <td>98.95</td>\n",
       "      <td>...</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8210.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10210</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>98.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10211</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22700.0</td>\n",
       "      <td>98.26</td>\n",
       "      <td>...</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10212 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  Class M  Class S  Class I  Class P  Class B  Policy Passed  \\\n",
       "0      2012        1        0        0        0        0              0   \n",
       "1      2012        0        0        3        0        0              0   \n",
       "2      2003        0        0        0        0        0              0   \n",
       "3      1995        0        0        0        0        0              1   \n",
       "4      2007        0        0        0        0        0              0   \n",
       "...     ...      ...      ...      ...      ...      ...            ...   \n",
       "10207  2004        0        0        0        0        0              0   \n",
       "10208  1994        0        0        0        0        0              0   \n",
       "10209  2013        0        0        0        0        0              0   \n",
       "10210  2009        0        0        0        0        0              0   \n",
       "10211  2016        0        0        0        0        0              0   \n",
       "\n",
       "       Conflict Indicator       WC  Analytic  ...  Comma  Colon  SemiC  QMark  \\\n",
       "0                       1  20075.0     99.00  ...   4.34   0.03   0.04   0.00   \n",
       "1                       0    822.0     99.00  ...   3.04   1.70   0.00   0.00   \n",
       "2                       0    314.0     99.00  ...   3.50   0.96   0.00   0.00   \n",
       "3                       1  17021.0     98.45  ...   4.91   0.25   0.17   0.02   \n",
       "4                       0   9289.0     98.94  ...   3.80   0.16   0.15   0.00   \n",
       "...                   ...      ...       ...  ...    ...    ...    ...    ...   \n",
       "10207                   0   4059.0     98.95  ...   3.72   0.15   0.12   0.00   \n",
       "10208                   0   8210.0     99.00  ...   3.58   0.12   0.22   0.00   \n",
       "10209                   0    583.0     99.00  ...   3.09   0.86   0.00   0.69   \n",
       "10210                   0   1562.0     98.88  ...   2.82   0.19   0.45   0.00   \n",
       "10211                   0  22700.0     98.26  ...   3.69   0.04   0.03   0.04   \n",
       "\n",
       "       Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "0         0.0  1.23   0.07     0.64     0.82    0.60  \n",
       "1         0.0  0.85   0.49     0.12     4.38    1.46  \n",
       "2         0.0  2.23   0.00     0.64     3.18    2.87  \n",
       "3         0.0  1.33   0.22     0.16     0.64    2.18  \n",
       "4         0.0  0.93   0.28     0.75     1.42    1.52  \n",
       "...       ...   ...    ...      ...      ...     ...  \n",
       "10207     0.0  1.18   0.00     0.76     1.72    1.23  \n",
       "10208     0.0  1.06   0.02     0.29     1.05    1.75  \n",
       "10209     0.0  1.89   0.34     0.00     4.80    4.63  \n",
       "10210     0.0  1.34   0.00     0.77     1.66    2.18  \n",
       "10211     0.0  1.44   0.10     0.48     1.01    1.58  \n",
       "\n",
       "[10212 rows x 101 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect and Clean the Data\n",
    "\n",
    "UN_Data.head(5)\n",
    "\n",
    "UN_Data.drop(['Unnamed: 0'], axis = 1, inplace= True)\n",
    "\n",
    "UN_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.581151832460733, 1: 3.5806451612903225}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "#Balance Policy Passage\n",
    "\n",
    "# Count samples per class\n",
    "classes_zero = UN_Data[UN_Data['Policy Passed'] == 0]\n",
    "classes_one = UN_Data[UN_Data['Policy Passed'] == 1]\n",
    "\n",
    "# Convert parts into NumPy arrays for weight computation\n",
    "zero_numpy = classes_zero['Policy Passed'].to_numpy()\n",
    "one_numpy = classes_one['Policy Passed'].to_numpy()\n",
    "all_together = np.concatenate((zero_numpy, one_numpy))\n",
    "unique_classes = np.unique(all_together)\n",
    "\n",
    "# Compute weights\n",
    "weights = sklearn.utils.class_weight.compute_class_weight('balanced', unique_classes, all_together)\n",
    "weights = dict(enumerate(weights))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 4368,
     "status": "ok",
     "timestamp": 1611639237252,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "tkn1c4RqeEdO",
    "outputId": "d56ecbce-990a-427d-e279-772f8546aeac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Policy Passed</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.00000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2005.852135</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>0.009205</td>\n",
       "      <td>0.168625</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.13964</td>\n",
       "      <td>0.457697</td>\n",
       "      <td>9442.204220</td>\n",
       "      <td>98.190282</td>\n",
       "      <td>...</td>\n",
       "      <td>5.193233</td>\n",
       "      <td>0.303105</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>1.154469</td>\n",
       "      <td>0.260464</td>\n",
       "      <td>0.397916</td>\n",
       "      <td>1.744508</td>\n",
       "      <td>1.768528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.542111</td>\n",
       "      <td>0.229542</td>\n",
       "      <td>0.099521</td>\n",
       "      <td>0.861341</td>\n",
       "      <td>0.307019</td>\n",
       "      <td>0.156540</td>\n",
       "      <td>0.34663</td>\n",
       "      <td>0.498232</td>\n",
       "      <td>7786.325195</td>\n",
       "      <td>1.085791</td>\n",
       "      <td>...</td>\n",
       "      <td>3.290078</td>\n",
       "      <td>0.466117</td>\n",
       "      <td>0.190811</td>\n",
       "      <td>0.105541</td>\n",
       "      <td>0.713727</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>0.439789</td>\n",
       "      <td>1.273977</td>\n",
       "      <td>1.466890</td>\n",
       "      <td>4.337472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1993.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3838.500000</td>\n",
       "      <td>97.880000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2006.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7439.500000</td>\n",
       "      <td>98.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2012.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12438.750000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>2.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>74776.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>47.760000</td>\n",
       "      <td>22.710000</td>\n",
       "      <td>11.150000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>49.920000</td>\n",
       "      <td>22.880000</td>\n",
       "      <td>14.030000</td>\n",
       "      <td>89.620000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>213.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       Class M       Class S       Class I       Class P  \\\n",
       "count  10212.000000  10212.000000  10212.000000  10212.000000  10212.000000   \n",
       "mean    2005.852135      0.032805      0.009205      0.168625      0.051900   \n",
       "std        7.542111      0.229542      0.099521      0.861341      0.307019   \n",
       "min     1993.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     1999.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     2006.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%     2012.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max     2020.000000      9.000000      2.000000     28.000000      9.000000   \n",
       "\n",
       "            Class B  Policy Passed  Conflict Indicator            WC  \\\n",
       "count  10212.000000    10212.00000        10212.000000  10190.000000   \n",
       "mean       0.021641        0.13964            0.457697   9442.204220   \n",
       "std        0.156540        0.34663            0.498232   7786.325195   \n",
       "min        0.000000        0.00000            0.000000     44.000000   \n",
       "25%        0.000000        0.00000            0.000000   3838.500000   \n",
       "50%        0.000000        0.00000            0.000000   7439.500000   \n",
       "75%        0.000000        0.00000            1.000000  12438.750000   \n",
       "max        3.000000        1.00000            1.000000  74776.000000   \n",
       "\n",
       "           Analytic  ...         Comma         Colon         SemiC  \\\n",
       "count  10190.000000  ...  10190.000000  10190.000000  10190.000000   \n",
       "mean      98.190282  ...      5.193233      0.303105      0.144628   \n",
       "std        1.085791  ...      3.290078      0.466117      0.190811   \n",
       "min       80.460000  ...      0.210000      0.000000      0.000000   \n",
       "25%       97.880000  ...      3.980000      0.090000      0.060000   \n",
       "50%       98.440000  ...      4.440000      0.160000      0.110000   \n",
       "75%       99.000000  ...      4.990000      0.320000      0.190000   \n",
       "max       99.000000  ...     47.760000     22.710000     11.150000   \n",
       "\n",
       "              QMark        Exclam          Dash         Quote       Apostro  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.031364      0.014043      1.154469      0.260464      0.397916   \n",
       "std        0.105541      0.713727      0.643019      0.439789      1.273977   \n",
       "min        0.000000      0.000000      0.020000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.800000      0.050000      0.230000   \n",
       "50%        0.000000      0.000000      1.040000      0.130000      0.340000   \n",
       "75%        0.020000      0.000000      1.380000      0.300000      0.490000   \n",
       "max        3.400000     49.920000     22.880000     14.030000     89.620000   \n",
       "\n",
       "            Parenth        OtherP  \n",
       "count  10190.000000  10190.000000  \n",
       "mean       1.744508      1.768528  \n",
       "std        1.466890      4.337472  \n",
       "min        0.130000      0.010000  \n",
       "25%        0.860000      0.740000  \n",
       "50%        1.330000      1.170000  \n",
       "75%        2.090000      2.070000  \n",
       "max       17.900000    213.040000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect the data by key descriptive statistics\n",
    "\n",
    "UN_Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 4367,
     "status": "ok",
     "timestamp": 1611639237253,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "1c56fLBjJKA3",
    "outputId": "8b5f5028-9f16-4543-a585-e014b90e8ef7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Passed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>...</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>...</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  Class M  Class S  Class I  Class P  Class B  \\\n",
       "Policy Passed                                                      \n",
       "0              8786     8786     8786     8786     8786     8786   \n",
       "1              1426     1426     1426     1426     1426     1426   \n",
       "\n",
       "               Conflict Indicator    WC  Analytic  Clout  ...  Comma  Colon  \\\n",
       "Policy Passed                                             ...                 \n",
       "0                            8786  8764      8764   8764  ...   8764   8764   \n",
       "1                            1426  1426      1426   1426  ...   1426   1426   \n",
       "\n",
       "               SemiC  QMark  Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "Policy Passed                                                               \n",
       "0               8764   8764    8764  8764   8764     8764     8764    8764  \n",
       "1               1426   1426    1426  1426   1426     1426     1426    1426  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group the data by our label (dependent variable) of policy passage\n",
    "\n",
    "UN_Data.groupby(['Policy Passed']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Policy Passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>1.019000e+04</td>\n",
       "      <td>10212.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.345714</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.881989</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>4.345803e-04</td>\n",
       "      <td>0.13964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.252698</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.193529</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>1.748792e-03</td>\n",
       "      <td>0.34663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.026924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.473887e-07</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.159519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885374</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>6.634241e-05</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.260213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965297</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1.553934e-04</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.463364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.987100</td>\n",
       "      <td>0.022704</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>3.987904e-04</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997075</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.049344</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>9.570280e-02</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       Class M       Class S       Class I       Class P  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.345714      0.000004      0.000001      0.000021      0.000007   \n",
       "std        0.252698      0.000030      0.000017      0.000110      0.000050   \n",
       "min        0.026924      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.159519      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.260213      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.463364      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.997075      0.001033      0.000412      0.002388      0.001449   \n",
       "\n",
       "            Class B  Conflict Indicator            WC      Analytic  \\\n",
       "count  10190.000000        10190.000000  10190.000000  10190.000000   \n",
       "mean       0.000002            0.000056      0.881989      0.016961   \n",
       "std        0.000021            0.000086      0.193529      0.012448   \n",
       "min        0.000000            0.000000      0.021901      0.001303   \n",
       "25%        0.000000            0.000000      0.885374      0.007792   \n",
       "50%        0.000000            0.000000      0.965297      0.012765   \n",
       "75%        0.000000            0.000094      0.987100      0.022704   \n",
       "max        0.000514            0.000492      0.999635      0.049344   \n",
       "\n",
       "              Clout  ...         Colon         SemiC         QMark  \\\n",
       "count  10190.000000  ...  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.011428  ...      0.000081      0.000027      0.000006   \n",
       "std        0.008500  ...      0.000179      0.000070      0.000029   \n",
       "min        0.001053  ...      0.000000      0.000000      0.000000   \n",
       "25%        0.005150  ...      0.000008      0.000004      0.000000   \n",
       "50%        0.008537  ...      0.000020      0.000012      0.000000   \n",
       "75%        0.015068  ...      0.000066      0.000029      0.000001   \n",
       "max        0.040877  ...      0.005398      0.004388      0.001033   \n",
       "\n",
       "             Exclam          Dash         Quote       Apostro       Parenth  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.000003      0.000222      0.000051      0.000066      0.000417   \n",
       "std        0.000154      0.000308      0.000150      0.000281      0.000665   \n",
       "min        0.000000      0.000003      0.000000      0.000000      0.000003   \n",
       "25%        0.000000      0.000069      0.000004      0.000020      0.000073   \n",
       "50%        0.000000      0.000129      0.000013      0.000040      0.000180   \n",
       "75%        0.000000      0.000252      0.000041      0.000077      0.000431   \n",
       "max        0.010615      0.010232      0.006303      0.019591      0.008041   \n",
       "\n",
       "             OtherP  Policy Passed  \n",
       "count  1.019000e+04    10212.00000  \n",
       "mean   4.345803e-04        0.13964  \n",
       "std    1.748792e-03        0.34663  \n",
       "min    4.473887e-07        0.00000  \n",
       "25%    6.634241e-05        0.00000  \n",
       "50%    1.553934e-04        0.00000  \n",
       "75%    3.987904e-04        0.00000  \n",
       "max    9.570280e-02        1.00000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize the data \n",
    "\n",
    "UN_Data1 = tf.keras.utils.normalize(UN_Data.drop(columns = ['Policy Passed']))\n",
    "\n",
    "UN_Data1[\"Policy Passed\"] = UN_Data['Policy Passed']\n",
    "\n",
    "UN_Data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "r6BF8hmXGEnF"
   },
   "outputs": [],
   "source": [
    "#Divide our variables between the independent variables (features) and dependent variables (policy passage)\n",
    "\n",
    "labels = UN_Data1 ['Policy Passed']\n",
    "features = UN_Data1.drop(columns= ['Policy Passed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Null Values\n",
    "\n",
    "features = features.fillna(0)\n",
    "labels = labels.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1611639237254,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "4FJRWpUNH8Dk",
    "outputId": "fe514d19-f576-4030-c91f-fc2b562d2213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10212, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect shape of features\n",
    "\n",
    "features = pd.get_dummies(features)\n",
    "features.shape[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-oGgPi4CG0xx"
   },
   "outputs": [],
   "source": [
    "#Define type of feature and label values\n",
    "\n",
    "features = features.values.astype('float32')\n",
    "labels = labels.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XOea_aP_HPSE"
   },
   "outputs": [],
   "source": [
    "#Data Sets for Training\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)\n",
    "features_train, features_validation, labels_train, labels_validation = train_test_split(features_train, labels_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vHSYOIsh5vq4"
   },
   "outputs": [],
   "source": [
    "#Define Precision, Recall, and F1 score metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    weights = sklearn.utils.class_weight.compute_class_weight('balanced', unique_classes, all_together)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4VQ_OqkCHkLM"
   },
   "outputs": [],
   "source": [
    "#Create your model\n",
    "\n",
    "model1 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model2 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model3 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model4 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model5 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model6 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model7 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model8 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model9 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model10 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model11 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model12 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model13 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model14 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model15 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model16 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model17 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model18 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model19 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model20 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model21 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model22 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model23 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model24 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model25 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model26 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model27 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model28 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model29 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model30 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1611639239019,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "7JTlq8aH8mzi",
    "outputId": "a7831e8c-a787-4b03-8006-c9aebd129183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "### Inspect form of model\n",
    "\n",
    "tf.keras.utils.plot_model(model1, to_file='model.png', show_shapes = True, show_dtype=True, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), \n",
    "', 'for `pydotprint` to work.')\n",
    "\n",
    "'''\n",
    "\n",
    "# Make sure to have Pydot install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1611639239020,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "52Vb8b-qSIQ4",
    "outputId": "dd0ce729-358b-43ac-8d65-6c1f00ba812a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 3,794\n",
      "Trainable params: 3,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Check Trainable Parameters\n",
    "# Note: All the models are similarly structured\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Vyx2tehuTPfe"
   },
   "outputs": [],
   "source": [
    "#Set checkpoints, metrics, loss, and optimizer functions for the model\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model1.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model3.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model4.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model5.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model6.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model7.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model8.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model9.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model10.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model11.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model12.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model13.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model14.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model15.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model16.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model17.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model18.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model19.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model20.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model21.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model22.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model23.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model24.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model25.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model26.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model27.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model28.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model29.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model30.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 150145,
     "status": "error",
     "timestamp": 1611639383055,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "RhwA7DthFS72",
    "outputId": "64a70fe2-c811-4102-e25e-f708c8ee64e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Fitting\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:390: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 7s 10ms/step - loss: 0.6994 - acc: 0.5906 - precision: 0.1234 - recall: 0.2578 - f1_metric: 0.1572 - val_loss: 0.6998 - val_acc: 0.3121 - val_precision: 0.1496 - val_recall: 0.6998 - val_f1_metric: 0.2384\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6902 - acc: 0.4188 - precision: 0.1415 - recall: 1.0773 - f1_metric: 0.2429 - val_loss: 0.6765 - val_acc: 0.4284 - val_precision: 0.1477 - val_recall: 1.3237 - val_f1_metric: 0.2596\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.6915 - acc: 0.4178 - precision: 0.1433 - recall: 1.1959 - f1_metric: 0.2514 - val_loss: 0.6717 - val_acc: 0.4253 - val_precision: 0.1475 - val_recall: 1.1532 - val_f1_metric: 0.2548\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.6786 - acc: 0.4262 - precision: 0.1369 - recall: 1.1963 - f1_metric: 0.2410 - val_loss: 0.6940 - val_acc: 0.3709 - val_precision: 0.1474 - val_recall: 1.1701 - val_f1_metric: 0.2559\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6826 - acc: 0.4240 - precision: 0.1396 - recall: 1.2287 - f1_metric: 0.2473 - val_loss: 0.6758 - val_acc: 0.4406 - val_precision: 0.1460 - val_recall: 1.3138 - val_f1_metric: 0.2567\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.6955 - acc: 0.3932 - precision: 0.1436 - recall: 1.1075 - f1_metric: 0.2489 - val_loss: 0.6725 - val_acc: 0.4565 - val_precision: 0.1469 - val_recall: 1.0019 - val_f1_metric: 0.2498\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6864 - acc: 0.4108 - precision: 0.1361 - recall: 0.9807 - f1_metric: 0.2344 - val_loss: 0.7061 - val_acc: 0.3348 - val_precision: 0.1515 - val_recall: 0.8708 - val_f1_metric: 0.2506\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6772 - acc: 0.3459 - precision: 0.1370 - recall: 0.8784 - f1_metric: 0.2325 - val_loss: 0.6912 - val_acc: 0.3874 - val_precision: 0.1555 - val_recall: 0.8389 - val_f1_metric: 0.2545\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6932 - acc: 0.3525 - precision: 0.1501 - recall: 0.8656 - f1_metric: 0.2489 - val_loss: 0.6764 - val_acc: 0.4339 - val_precision: 0.1482 - val_recall: 0.7002 - val_f1_metric: 0.2367\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.6741 - acc: 0.4769 - precision: 0.1377 - recall: 0.7273 - f1_metric: 0.2249 - val_loss: 0.7187 - val_acc: 0.3433 - val_precision: 0.1511 - val_recall: 0.8600 - val_f1_metric: 0.2496\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6799 - acc: 0.3844 - precision: 0.1368 - recall: 0.7773 - f1_metric: 0.2255 - val_loss: 0.6816 - val_acc: 0.4315 - val_precision: 0.1493 - val_recall: 0.7303 - val_f1_metric: 0.2399\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6816 - acc: 0.4652 - precision: 0.1326 - recall: 0.6871 - f1_metric: 0.2161 - val_loss: 0.6831 - val_acc: 0.4229 - val_precision: 0.1431 - val_recall: 0.9056 - val_f1_metric: 0.2405\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.7009 - acc: 0.4251 - precision: 0.1543 - recall: 0.9710 - f1_metric: 0.2585 - val_loss: 0.6768 - val_acc: 0.4468 - val_precision: 0.1451 - val_recall: 1.0559 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6918 - acc: 0.4554 - precision: 0.1408 - recall: 1.0581 - f1_metric: 0.2440 - val_loss: 0.7043 - val_acc: 0.3800 - val_precision: 0.1454 - val_recall: 1.0223 - val_f1_metric: 0.2486\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6931 - acc: 0.3906 - precision: 0.1437 - recall: 1.0461 - f1_metric: 0.2480 - val_loss: 0.6551 - val_acc: 0.5692 - val_precision: 0.1464 - val_recall: 1.3212 - val_f1_metric: 0.2575\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.7012 - acc: 0.4975 - precision: 0.1504 - recall: 1.1185 - f1_metric: 0.2598 - val_loss: 0.6597 - val_acc: 0.5734 - val_precision: 0.1457 - val_recall: 1.2346 - val_f1_metric: 0.2545\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6773 - acc: 0.5497 - precision: 0.1388 - recall: 1.1214 - f1_metric: 0.2428 - val_loss: 0.7009 - val_acc: 0.3984 - val_precision: 0.1482 - val_recall: 0.9492 - val_f1_metric: 0.2497\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6730 - acc: 0.4931 - precision: 0.1369 - recall: 0.9231 - f1_metric: 0.2333 - val_loss: 0.7053 - val_acc: 0.3837 - val_precision: 0.1536 - val_recall: 0.8581 - val_f1_metric: 0.2528\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.6764 - acc: 0.4540 - precision: 0.1400 - recall: 0.7918 - f1_metric: 0.2326 - val_loss: 0.6717 - val_acc: 0.4951 - val_precision: 0.1537 - val_recall: 0.6590 - val_f1_metric: 0.2411\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.6678 - acc: 0.5336 - precision: 0.1353 - recall: 0.6557 - f1_metric: 0.2161 - val_loss: 0.6740 - val_acc: 0.4853 - val_precision: 0.1531 - val_recall: 0.7176 - val_f1_metric: 0.2441\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6837 - acc: 0.5110 - precision: 0.1466 - recall: 0.7937 - f1_metric: 0.2407 - val_loss: 0.7048 - val_acc: 0.4045 - val_precision: 0.1550 - val_recall: 0.8438 - val_f1_metric: 0.2537\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.6846 - acc: 0.4619 - precision: 0.1432 - recall: 0.8207 - f1_metric: 0.2384 - val_loss: 0.6840 - val_acc: 0.4627 - val_precision: 0.1515 - val_recall: 0.7913 - val_f1_metric: 0.2465\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6912 - acc: 0.4717 - precision: 0.1505 - recall: 0.8595 - f1_metric: 0.2507 - val_loss: 0.6961 - val_acc: 0.4266 - val_precision: 0.1466 - val_recall: 0.9367 - val_f1_metric: 0.2467\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6808 - acc: 0.4750 - precision: 0.1396 - recall: 0.9406 - f1_metric: 0.2384 - val_loss: 0.6719 - val_acc: 0.5037 - val_precision: 0.1461 - val_recall: 0.9349 - val_f1_metric: 0.2459\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.7013 - acc: 0.4515 - precision: 0.1524 - recall: 0.9527 - f1_metric: 0.2571 - val_loss: 0.5789 - val_acc: 0.8531 - val_precision: 0.1477 - val_recall: 0.9931 - val_f1_metric: 0.2508\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6801 - acc: 0.5708 - precision: 0.1452 - recall: 0.9093 - f1_metric: 0.2454 - val_loss: 0.6728 - val_acc: 0.5031 - val_precision: 0.1528 - val_recall: 0.8825 - val_f1_metric: 0.2529\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6751 - acc: 0.5628 - precision: 0.1446 - recall: 0.8936 - f1_metric: 0.2446 - val_loss: 0.7063 - val_acc: 0.4119 - val_precision: 0.1524 - val_recall: 0.9142 - val_f1_metric: 0.2539\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6889 - acc: 0.4827 - precision: 0.1488 - recall: 0.9490 - f1_metric: 0.2514 - val_loss: 0.6619 - val_acc: 0.5312 - val_precision: 0.1438 - val_recall: 0.9748 - val_f1_metric: 0.2445\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6739 - acc: 0.5492 - precision: 0.1450 - recall: 0.9678 - f1_metric: 0.2477 - val_loss: 0.6662 - val_acc: 0.5202 - val_precision: 0.1571 - val_recall: 0.8575 - val_f1_metric: 0.2577\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6742 - acc: 0.5374 - precision: 0.1458 - recall: 0.9112 - f1_metric: 0.2446 - val_loss: 0.6125 - val_acc: 0.7326 - val_precision: 0.1421 - val_recall: 0.9222 - val_f1_metric: 0.2398\n",
      "Model 2 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.5240 - acc: 0.7560 - precision: 0.1383 - recall: 1.1139 - f1_metric: 0.2401 - val_loss: 0.4212 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4156 - acc: 0.8589 - precision: 0.1408 - recall: 0.9951 - f1_metric: 0.2415 - val_loss: 0.4179 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4023 - acc: 0.8617 - precision: 0.1383 - recall: 0.9895 - f1_metric: 0.2388 - val_loss: 0.4184 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3996 - acc: 0.8635 - precision: 0.1365 - recall: 1.0000 - f1_metric: 0.2344 - val_loss: 0.4191 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4225 - acc: 0.8512 - precision: 0.1488 - recall: 0.9845 - f1_metric: 0.2535 - val_loss: 0.4233 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4020 - acc: 0.8620 - precision: 0.1380 - recall: 0.9669 - f1_metric: 0.2381 - val_loss: 0.4183 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4007 - acc: 0.8622 - precision: 0.1378 - recall: 0.9645 - f1_metric: 0.2368 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4072 - acc: 0.8586 - precision: 0.1414 - recall: 0.9987 - f1_metric: 0.2431 - val_loss: 0.4172 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3936 - acc: 0.8659 - precision: 0.1341 - recall: 1.0000 - f1_metric: 0.2317 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3968 - acc: 0.8639 - precision: 0.1361 - recall: 0.9859 - f1_metric: 0.2342 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3981 - acc: 0.8632 - precision: 0.1368 - recall: 1.0000 - f1_metric: 0.2358 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4060 - acc: 0.8598 - precision: 0.1402 - recall: 1.0000 - f1_metric: 0.2403 - val_loss: 0.4153 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4018 - acc: 0.8603 - precision: 0.1397 - recall: 0.9887 - f1_metric: 0.2396 - val_loss: 0.4149 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3999 - acc: 0.8603 - precision: 0.1397 - recall: 0.9945 - f1_metric: 0.2403 - val_loss: 0.4141 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3970 - acc: 0.8621 - precision: 0.1379 - recall: 0.9972 - f1_metric: 0.2385 - val_loss: 0.4138 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3965 - acc: 0.8624 - precision: 0.1376 - recall: 0.9904 - f1_metric: 0.2373 - val_loss: 0.4129 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4178 - acc: 0.8494 - precision: 0.1506 - recall: 1.0000 - f1_metric: 0.2571 - val_loss: 0.4132 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3910 - acc: 0.8655 - precision: 0.1345 - recall: 0.9979 - f1_metric: 0.2314 - val_loss: 0.4118 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4066 - acc: 0.8552 - precision: 0.1448 - recall: 1.0000 - f1_metric: 0.2480 - val_loss: 0.4114 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4000 - acc: 0.8587 - precision: 0.1413 - recall: 0.9825 - f1_metric: 0.2428 - val_loss: 0.4112 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3909 - acc: 0.8648 - precision: 0.1352 - recall: 0.9995 - f1_metric: 0.2335 - val_loss: 0.4098 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4096 - acc: 0.8535 - precision: 0.1465 - recall: 0.9916 - f1_metric: 0.2501 - val_loss: 0.4084 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4037 - acc: 0.8556 - precision: 0.1444 - recall: 0.9874 - f1_metric: 0.2458 - val_loss: 0.4084 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3929 - acc: 0.8626 - precision: 0.1374 - recall: 0.9959 - f1_metric: 0.2370 - val_loss: 0.4039 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3944 - acc: 0.8591 - precision: 0.1408 - recall: 1.0000 - f1_metric: 0.2423 - val_loss: 0.4065 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3864 - acc: 0.8623 - precision: 0.1376 - recall: 0.9935 - f1_metric: 0.2374 - val_loss: 0.4005 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3886 - acc: 0.8597 - precision: 0.1403 - recall: 0.9957 - f1_metric: 0.2413 - val_loss: 0.4022 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3866 - acc: 0.8607 - precision: 0.1393 - recall: 0.9977 - f1_metric: 0.2395 - val_loss: 0.3943 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3785 - acc: 0.8623 - precision: 0.1377 - recall: 0.9936 - f1_metric: 0.2373 - val_loss: 0.3938 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3781 - acc: 0.8629 - precision: 0.1372 - recall: 0.9873 - f1_metric: 0.2358 - val_loss: 0.3880 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 3 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 13ms/step - loss: 0.5251 - acc: 0.7457 - precision: 0.1509 - recall: 1.0379 - f1_metric: 0.2577 - val_loss: 0.4212 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4111 - acc: 0.8628 - precision: 0.1372 - recall: 1.0000 - f1_metric: 0.2363 - val_loss: 0.4180 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3972 - acc: 0.8649 - precision: 0.1351 - recall: 0.9811 - f1_metric: 0.2336 - val_loss: 0.4176 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4039 - acc: 0.8623 - precision: 0.1377 - recall: 0.9852 - f1_metric: 0.2376 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4114 - acc: 0.8584 - precision: 0.1416 - recall: 0.9870 - f1_metric: 0.2439 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4051 - acc: 0.8598 - precision: 0.1401 - recall: 0.9830 - f1_metric: 0.2397 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4096 - acc: 0.8574 - precision: 0.1426 - recall: 0.9981 - f1_metric: 0.2442 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3987 - acc: 0.8661 - precision: 0.1339 - recall: 0.9806 - f1_metric: 0.2312 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4105 - acc: 0.8586 - precision: 0.1414 - recall: 0.9903 - f1_metric: 0.2440 - val_loss: 0.4165 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3997 - acc: 0.8625 - precision: 0.1375 - recall: 0.9983 - f1_metric: 0.2378 - val_loss: 0.4195 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4018 - acc: 0.8621 - precision: 0.1379 - recall: 0.9902 - f1_metric: 0.2382 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4114 - acc: 0.8559 - precision: 0.1441 - recall: 1.0000 - f1_metric: 0.2481 - val_loss: 0.4192 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8615 - precision: 0.1385 - recall: 0.9971 - f1_metric: 0.2380 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4123 - acc: 0.8551 - precision: 0.1449 - recall: 0.9936 - f1_metric: 0.2479 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3894 - acc: 0.8678 - precision: 0.1322 - recall: 0.9714 - f1_metric: 0.2278 - val_loss: 0.4154 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4111 - acc: 0.8567 - precision: 0.1433 - recall: 0.9933 - f1_metric: 0.2455 - val_loss: 0.4128 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8644 - precision: 0.1356 - recall: 0.9976 - f1_metric: 0.2346 - val_loss: 0.4141 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4050 - acc: 0.8568 - precision: 0.1432 - recall: 0.9967 - f1_metric: 0.2460 - val_loss: 0.4119 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4052 - acc: 0.8567 - precision: 0.1433 - recall: 0.9982 - f1_metric: 0.2463 - val_loss: 0.4114 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3925 - acc: 0.8628 - precision: 0.1372 - recall: 0.9896 - f1_metric: 0.2375 - val_loss: 0.4110 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3990 - acc: 0.8586 - precision: 0.1414 - recall: 0.9979 - f1_metric: 0.2432 - val_loss: 0.4111 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3961 - acc: 0.8606 - precision: 0.1394 - recall: 0.9885 - f1_metric: 0.2397 - val_loss: 0.4088 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8616 - precision: 0.1384 - recall: 0.9940 - f1_metric: 0.2371 - val_loss: 0.4099 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8624 - precision: 0.1376 - recall: 0.9927 - f1_metric: 0.2374 - val_loss: 0.4065 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8571 - precision: 0.1429 - recall: 1.0000 - f1_metric: 0.2452 - val_loss: 0.4054 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4048 - acc: 0.8535 - precision: 0.1465 - recall: 1.0000 - f1_metric: 0.2507 - val_loss: 0.4039 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3803 - acc: 0.8684 - precision: 0.1316 - recall: 0.9884 - f1_metric: 0.2278 - val_loss: 0.4014 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3861 - acc: 0.8636 - precision: 0.1364 - recall: 0.9880 - f1_metric: 0.2358 - val_loss: 0.3990 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3889 - acc: 0.8587 - precision: 0.1413 - recall: 1.0000 - f1_metric: 0.2430 - val_loss: 0.3994 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8583 - precision: 0.1417 - recall: 0.9835 - f1_metric: 0.2429 - val_loss: 0.3951 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 4 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.5227 - acc: 0.7715 - precision: 0.1242 - recall: 0.7109 - f1_metric: 0.1984 - val_loss: 0.4177 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4078 - acc: 0.8631 - precision: 0.1369 - recall: 0.9800 - f1_metric: 0.2359 - val_loss: 0.4181 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4222 - acc: 0.8532 - precision: 0.1471 - recall: 0.9917 - f1_metric: 0.2516 - val_loss: 0.4176 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4041 - acc: 0.8622 - precision: 0.1376 - recall: 0.9871 - f1_metric: 0.2374 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3973 - acc: 0.8664 - precision: 0.1337 - recall: 0.9945 - f1_metric: 0.2308 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3994 - acc: 0.8656 - precision: 0.1338 - recall: 0.9958 - f1_metric: 0.2319 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4166 - acc: 0.8543 - precision: 0.1458 - recall: 0.9913 - f1_metric: 0.2500 - val_loss: 0.4193 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4107 - acc: 0.8579 - precision: 0.1423 - recall: 1.0000 - f1_metric: 0.2444 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4127 - acc: 0.8540 - precision: 0.1460 - recall: 0.9979 - f1_metric: 0.2509 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4126 - acc: 0.8548 - precision: 0.1452 - recall: 0.9931 - f1_metric: 0.24 - 1s 3ms/step - loss: 0.4125 - acc: 0.8549 - precision: 0.1451 - recall: 0.9932 - f1_metric: 0.2483 - val_loss: 0.4155 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4030 - acc: 0.8606 - precision: 0.1394 - recall: 0.9902 - f1_metric: 0.2401 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4093 - acc: 0.8585 - precision: 0.1415 - recall: 0.9800 - f1_metric: 0.2428 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3965 - acc: 0.8640 - precision: 0.1360 - recall: 1.0000 - f1_metric: 0.2345 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8619 - precision: 0.1381 - recall: 0.9816 - f1_metric: 0.2388 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8607 - precision: 0.1393 - recall: 0.9742 - f1_metric: 0.2386 - val_loss: 0.4140 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3895 - acc: 0.8664 - precision: 0.1336 - recall: 0.9992 - f1_metric: 0.2318 - val_loss: 0.4122 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3876 - acc: 0.8659 - precision: 0.1340 - recall: 0.9962 - f1_metric: 0.2319 - val_loss: 0.4118 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3967 - acc: 0.8614 - precision: 0.1386 - recall: 0.9967 - f1_metric: 0.2384 - val_loss: 0.4108 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4112 - acc: 0.8514 - precision: 0.1486 - recall: 0.9924 - f1_metric: 0.2541 - val_loss: 0.4130 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4017 - acc: 0.8574 - precision: 0.1426 - recall: 0.9949 - f1_metric: 0.2432 - val_loss: 0.4100 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3843 - acc: 0.8694 - precision: 0.1306 - recall: 0.9686 - f1_metric: 0.2252 - val_loss: 0.4083 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3991 - acc: 0.8575 - precision: 0.1426 - recall: 1.0000 - f1_metric: 0.2447 - val_loss: 0.4079 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3903 - acc: 0.8622 - precision: 0.1378 - recall: 1.0000 - f1_metric: 0.2377 - val_loss: 0.4055 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3983 - acc: 0.8591 - precision: 0.1409 - recall: 0.9970 - f1_metric: 0.2415 - val_loss: 0.4031 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3807 - acc: 0.8661 - precision: 0.1339 - recall: 0.9694 - f1_metric: 0.2311 - val_loss: 0.4021 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3842 - acc: 0.8645 - precision: 0.1355 - recall: 0.9953 - f1_metric: 0.2338 - val_loss: 0.3993 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4012 - acc: 0.8528 - precision: 0.1472 - recall: 0.9935 - f1_metric: 0.2526 - val_loss: 0.3990 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3861 - acc: 0.8609 - precision: 0.1391 - recall: 0.9991 - f1_metric: 0.2397 - val_loss: 0.3944 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3864 - acc: 0.8575 - precision: 0.1425 - recall: 0.9704 - f1_metric: 0.2434 - val_loss: 0.3908 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3773 - acc: 0.8603 - precision: 0.1397 - recall: 0.9877 - f1_metric: 0.2400 - val_loss: 0.3886 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 5 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 15ms/step - loss: 0.4922 - acc: 0.8379 - precision: 0.1425 - recall: 1.0285 - f1_metric: 0.2462 - val_loss: 0.4180 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4061 - acc: 0.8617 - precision: 0.1383 - recall: 1.0000 - f1_metric: 0.2380 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4012 - acc: 0.8633 - precision: 0.1367 - recall: 0.9959 - f1_metric: 0.2359 - val_loss: 0.4182 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4035 - acc: 0.8635 - precision: 0.1365 - recall: 0.9930 - f1_metric: 0.2345 - val_loss: 0.4165 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4029 - acc: 0.8618 - precision: 0.1382 - recall: 0.9863 - f1_metric: 0.2375 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3901 - acc: 0.8671 - precision: 0.1329 - recall: 0.9727 - f1_metric: 0.2282 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3966 - acc: 0.8658 - precision: 0.1342 - recall: 0.9879 - f1_metric: 0.2320 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4005 - acc: 0.8626 - precision: 0.1374 - recall: 1.0000 - f1_metric: 0.2370 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4042 - acc: 0.8604 - precision: 0.1395 - recall: 1.0000 - f1_metric: 0.2396 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4050 - acc: 0.8583 - precision: 0.1417 - recall: 0.9849 - f1_metric: 0.2434 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4013 - acc: 0.8617 - precision: 0.1383 - recall: 0.9628 - f1_metric: 0.2376 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3936 - acc: 0.8630 - precision: 0.1370 - recall: 0.9927 - f1_metric: 0.2359 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3972 - acc: 0.8618 - precision: 0.1382 - recall: 1.0000 - f1_metric: 0.2387 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8628 - precision: 0.1372 - recall: 0.9813 - f1_metric: 0.2354 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3937 - acc: 0.8636 - precision: 0.1364 - recall: 0.9962 - f1_metric: 0.2353 - val_loss: 0.4128 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4079 - acc: 0.8564 - precision: 0.1436 - recall: 0.9944 - f1_metric: 0.2455 - val_loss: 0.4162 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4044 - acc: 0.8591 - precision: 0.1409 - recall: 0.9943 - f1_metric: 0.2422 - val_loss: 0.4118 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4193 - acc: 0.8480 - precision: 0.1521 - recall: 0.9992 - f1_metric: 0.2591 - val_loss: 0.4109 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4053 - acc: 0.8554 - precision: 0.1446 - recall: 0.9933 - f1_metric: 0.2478 - val_loss: 0.4130 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3870 - acc: 0.8666 - precision: 0.1334 - recall: 0.9883 - f1_metric: 0.2302 - val_loss: 0.4083 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3914 - acc: 0.8645 - precision: 0.1355 - recall: 0.9827 - f1_metric: 0.2338 - val_loss: 0.4067 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4008 - acc: 0.8575 - precision: 0.1425 - recall: 0.9931 - f1_metric: 0.2438 - val_loss: 0.4065 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3975 - acc: 0.8572 - precision: 0.1428 - recall: 1.0000 - f1_metric: 0.2447 - val_loss: 0.4046 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3965 - acc: 0.8585 - precision: 0.1415 - recall: 0.9965 - f1_metric: 0.2413 - val_loss: 0.4002 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3909 - acc: 0.8585 - precision: 0.1415 - recall: 0.9973 - f1_metric: 0.2430 - val_loss: 0.3970 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8577 - precision: 0.1423 - recall: 0.9874 - f1_metric: 0.2440 - val_loss: 0.3945 - val_acc: 0.8507 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3815 - acc: 0.8623 - precision: 0.1377 - recall: 0.9852 - f1_metric: 0.2367 - val_loss: 0.3966 - val_acc: 0.8501 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3781 - acc: 0.8606 - precision: 0.1393 - recall: 0.9956 - f1_metric: 0.2407 - val_loss: 0.3987 - val_acc: 0.8501 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3801 - acc: 0.8599 - precision: 0.1393 - recall: 0.9988 - f1_metric: 0.2398 - val_loss: 0.3826 - val_acc: 0.8501 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3701 - acc: 0.8641 - precision: 0.1358 - recall: 0.9832 - f1_metric: 0.2338 - val_loss: 0.3790 - val_acc: 0.8501 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Model 6 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.5080 - acc: 0.8311 - precision: 0.1396 - recall: 1.0361 - f1_metric: 0.2412 - val_loss: 0.4204 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4146 - acc: 0.8571 - precision: 0.1429 - recall: 0.9726 - f1_metric: 0.2436 - val_loss: 0.4199 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4151 - acc: 0.8585 - precision: 0.1415 - recall: 1.0000 - f1_metric: 0.2435 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4049 - acc: 0.8608 - precision: 0.1392 - recall: 0.9990 - f1_metric: 0.2390 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4012 - acc: 0.8620 - precision: 0.1380 - recall: 0.9921 - f1_metric: 0.2382 - val_loss: 0.4169 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3962 - acc: 0.8652 - precision: 0.1348 - recall: 0.9932 - f1_metric: 0.2326 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3951 - acc: 0.8666 - precision: 0.1334 - recall: 0.9959 - f1_metric: 0.2309 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4044 - acc: 0.8594 - precision: 0.1406 - recall: 0.9938 - f1_metric: 0.2416 - val_loss: 0.4183 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8583 - precision: 0.1417 - recall: 0.9870 - f1_metric: 0.2436 - val_loss: 0.4188 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.8600 - precision: 0.1400 - recall: 0.9707 - f1_metric: 0.2389 - val_loss: 0.4154 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4061 - acc: 0.8599 - precision: 0.1401 - recall: 0.9733 - f1_metric: 0.2409 - val_loss: 0.4155 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3963 - acc: 0.8642 - precision: 0.1358 - recall: 0.9928 - f1_metric: 0.2327 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3931 - acc: 0.8641 - precision: 0.1359 - recall: 0.9941 - f1_metric: 0.2341 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4014 - acc: 0.8602 - precision: 0.1398 - recall: 0.9833 - f1_metric: 0.2401 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8635 - precision: 0.1365 - recall: 0.9967 - f1_metric: 0.2360 - val_loss: 0.4144 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4080 - acc: 0.8561 - precision: 0.1439 - recall: 0.9995 - f1_metric: 0.2472 - val_loss: 0.4141 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4082 - acc: 0.8563 - precision: 0.1437 - recall: 1.0000 - f1_metric: 0.2470 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4010 - acc: 0.8591 - precision: 0.1409 - recall: 0.9875 - f1_metric: 0.2416 - val_loss: 0.4149 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4044 - acc: 0.8592 - precision: 0.1408 - recall: 0.9762 - f1_metric: 0.2414 - val_loss: 0.4118 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4012 - acc: 0.8598 - precision: 0.1402 - recall: 1.0000 - f1_metric: 0.2407 - val_loss: 0.4117 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4017 - acc: 0.8595 - precision: 0.1405 - recall: 1.0000 - f1_metric: 0.2408 - val_loss: 0.4135 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3944 - acc: 0.8617 - precision: 0.1383 - recall: 1.0000 - f1_metric: 0.2385 - val_loss: 0.4128 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4089 - acc: 0.8545 - precision: 0.1455 - recall: 0.9902 - f1_metric: 0.2501 - val_loss: 0.4078 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3931 - acc: 0.8611 - precision: 0.1389 - recall: 0.9941 - f1_metric: 0.2381 - val_loss: 0.4067 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8577 - precision: 0.1423 - recall: 1.0000 - f1_metric: 0.2450 - val_loss: 0.4081 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3898 - acc: 0.8612 - precision: 0.1388 - recall: 0.9824 - f1_metric: 0.2391 - val_loss: 0.4028 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4000 - acc: 0.8554 - precision: 0.1446 - recall: 0.9986 - f1_metric: 0.2461 - val_loss: 0.4126 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3920 - acc: 0.8584 - precision: 0.1416 - recall: 0.9934 - f1_metric: 0.2428 - val_loss: 0.3987 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3913 - acc: 0.8564 - precision: 0.1436 - recall: 0.9935 - f1_metric: 0.2464 - val_loss: 0.3951 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3787 - acc: 0.8626 - precision: 0.1374 - recall: 0.9960 - f1_metric: 0.2372 - val_loss: 0.3955 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 7 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.4574 - acc: 0.8571 - precision: 0.1433 - recall: 1.0000 - f1_metric: 0.2465 - val_loss: 0.4185 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4035 - acc: 0.8636 - precision: 0.1364 - recall: 0.9883 - f1_metric: 0.2340 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4074 - acc: 0.8622 - precision: 0.1378 - recall: 0.9944 - f1_metric: 0.2368 - val_loss: 0.4189 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3958 - acc: 0.8673 - precision: 0.1327 - recall: 0.9791 - f1_metric: 0.2292 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4130 - acc: 0.8574 - precision: 0.1426 - recall: 1.0000 - f1_metric: 0.2439 - val_loss: 0.4181 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4110 - acc: 0.8583 - precision: 0.1417 - recall: 0.9923 - f1_metric: 0.2434 - val_loss: 0.4190 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4072 - acc: 0.8601 - precision: 0.1399 - recall: 0.9806 - f1_metric: 0.2404 - val_loss: 0.4179 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4086 - acc: 0.8585 - precision: 0.1415 - recall: 0.9954 - f1_metric: 0.2436 - val_loss: 0.4162 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4139 - acc: 0.8537 - precision: 0.1463 - recall: 0.9966 - f1_metric: 0.2494 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3929 - acc: 0.8673 - precision: 0.1327 - recall: 0.9925 - f1_metric: 0.2297 - val_loss: 0.4163 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4008 - acc: 0.8619 - precision: 0.1381 - recall: 0.9997 - f1_metric: 0.2384 - val_loss: 0.4152 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4029 - acc: 0.8601 - precision: 0.1399 - recall: 1.0000 - f1_metric: 0.2410 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3995 - acc: 0.8629 - precision: 0.1371 - recall: 0.9954 - f1_metric: 0.2367 - val_loss: 0.4145 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4055 - acc: 0.8573 - precision: 0.1427 - recall: 0.9900 - f1_metric: 0.2455 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4038 - acc: 0.8584 - precision: 0.1416 - recall: 0.9955 - f1_metric: 0.2426 - val_loss: 0.4136 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4039 - acc: 0.8592 - precision: 0.1408 - recall: 0.9817 - f1_metric: 0.2410 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3911 - acc: 0.8637 - precision: 0.1363 - recall: 1.0000 - f1_metric: 0.2325 - val_loss: 0.4127 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3981 - acc: 0.8621 - precision: 0.1379 - recall: 0.9917 - f1_metric: 0.2372 - val_loss: 0.4127 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4008 - acc: 0.8603 - precision: 0.1397 - recall: 0.9902 - f1_metric: 0.2405 - val_loss: 0.4113 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3947 - acc: 0.8629 - precision: 0.1371 - recall: 1.0000 - f1_metric: 0.2366 - val_loss: 0.4107 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3897 - acc: 0.8643 - precision: 0.1357 - recall: 0.9828 - f1_metric: 0.2344 - val_loss: 0.4102 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3943 - acc: 0.8611 - precision: 0.1389 - recall: 1.0000 - f1_metric: 0.2390 - val_loss: 0.4087 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3961 - acc: 0.8599 - precision: 0.1401 - recall: 1.0000 - f1_metric: 0.2411 - val_loss: 0.4074 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4011 - acc: 0.8564 - precision: 0.1436 - recall: 0.9960 - f1_metric: 0.2465 - val_loss: 0.4058 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4011 - acc: 0.8552 - precision: 0.1448 - recall: 0.9957 - f1_metric: 0.2487 - val_loss: 0.4044 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3846 - acc: 0.8653 - precision: 0.1347 - recall: 1.0000 - f1_metric: 0.2327 - val_loss: 0.4011 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4035 - acc: 0.8517 - precision: 0.1483 - recall: 1.0000 - f1_metric: 0.2533 - val_loss: 0.4042 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3923 - acc: 0.8572 - precision: 0.1428 - recall: 1.0000 - f1_metric: 0.2459 - val_loss: 0.3954 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3621 - acc: 0.8719 - precision: 0.1281 - recall: 0.9976 - f1_metric: 0.2225 - val_loss: 0.3933 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3910 - acc: 0.8550 - precision: 0.1450 - recall: 0.9990 - f1_metric: 0.2485 - val_loss: 0.3894 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 8 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 15ms/step - loss: 0.4960 - acc: 0.8221 - precision: 0.1416 - recall: 1.0968 - f1_metric: 0.2452 - val_loss: 0.4203 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4134 - acc: 0.8568 - precision: 0.1432 - recall: 1.0000 - f1_metric: 0.2457 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4079 - acc: 0.8613 - precision: 0.1386 - recall: 0.9919 - f1_metric: 0.2382 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4113 - acc: 0.8584 - precision: 0.1416 - recall: 1.0000 - f1_metric: 0.2428 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3883 - acc: 0.8694 - precision: 0.1306 - recall: 0.9885 - f1_metric: 0.2261 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4038 - acc: 0.8602 - precision: 0.1398 - recall: 0.9983 - f1_metric: 0.2411 - val_loss: 0.4182 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4044 - acc: 0.8603 - precision: 0.1397 - recall: 0.9983 - f1_metric: 0.2411 - val_loss: 0.4162 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4090 - acc: 0.8569 - precision: 0.1431 - recall: 1.0000 - f1_metric: 0.2459 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4137 - acc: 0.8542 - precision: 0.1458 - recall: 0.9953 - f1_metric: 0.2495 - val_loss: 0.4180 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4019 - acc: 0.8608 - precision: 0.1392 - recall: 0.9972 - f1_metric: 0.2400 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4081 - acc: 0.8568 - precision: 0.1432 - recall: 0.9944 - f1_metric: 0.2442 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3962 - acc: 0.8628 - precision: 0.1372 - recall: 0.9718 - f1_metric: 0.2341 - val_loss: 0.4149 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4036 - acc: 0.8599 - precision: 0.1401 - recall: 0.9978 - f1_metric: 0.2409 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4072 - acc: 0.8574 - precision: 0.1426 - recall: 0.9805 - f1_metric: 0.2441 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4071 - acc: 0.8564 - precision: 0.1436 - recall: 0.9972 - f1_metric: 0.2469 - val_loss: 0.4144 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3787 - acc: 0.8715 - precision: 0.1285 - recall: 0.9878 - f1_metric: 0.2229 - val_loss: 0.4140 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4076 - acc: 0.8580 - precision: 0.1420 - recall: 0.9836 - f1_metric: 0.2430 - val_loss: 0.4129 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3973 - acc: 0.8607 - precision: 0.1393 - recall: 0.9976 - f1_metric: 0.2401 - val_loss: 0.4128 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4034 - acc: 0.8578 - precision: 0.1422 - recall: 0.9909 - f1_metric: 0.2438 - val_loss: 0.4117 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3857 - acc: 0.8677 - precision: 0.1323 - recall: 1.0000 - f1_metric: 0.2288 - val_loss: 0.4097 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4013 - acc: 0.8592 - precision: 0.1408 - recall: 0.9902 - f1_metric: 0.2407 - val_loss: 0.4119 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4025 - acc: 0.8568 - precision: 0.1432 - recall: 1.0000 - f1_metric: 0.2466 - val_loss: 0.4078 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8612 - precision: 0.1388 - recall: 1.0000 - f1_metric: 0.2400 - val_loss: 0.4083 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3904 - acc: 0.8625 - precision: 0.1375 - recall: 0.9993 - f1_metric: 0.2354 - val_loss: 0.4058 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3986 - acc: 0.8571 - precision: 0.1429 - recall: 0.9922 - f1_metric: 0.2448 - val_loss: 0.4019 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3883 - acc: 0.8618 - precision: 0.1382 - recall: 1.0000 - f1_metric: 0.2381 - val_loss: 0.3984 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3878 - acc: 0.8603 - precision: 0.1397 - recall: 0.9932 - f1_metric: 0.2407 - val_loss: 0.3959 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3877 - acc: 0.8559 - precision: 0.1441 - recall: 0.9913 - f1_metric: 0.2465 - val_loss: 0.3969 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3910 - acc: 0.8563 - precision: 0.1437 - recall: 0.9983 - f1_metric: 0.2458 - val_loss: 0.3894 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3741 - acc: 0.8634 - precision: 0.1366 - recall: 0.9917 - f1_metric: 0.2356 - val_loss: 0.3865 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 9 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 15ms/step - loss: 0.4948 - acc: 0.8186 - precision: 0.1408 - recall: 0.9188 - f1_metric: 0.2390 - val_loss: 0.4233 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4078 - acc: 0.8599 - precision: 0.1397 - recall: 0.9924 - f1_metric: 0.2404 - val_loss: 0.4179 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4057 - acc: 0.8607 - precision: 0.1393 - recall: 0.9986 - f1_metric: 0.2392 - val_loss: 0.4182 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3935 - acc: 0.8680 - precision: 0.1320 - recall: 0.9902 - f1_metric: 0.2289 - val_loss: 0.4172 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8660 - precision: 0.1340 - recall: 1.0000 - f1_metric: 0.2312 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4175 - acc: 0.8531 - precision: 0.1469 - recall: 0.9856 - f1_metric: 0.2504 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4096 - acc: 0.8566 - precision: 0.1434 - recall: 0.9934 - f1_metric: 0.2448 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3960 - acc: 0.8644 - precision: 0.1356 - recall: 1.0000 - f1_metric: 0.2336 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3909 - acc: 0.8664 - precision: 0.1336 - recall: 0.9671 - f1_metric: 0.2296 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4083 - acc: 0.8564 - precision: 0.1436 - recall: 1.0000 - f1_metric: 0.2471 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4004 - acc: 0.8615 - precision: 0.1385 - recall: 1.0000 - f1_metric: 0.2385 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4011 - acc: 0.8611 - precision: 0.1389 - recall: 1.0000 - f1_metric: 0.2394 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4121 - acc: 0.8546 - precision: 0.1454 - recall: 0.9984 - f1_metric: 0.2496 - val_loss: 0.4149 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4186 - acc: 0.8503 - precision: 0.1497 - recall: 0.9915 - f1_metric: 0.2555 - val_loss: 0.4194 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3949 - acc: 0.8647 - precision: 0.1353 - recall: 0.9870 - f1_metric: 0.2331 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3940 - acc: 0.8636 - precision: 0.1365 - recall: 1.0000 - f1_metric: 0.2358 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3898 - acc: 0.8661 - precision: 0.1339 - recall: 0.9891 - f1_metric: 0.2320 - val_loss: 0.4136 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3844 - acc: 0.8679 - precision: 0.1321 - recall: 0.9814 - f1_metric: 0.2285 - val_loss: 0.4128 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3925 - acc: 0.8651 - precision: 0.1349 - recall: 0.9945 - f1_metric: 0.2327 - val_loss: 0.4120 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3906 - acc: 0.8652 - precision: 0.1348 - recall: 0.9824 - f1_metric: 0.2319 - val_loss: 0.4112 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8565 - precision: 0.1435 - recall: 0.9964 - f1_metric: 0.2459 - val_loss: 0.4107 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3967 - acc: 0.8594 - precision: 0.1406 - recall: 0.9934 - f1_metric: 0.2418 - val_loss: 0.4094 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3974 - acc: 0.8584 - precision: 0.1416 - recall: 1.0000 - f1_metric: 0.2437 - val_loss: 0.4109 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3873 - acc: 0.8655 - precision: 0.1345 - recall: 0.9947 - f1_metric: 0.23 - 1s 3ms/step - loss: 0.3873 - acc: 0.8654 - precision: 0.1346 - recall: 0.9947 - f1_metric: 0.2322 - val_loss: 0.4068 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8568 - precision: 0.1432 - recall: 0.9949 - f1_metric: 0.2473 - val_loss: 0.4057 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3788 - acc: 0.8671 - precision: 0.1329 - recall: 0.9946 - f1_metric: 0.2287 - val_loss: 0.4093 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4007 - acc: 0.8556 - precision: 0.1444 - recall: 0.9710 - f1_metric: 0.2475 - val_loss: 0.4230 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3898 - acc: 0.8594 - precision: 0.1406 - recall: 0.9993 - f1_metric: 0.2420 - val_loss: 0.4065 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3885 - acc: 0.8592 - precision: 0.1408 - recall: 1.0000 - f1_metric: 0.2425 - val_loss: 0.3956 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3794 - acc: 0.8635 - precision: 0.1365 - recall: 1.0000 - f1_metric: 0.2355 - val_loss: 0.3937 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 10 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.5004 - acc: 0.8200 - precision: 0.1424 - recall: 1.0180 - f1_metric: 0.2450 - val_loss: 0.4193 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4042 - acc: 0.8609 - precision: 0.1391 - recall: 0.9719 - f1_metric: 0.2389 - val_loss: 0.4172 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4114 - acc: 0.8590 - precision: 0.1409 - recall: 0.9759 - f1_metric: 0.2413 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4138 - acc: 0.8541 - precision: 0.1459 - recall: 0.9844 - f1_metric: 0.2498 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4062 - acc: 0.8600 - precision: 0.1400 - recall: 1.0000 - f1_metric: 0.2404 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4031 - acc: 0.8597 - precision: 0.1403 - recall: 0.9718 - f1_metric: 0.2405 - val_loss: 0.4178 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3993 - acc: 0.8638 - precision: 0.1362 - recall: 0.9857 - f1_metric: 0.2356 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.8586 - precision: 0.1414 - recall: 1.0000 - f1_metric: 0.2430 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4130 - acc: 0.8554 - precision: 0.1446 - recall: 0.9865 - f1_metric: 0.2469 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8635 - precision: 0.1365 - recall: 0.9902 - f1_metric: 0.2363 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4006 - acc: 0.8615 - precision: 0.1385 - recall: 1.0000 - f1_metric: 0.2386 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3969 - acc: 0.8626 - precision: 0.1374 - recall: 0.9854 - f1_metric: 0.2374 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8669 - precision: 0.1331 - recall: 0.9859 - f1_metric: 0.2307 - val_loss: 0.4140 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4004 - acc: 0.8615 - precision: 0.1385 - recall: 1.0000 - f1_metric: 0.2391 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3997 - acc: 0.8605 - precision: 0.1395 - recall: 0.9988 - f1_metric: 0.2397 - val_loss: 0.4130 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4195 - acc: 0.8492 - precision: 0.1508 - recall: 0.9951 - f1_metric: 0.2570 - val_loss: 0.4265 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3906 - acc: 0.8657 - precision: 0.1343 - recall: 0.9853 - f1_metric: 0.2319 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3987 - acc: 0.8612 - precision: 0.1388 - recall: 0.9894 - f1_metric: 0.2393 - val_loss: 0.4114 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3959 - acc: 0.8606 - precision: 0.1394 - recall: 0.9734 - f1_metric: 0.2395 - val_loss: 0.4106 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3858 - acc: 0.8671 - precision: 0.1329 - recall: 0.9764 - f1_metric: 0.2296 - val_loss: 0.4095 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3924 - acc: 0.8626 - precision: 0.1374 - recall: 1.0000 - f1_metric: 0.2372 - val_loss: 0.4082 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3920 - acc: 0.8643 - precision: 0.1357 - recall: 0.9850 - f1_metric: 0.2344 - val_loss: 0.4070 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3816 - acc: 0.8668 - precision: 0.1332 - recall: 0.9802 - f1_metric: 0.2297 - val_loss: 0.4057 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3890 - acc: 0.8628 - precision: 0.1372 - recall: 0.9830 - f1_metric: 0.2360 - val_loss: 0.4041 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3946 - acc: 0.8589 - precision: 0.1410 - recall: 0.9996 - f1_metric: 0.2418 - val_loss: 0.4016 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3775 - acc: 0.8677 - precision: 0.1323 - recall: 0.9994 - f1_metric: 0.2299 - val_loss: 0.4027 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3856 - acc: 0.8637 - precision: 0.1363 - recall: 0.9622 - f1_metric: 0.2341 - val_loss: 0.3978 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3810 - acc: 0.8639 - precision: 0.1361 - recall: 0.9890 - f1_metric: 0.2352 - val_loss: 0.3965 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3777 - acc: 0.8635 - precision: 0.1365 - recall: 0.9885 - f1_metric: 0.2353 - val_loss: 0.3909 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3595 - acc: 0.8717 - precision: 0.1283 - recall: 0.9941 - f1_metric: 0.2229 - val_loss: 0.3875 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 11 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.4629 - acc: 0.8558 - precision: 0.1426 - recall: 1.0604 - f1_metric: 0.2469 - val_loss: 0.4190 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4049 - acc: 0.8598 - precision: 0.1402 - recall: 0.9999 - f1_metric: 0.2411 - val_loss: 0.4185 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4124 - acc: 0.8594 - precision: 0.1406 - recall: 0.9967 - f1_metric: 0.2421 - val_loss: 0.4181 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4126 - acc: 0.8554 - precision: 0.1446 - recall: 0.9869 - f1_metric: 0.2476 - val_loss: 0.4180 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4082 - acc: 0.8592 - precision: 0.1408 - recall: 0.9838 - f1_metric: 0.2427 - val_loss: 0.4172 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3946 - acc: 0.8663 - precision: 0.1337 - recall: 0.9908 - f1_metric: 0.2307 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4156 - acc: 0.8556 - precision: 0.1444 - recall: 0.9992 - f1_metric: 0.2482 - val_loss: 0.4183 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3827 - acc: 0.8715 - precision: 0.1285 - recall: 0.9611 - f1_metric: 0.2222 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4012 - acc: 0.8614 - precision: 0.1386 - recall: 1.0000 - f1_metric: 0.2376 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3964 - acc: 0.8655 - precision: 0.1345 - recall: 0.9904 - f1_metric: 0.2319 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4033 - acc: 0.8613 - precision: 0.1387 - recall: 0.9924 - f1_metric: 0.2377 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3932 - acc: 0.8637 - precision: 0.1363 - recall: 0.9993 - f1_metric: 0.2352 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4119 - acc: 0.8559 - precision: 0.1441 - recall: 1.0000 - f1_metric: 0.2476 - val_loss: 0.4165 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4021 - acc: 0.8607 - precision: 0.1393 - recall: 0.9981 - f1_metric: 0.2398 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4073 - acc: 0.8575 - precision: 0.1425 - recall: 1.0000 - f1_metric: 0.2448 - val_loss: 0.4192 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4024 - acc: 0.8628 - precision: 0.1372 - recall: 0.9918 - f1_metric: 0.2366 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8603 - precision: 0.1396 - recall: 0.9920 - f1_metric: 0.2399 - val_loss: 0.4140 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3972 - acc: 0.8625 - precision: 0.1375 - recall: 1.0000 - f1_metric: 0.2373 - val_loss: 0.4144 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3952 - acc: 0.8620 - precision: 0.1380 - recall: 0.9964 - f1_metric: 0.2387 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4024 - acc: 0.8604 - precision: 0.1396 - recall: 0.9850 - f1_metric: 0.2400 - val_loss: 0.4125 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3946 - acc: 0.8626 - precision: 0.1374 - recall: 0.9877 - f1_metric: 0.2369 - val_loss: 0.4123 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3997 - acc: 0.8613 - precision: 0.1387 - recall: 0.9837 - f1_metric: 0.2384 - val_loss: 0.4122 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3940 - acc: 0.8619 - precision: 0.1381 - recall: 0.9845 - f1_metric: 0.2376 - val_loss: 0.4106 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3916 - acc: 0.8640 - precision: 0.1360 - recall: 1.0000 - f1_metric: 0.2348 - val_loss: 0.4098 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4011 - acc: 0.8573 - precision: 0.1427 - recall: 0.9802 - f1_metric: 0.2444 - val_loss: 0.4110 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4022 - acc: 0.8576 - precision: 0.1424 - recall: 1.0000 - f1_metric: 0.2452 - val_loss: 0.4067 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4049 - acc: 0.8536 - precision: 0.1464 - recall: 1.0000 - f1_metric: 0.2505 - val_loss: 0.4046 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3918 - acc: 0.8602 - precision: 0.1398 - recall: 0.9902 - f1_metric: 0.2399 - val_loss: 0.4034 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4075 - acc: 0.8531 - precision: 0.1469 - recall: 1.0000 - f1_metric: 0.2502 - val_loss: 0.4018 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3872 - acc: 0.8618 - precision: 0.1382 - recall: 1.0000 - f1_metric: 0.2384 - val_loss: 0.3995 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 12 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 15ms/step - loss: 0.5227 - acc: 0.7746 - precision: 0.1394 - recall: 0.9196 - f1_metric: 0.2350 - val_loss: 0.4187 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3981 - acc: 0.8655 - precision: 0.1348 - recall: 0.9996 - f1_metric: 0.2332 - val_loss: 0.4176 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4130 - acc: 0.8585 - precision: 0.1417 - recall: 0.9950 - f1_metric: 0.2437 - val_loss: 0.4179 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4031 - acc: 0.8625 - precision: 0.1371 - recall: 0.9932 - f1_metric: 0.2359 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4103 - acc: 0.8592 - precision: 0.1407 - recall: 0.9977 - f1_metric: 0.2424 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4047 - acc: 0.8616 - precision: 0.1384 - recall: 0.9871 - f1_metric: 0.2384 - val_loss: 0.4183 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4184 - acc: 0.8551 - precision: 0.1449 - recall: 1.0007 - f1_metric: 0.2491 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4126 - acc: 0.8578 - precision: 0.1421 - recall: 0.9845 - f1_metric: 0.2438 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3925 - acc: 0.8675 - precision: 0.1323 - recall: 0.9952 - f1_metric: 0.2291 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4028 - acc: 0.8607 - precision: 0.1394 - recall: 0.9875 - f1_metric: 0.2391 - val_loss: 0.4178 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3856 - acc: 0.8694 - precision: 0.1306 - recall: 0.9918 - f1_metric: 0.2258 - val_loss: 0.4155 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4038 - acc: 0.8608 - precision: 0.1391 - recall: 0.9683 - f1_metric: 0.2380 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3856 - acc: 0.8696 - precision: 0.1304 - recall: 0.9861 - f1_metric: 0.2260 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4027 - acc: 0.8617 - precision: 0.1383 - recall: 0.9859 - f1_metric: 0.2379 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4027 - acc: 0.8582 - precision: 0.1418 - recall: 1.0000 - f1_metric: 0.2430 - val_loss: 0.4162 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3970 - acc: 0.8621 - precision: 0.1379 - recall: 1.0000 - f1_metric: 0.2386 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3984 - acc: 0.8602 - precision: 0.1398 - recall: 0.9924 - f1_metric: 0.2409 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3887 - acc: 0.8666 - precision: 0.1334 - recall: 0.9944 - f1_metric: 0.2311 - val_loss: 0.4125 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3975 - acc: 0.8620 - precision: 0.1380 - recall: 0.9981 - f1_metric: 0.2379 - val_loss: 0.4124 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3955 - acc: 0.8616 - precision: 0.1384 - recall: 0.9907 - f1_metric: 0.2389 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4171 - acc: 0.8516 - precision: 0.1484 - recall: 0.9921 - f1_metric: 0.2529 - val_loss: 0.4124 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8606 - precision: 0.1394 - recall: 1.0000 - f1_metric: 0.2399 - val_loss: 0.4098 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3908 - acc: 0.8621 - precision: 0.1379 - recall: 0.9663 - f1_metric: 0.2372 - val_loss: 0.4098 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3799 - acc: 0.8699 - precision: 0.1301 - recall: 0.9874 - f1_metric: 0.2249 - val_loss: 0.4073 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3886 - acc: 0.8624 - precision: 0.1376 - recall: 0.9749 - f1_metric: 0.2371 - val_loss: 0.4068 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3950 - acc: 0.8600 - precision: 0.1400 - recall: 0.9992 - f1_metric: 0.2405 - val_loss: 0.4045 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3965 - acc: 0.8577 - precision: 0.1423 - recall: 0.9916 - f1_metric: 0.2440 - val_loss: 0.4039 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3928 - acc: 0.8584 - precision: 0.1416 - recall: 0.9874 - f1_metric: 0.2431 - val_loss: 0.3993 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3786 - acc: 0.8654 - precision: 0.1346 - recall: 1.0000 - f1_metric: 0.2331 - val_loss: 0.4021 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3969 - acc: 0.8555 - precision: 0.1445 - recall: 0.9860 - f1_metric: 0.2483 - val_loss: 0.3994 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 13 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 16ms/step - loss: 0.5601 - acc: 0.7436 - precision: 0.1441 - recall: 0.6267 - f1_metric: 0.2254 - val_loss: 0.4220 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4090 - acc: 0.8603 - precision: 0.1498 - recall: 0.5826 - f1_metric: 0.2285 - val_loss: 0.4174 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4034 - acc: 0.8617 - precision: 0.1492 - recall: 0.4118 - f1_metric: 0.2069 - val_loss: 0.4184 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8664 - precision: 0.1209 - recall: 0.2199 - f1_metric: 0.1463 - val_loss: 0.4188 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4060 - acc: 0.8611 - precision: 0.1613 - recall: 0.2208 - f1_metric: 0.1724 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4027 - acc: 0.8619 - precision: 0.1441 - recall: 0.2672 - f1_metric: 0.1706 - val_loss: 0.4162 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3998 - acc: 0.8631 - precision: 0.1405 - recall: 0.1824 - f1_metric: 0.1428 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8594 - precision: 0.1242 - recall: 0.1619 - f1_metric: 0.1275 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4112 - acc: 0.8557 - precision: 0.1427 - recall: 0.1933 - f1_metric: 0.1559 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4250 - acc: 0.8481 - precision: 0.1331 - recall: 0.1393 - f1_metric: 0.1275 - val_loss: 0.4219 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4078 - acc: 0.8571 - precision: 0.1261 - recall: 0.1849 - f1_metric: 0.1415 - val_loss: 0.4217 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3991 - acc: 0.8634 - precision: 0.1612 - recall: 0.1650 - f1_metric: 0.1498 - val_loss: 0.4152 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3991 - acc: 0.8619 - precision: 0.1455 - recall: 0.2496 - f1_metric: 0.1733 - val_loss: 0.4145 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3927 - acc: 0.8660 - precision: 0.1352 - recall: 0.3073 - f1_metric: 0.1778 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4029 - acc: 0.8594 - precision: 0.1494 - recall: 0.3144 - f1_metric: 0.1936 - val_loss: 0.4141 - val_acc: 0.8507 - val_precision: 0.1679 - val_recall: 0.0923 - val_f1_metric: 0.1097\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4014 - acc: 0.8596 - precision: 0.1388 - recall: 0.3929 - f1_metric: 0.1957 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1814 - val_recall: 0.6975 - val_f1_metric: 0.2770\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8637 - precision: 0.1545 - recall: 0.5165 - f1_metric: 0.2268 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.2119 - val_recall: 0.3266 - val_f1_metric: 0.2429\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3950 - acc: 0.8643 - precision: 0.1354 - recall: 0.3926 - f1_metric: 0.1932 - val_loss: 0.4134 - val_acc: 0.8507 - val_precision: 0.1715 - val_recall: 0.8039 - val_f1_metric: 0.2730\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8615 - precision: 0.1630 - recall: 0.6069 - f1_metric: 0.2476 - val_loss: 0.4130 - val_acc: 0.8507 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4041 - acc: 0.8580 - precision: 0.1687 - recall: 0.3475 - f1_metric: 0.2138 - val_loss: 0.4121 - val_acc: 0.8507 - val_precision: 0.1865 - val_recall: 0.6824 - val_f1_metric: 0.2816\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4069 - acc: 0.8575 - precision: 0.1461 - recall: 0.5689 - f1_metric: 0.2241 - val_loss: 0.4125 - val_acc: 0.8507 - val_precision: 0.1586 - val_recall: 0.9501 - val_f1_metric: 0.2645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3963 - acc: 0.8622 - precision: 0.1434 - recall: 0.7721 - f1_metric: 0.2375 - val_loss: 0.4106 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4041 - acc: 0.8583 - precision: 0.1537 - recall: 0.8880 - f1_metric: 0.2573 - val_loss: 0.4093 - val_acc: 0.8507 - val_precision: 0.1594 - val_recall: 0.9525 - val_f1_metric: 0.2657\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4035 - acc: 0.8559 - precision: 0.1489 - recall: 0.8234 - f1_metric: 0.2449 - val_loss: 0.4092 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3907 - acc: 0.8630 - precision: 0.1445 - recall: 0.8967 - f1_metric: 0.2447 - val_loss: 0.4071 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8613 - precision: 0.1408 - recall: 0.9520 - f1_metric: 0.2402 - val_loss: 0.4053 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8688 - precision: 0.1333 - recall: 0.9582 - f1_metric: 0.2291 - val_loss: 0.4048 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3866 - acc: 0.8624 - precision: 0.1381 - recall: 0.9738 - f1_metric: 0.2374 - val_loss: 0.4018 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3977 - acc: 0.8572 - precision: 0.1429 - recall: 1.0000 - f1_metric: 0.2462 - val_loss: 0.3996 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3916 - acc: 0.8595 - precision: 0.1407 - recall: 0.9991 - f1_metric: 0.2428 - val_loss: 0.3962 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 14 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 15ms/step - loss: 0.4849 - acc: 0.8524 - precision: 0.1402 - recall: 0.9730 - f1_metric: 0.2404 - val_loss: 0.4190 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4142 - acc: 0.8580 - precision: 0.1420 - recall: 0.9978 - f1_metric: 0.2433 - val_loss: 0.4206 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4207 - acc: 0.8539 - precision: 0.1463 - recall: 0.9998 - f1_metric: 0.2498 - val_loss: 0.4191 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3992 - acc: 0.8635 - precision: 0.1365 - recall: 1.0000 - f1_metric: 0.2350 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4024 - acc: 0.8627 - precision: 0.1373 - recall: 1.0000 - f1_metric: 0.2374 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4016 - acc: 0.8612 - precision: 0.1388 - recall: 0.9937 - f1_metric: 0.2390 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4047 - acc: 0.8606 - precision: 0.1394 - recall: 0.9810 - f1_metric: 0.2406 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4111 - acc: 0.8593 - precision: 0.1407 - recall: 1.0000 - f1_metric: 0.2416 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4082 - acc: 0.8580 - precision: 0.1420 - recall: 0.9897 - f1_metric: 0.2442 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4198 - acc: 0.8517 - precision: 0.1483 - recall: 0.9943 - f1_metric: 0.2526 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3937 - acc: 0.8655 - precision: 0.1345 - recall: 0.9915 - f1_metric: 0.2329 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4065 - acc: 0.8581 - precision: 0.1419 - recall: 0.9955 - f1_metric: 0.2442 - val_loss: 0.4145 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3980 - acc: 0.8614 - precision: 0.1386 - recall: 1.0000 - f1_metric: 0.2399 - val_loss: 0.4154 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4058 - acc: 0.8569 - precision: 0.1431 - recall: 0.9874 - f1_metric: 0.2455 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8590 - precision: 0.1410 - recall: 0.9897 - f1_metric: 0.2421 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3951 - acc: 0.8638 - precision: 0.1362 - recall: 1.0000 - f1_metric: 0.2345 - val_loss: 0.4131 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4104 - acc: 0.8559 - precision: 0.1441 - recall: 0.9960 - f1_metric: 0.2471 - val_loss: 0.4123 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4119 - acc: 0.8531 - precision: 0.1469 - recall: 0.9997 - f1_metric: 0.2497 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3956 - acc: 0.8624 - precision: 0.1376 - recall: 0.9975 - f1_metric: 0.2371 - val_loss: 0.4113 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3907 - acc: 0.8651 - precision: 0.1349 - recall: 0.9995 - f1_metric: 0.2318 - val_loss: 0.4095 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4067 - acc: 0.8560 - precision: 0.1440 - recall: 0.9871 - f1_metric: 0.2463 - val_loss: 0.4093 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3788 - acc: 0.8704 - precision: 0.1296 - recall: 0.9712 - f1_metric: 0.2241 - val_loss: 0.4111 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4046 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2488 - val_loss: 0.4090 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3987 - acc: 0.8581 - precision: 0.1419 - recall: 0.9949 - f1_metric: 0.2442 - val_loss: 0.4095 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3847 - acc: 0.8643 - precision: 0.1357 - recall: 0.9912 - f1_metric: 0.2350 - val_loss: 0.4019 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8553 - precision: 0.1447 - recall: 0.9959 - f1_metric: 0.2482 - val_loss: 0.4117 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3879 - acc: 0.8637 - precision: 0.1364 - recall: 0.9963 - f1_metric: 0.2353 - val_loss: 0.4020 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3892 - acc: 0.8581 - precision: 0.1419 - recall: 0.9939 - f1_metric: 0.2437 - val_loss: 0.3939 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3947 - acc: 0.8535 - precision: 0.1465 - recall: 0.9962 - f1_metric: 0.2513 - val_loss: 0.3897 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3780 - acc: 0.8606 - precision: 0.1394 - recall: 0.9997 - f1_metric: 0.2393 - val_loss: 0.3911 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 15 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 15ms/step - loss: 0.4934 - acc: 0.8268 - precision: 0.1432 - recall: 1.0955 - f1_metric: 0.2481 - val_loss: 0.4177 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4110 - acc: 0.8582 - precision: 0.1423 - recall: 0.9944 - f1_metric: 0.2441 - val_loss: 0.4178 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4225 - acc: 0.8538 - precision: 0.1463 - recall: 0.9894 - f1_metric: 0.2506 - val_loss: 0.4183 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4118 - acc: 0.8585 - precision: 0.1413 - recall: 1.0000 - f1_metric: 0.2428 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4105 - acc: 0.8586 - precision: 0.1412 - recall: 0.9924 - f1_metric: 0.2423 - val_loss: 0.4165 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4135 - acc: 0.8585 - precision: 0.1409 - recall: 0.9962 - f1_metric: 0.2426 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8624 - precision: 0.1378 - recall: 0.9694 - f1_metric: 0.2379 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8658 - precision: 0.1340 - recall: 1.0006 - f1_metric: 0.2318 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4183 - acc: 0.8546 - precision: 0.1453 - recall: 1.0007 - f1_metric: 0.2488 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4241 - acc: 0.8490 - precision: 0.1509 - recall: 0.9917 - f1_metric: 0.2567 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4024 - acc: 0.8590 - precision: 0.1409 - recall: 0.9859 - f1_metric: 0.2423 - val_loss: 0.4150 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3996 - acc: 0.8623 - precision: 0.1376 - recall: 1.0000 - f1_metric: 0.2373 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3969 - acc: 0.8634 - precision: 0.1366 - recall: 1.0002 - f1_metric: 0.2353 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4113 - acc: 0.8550 - precision: 0.1448 - recall: 1.0000 - f1_metric: 0.2487 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4024 - acc: 0.8599 - precision: 0.1401 - recall: 0.9893 - f1_metric: 0.2409 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4087 - acc: 0.8544 - precision: 0.1455 - recall: 0.9894 - f1_metric: 0.2493 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4107 - acc: 0.8546 - precision: 0.1454 - recall: 0.9919 - f1_metric: 0.2491 - val_loss: 0.4126 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3989 - acc: 0.8602 - precision: 0.1398 - recall: 0.9963 - f1_metric: 0.2407 - val_loss: 0.4120 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4013 - acc: 0.8599 - precision: 0.1400 - recall: 0.9914 - f1_metric: 0.2400 - val_loss: 0.4113 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3994 - acc: 0.8603 - precision: 0.1397 - recall: 0.9966 - f1_metric: 0.2403 - val_loss: 0.4106 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8552 - precision: 0.1448 - recall: 0.9985 - f1_metric: 0.2483 - val_loss: 0.4100 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3994 - acc: 0.8583 - precision: 0.1417 - recall: 0.9865 - f1_metric: 0.2429 - val_loss: 0.4075 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4073 - acc: 0.8520 - precision: 0.1480 - recall: 0.9856 - f1_metric: 0.2525 - val_loss: 0.4129 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3991 - acc: 0.8570 - precision: 0.1430 - recall: 0.9857 - f1_metric: 0.2447 - val_loss: 0.4039 - val_acc: 0.8507 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3896 - acc: 0.8627 - precision: 0.1371 - recall: 0.9972 - f1_metric: 0.2369 - val_loss: 0.4005 - val_acc: 0.8501 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3843 - acc: 0.8639 - precision: 0.1360 - recall: 0.9934 - f1_metric: 0.2350 - val_loss: 0.4023 - val_acc: 0.8501 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3877 - acc: 0.8587 - precision: 0.1411 - recall: 0.9937 - f1_metric: 0.2421 - val_loss: 0.3950 - val_acc: 0.8501 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3857 - acc: 0.8588 - precision: 0.1408 - recall: 0.9955 - f1_metric: 0.2417 - val_loss: 0.4010 - val_acc: 0.8501 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8536 - precision: 0.1462 - recall: 0.9839 - f1_metric: 0.2492 - val_loss: 0.3895 - val_acc: 0.8501 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3742 - acc: 0.8622 - precision: 0.1375 - recall: 0.9795 - f1_metric: 0.23 - 1s 3ms/step - loss: 0.3744 - acc: 0.8621 - precision: 0.1376 - recall: 0.9802 - f1_metric: 0.2374 - val_loss: 0.3848 - val_acc: 0.8501 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 16 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.4835 - acc: 0.8378 - precision: 0.1423 - recall: 0.8533 - f1_metric: 0.2335 - val_loss: 0.4185 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4131 - acc: 0.8600 - precision: 0.1399 - recall: 0.9815 - f1_metric: 0.2396 - val_loss: 0.4200 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4005 - acc: 0.8625 - precision: 0.1368 - recall: 0.9539 - f1_metric: 0.2346 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4179 - acc: 0.8542 - precision: 0.1453 - recall: 0.9361 - f1_metric: 0.2469 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4054 - acc: 0.8620 - precision: 0.1402 - recall: 0.9722 - f1_metric: 0.2393 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4003 - acc: 0.8632 - precision: 0.1358 - recall: 0.9167 - f1_metric: 0.2321 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4095 - acc: 0.8588 - precision: 0.1385 - recall: 0.9297 - f1_metric: 0.2353 - val_loss: 0.4172 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4057 - acc: 0.8607 - precision: 0.1382 - recall: 0.9443 - f1_metric: 0.2362 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3983 - acc: 0.8638 - precision: 0.1344 - recall: 0.9346 - f1_metric: 0.2301 - val_loss: 0.4179 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4163 - acc: 0.8540 - precision: 0.1463 - recall: 0.9605 - f1_metric: 0.2473 - val_loss: 0.4221 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4093 - acc: 0.8574 - precision: 0.1429 - recall: 0.9812 - f1_metric: 0.2443 - val_loss: 0.4153 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4068 - acc: 0.8578 - precision: 0.1423 - recall: 0.9964 - f1_metric: 0.2444 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4040 - acc: 0.8616 - precision: 0.1387 - recall: 0.9972 - f1_metric: 0.2387 - val_loss: 0.4150 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3914 - acc: 0.8659 - precision: 0.1341 - recall: 0.9900 - f1_metric: 0.2312 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4056 - acc: 0.8579 - precision: 0.1421 - recall: 0.9795 - f1_metric: 0.2437 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4029 - acc: 0.8600 - precision: 0.1400 - recall: 0.9821 - f1_metric: 0.2404 - val_loss: 0.4145 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4109 - acc: 0.8547 - precision: 0.1453 - recall: 0.9947 - f1_metric: 0.2488 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4035 - acc: 0.8594 - precision: 0.1406 - recall: 0.9840 - f1_metric: 0.2412 - val_loss: 0.4145 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3981 - acc: 0.8621 - precision: 0.1379 - recall: 0.9977 - f1_metric: 0.2379 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3936 - acc: 0.8634 - precision: 0.1366 - recall: 1.0000 - f1_metric: 0.2356 - val_loss: 0.4123 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4128 - acc: 0.8520 - precision: 0.1480 - recall: 0.9886 - f1_metric: 0.2529 - val_loss: 0.4128 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8599 - precision: 0.1401 - recall: 0.9750 - f1_metric: 0.2407 - val_loss: 0.4107 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4003 - acc: 0.8577 - precision: 0.1423 - recall: 0.9971 - f1_metric: 0.2434 - val_loss: 0.4106 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8607 - precision: 0.1393 - recall: 0.9949 - f1_metric: 0.2399 - val_loss: 0.4093 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4072 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2472 - val_loss: 0.4094 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4068 - acc: 0.8542 - precision: 0.1458 - recall: 0.9882 - f1_metric: 0.2491 - val_loss: 0.4056 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8568 - precision: 0.1432 - recall: 1.0000 - f1_metric: 0.2463 - val_loss: 0.4058 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3939 - acc: 0.8576 - precision: 0.1424 - recall: 1.0000 - f1_metric: 0.2454 - val_loss: 0.4020 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3849 - acc: 0.8626 - precision: 0.1374 - recall: 0.9992 - f1_metric: 0.2372 - val_loss: 0.4037 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3863 - acc: 0.8610 - precision: 0.1390 - recall: 0.9915 - f1_metric: 0.2396 - val_loss: 0.3984 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 17 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.4803 - acc: 0.8312 - precision: 0.1316 - recall: 0.8886 - f1_metric: 0.2242 - val_loss: 0.4179 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4144 - acc: 0.8585 - precision: 0.1415 - recall: 0.9882 - f1_metric: 0.2429 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4101 - acc: 0.8605 - precision: 0.1395 - recall: 0.9963 - f1_metric: 0.2397 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4055 - acc: 0.8613 - precision: 0.1387 - recall: 0.9870 - f1_metric: 0.2384 - val_loss: 0.4184 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8605 - precision: 0.1395 - recall: 0.9998 - f1_metric: 0.2404 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4069 - acc: 0.8619 - precision: 0.1381 - recall: 0.9980 - f1_metric: 0.2381 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4079 - acc: 0.8581 - precision: 0.1419 - recall: 0.9965 - f1_metric: 0.2433 - val_loss: 0.4163 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4054 - acc: 0.8596 - precision: 0.1404 - recall: 0.9982 - f1_metric: 0.2419 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.8599 - precision: 0.1401 - recall: 0.9889 - f1_metric: 0.2405 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4106 - acc: 0.8559 - precision: 0.1441 - recall: 0.9970 - f1_metric: 0.2468 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3983 - acc: 0.8624 - precision: 0.1376 - recall: 1.0000 - f1_metric: 0.2371 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4057 - acc: 0.8580 - precision: 0.1420 - recall: 1.0000 - f1_metric: 0.2433 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4014 - acc: 0.8614 - precision: 0.1386 - recall: 1.0000 - f1_metric: 0.2392 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4102 - acc: 0.8557 - precision: 0.1443 - recall: 0.9932 - f1_metric: 0.2471 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4021 - acc: 0.8609 - precision: 0.1391 - recall: 0.9815 - f1_metric: 0.2386 - val_loss: 0.4141 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4123 - acc: 0.8536 - precision: 0.1464 - recall: 1.0000 - f1_metric: 0.2503 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3986 - acc: 0.8611 - precision: 0.1389 - recall: 0.9672 - f1_metric: 0.2386 - val_loss: 0.4130 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4007 - acc: 0.8597 - precision: 0.1403 - recall: 0.9867 - f1_metric: 0.2405 - val_loss: 0.4126 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3918 - acc: 0.8643 - precision: 0.1356 - recall: 0.9935 - f1_metric: 0.2355 - val_loss: 0.4134 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4079 - acc: 0.8563 - precision: 0.1437 - recall: 0.9924 - f1_metric: 0.2461 - val_loss: 0.4122 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3870 - acc: 0.8659 - precision: 0.1341 - recall: 0.9928 - f1_metric: 0.2311 - val_loss: 0.4109 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3966 - acc: 0.8610 - precision: 0.1390 - recall: 0.9904 - f1_metric: 0.2391 - val_loss: 0.4098 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8577 - precision: 0.1423 - recall: 0.9898 - f1_metric: 0.2436 - val_loss: 0.4069 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4051 - acc: 0.8552 - precision: 0.1448 - recall: 0.9965 - f1_metric: 0.2480 - val_loss: 0.4047 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3946 - acc: 0.8613 - precision: 0.1387 - recall: 0.9923 - f1_metric: 0.2384 - val_loss: 0.4116 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8599 - precision: 0.1401 - recall: 0.9708 - f1_metric: 0.2397 - val_loss: 0.4026 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3959 - acc: 0.8556 - precision: 0.1445 - recall: 0.9997 - f1_metric: 0.2482 - val_loss: 0.4024 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8579 - precision: 0.1421 - recall: 0.9943 - f1_metric: 0.2441 - val_loss: 0.3956 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3775 - acc: 0.8651 - precision: 0.1349 - recall: 0.9985 - f1_metric: 0.2328 - val_loss: 0.3946 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8527 - precision: 0.1473 - recall: 1.0000 - f1_metric: 0.2520 - val_loss: 0.3943 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 18 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 18ms/step - loss: 0.4940 - acc: 0.8235 - precision: 0.1353 - recall: 1.1312 - f1_metric: 0.2366 - val_loss: 0.4190 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4049 - acc: 0.8636 - precision: 0.1364 - recall: 0.9932 - f1_metric: 0.2355 - val_loss: 0.4182 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4104 - acc: 0.8600 - precision: 0.1401 - recall: 0.9902 - f1_metric: 0.2402 - val_loss: 0.4185 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4003 - acc: 0.8651 - precision: 0.1349 - recall: 0.9838 - f1_metric: 0.2319 - val_loss: 0.4169 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4188 - acc: 0.8538 - precision: 0.1464 - recall: 0.9973 - f1_metric: 0.2502 - val_loss: 0.4226 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3941 - acc: 0.8666 - precision: 0.1334 - recall: 0.9873 - f1_metric: 0.2299 - val_loss: 0.4176 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3937 - acc: 0.8646 - precision: 0.1354 - recall: 0.9895 - f1_metric: 0.2337 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4002 - acc: 0.8611 - precision: 0.1389 - recall: 0.9968 - f1_metric: 0.2399 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4056 - acc: 0.8587 - precision: 0.1413 - recall: 1.0000 - f1_metric: 0.2424 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4108 - acc: 0.8561 - precision: 0.1439 - recall: 0.9927 - f1_metric: 0.2462 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4135 - acc: 0.8565 - precision: 0.1435 - recall: 0.9894 - f1_metric: 0.2457 - val_loss: 0.4152 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4065 - acc: 0.8597 - precision: 0.1403 - recall: 1.0004 - f1_metric: 0.2411 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4039 - acc: 0.8600 - precision: 0.1399 - recall: 0.9590 - f1_metric: 0.2396 - val_loss: 0.4149 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4102 - acc: 0.8561 - precision: 0.1439 - recall: 0.9991 - f1_metric: 0.2464 - val_loss: 0.4141 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.8594 - precision: 0.1406 - recall: 0.9988 - f1_metric: 0.2420 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4193 - acc: 0.8500 - precision: 0.1500 - recall: 0.9966 - f1_metric: 0.2544 - val_loss: 0.4181 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4039 - acc: 0.8594 - precision: 0.1406 - recall: 0.9872 - f1_metric: 0.2407 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8604 - precision: 0.1396 - recall: 0.9987 - f1_metric: 0.2397 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4027 - acc: 0.8585 - precision: 0.1415 - recall: 0.9979 - f1_metric: 0.2437 - val_loss: 0.4124 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3926 - acc: 0.8647 - precision: 0.1353 - recall: 0.9931 - f1_metric: 0.2340 - val_loss: 0.4115 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4075 - acc: 0.8538 - precision: 0.1462 - recall: 0.9908 - f1_metric: 0.2501 - val_loss: 0.4109 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3980 - acc: 0.8612 - precision: 0.1388 - recall: 0.9794 - f1_metric: 0.2383 - val_loss: 0.4098 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3995 - acc: 0.8593 - precision: 0.1407 - recall: 0.9814 - f1_metric: 0.2420 - val_loss: 0.4108 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3936 - acc: 0.8612 - precision: 0.1388 - recall: 0.9971 - f1_metric: 0.2395 - val_loss: 0.4077 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3855 - acc: 0.8649 - precision: 0.1351 - recall: 0.9944 - f1_metric: 0.2330 - val_loss: 0.4049 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3952 - acc: 0.8577 - precision: 0.1423 - recall: 0.9991 - f1_metric: 0.2444 - val_loss: 0.4036 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3818 - acc: 0.8660 - precision: 0.1340 - recall: 0.9728 - f1_metric: 0.2299 - val_loss: 0.4003 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3966 - acc: 0.8575 - precision: 0.1425 - recall: 0.9948 - f1_metric: 0.2443 - val_loss: 0.3984 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3956 - acc: 0.8541 - precision: 0.1459 - recall: 1.0000 - f1_metric: 0.2498 - val_loss: 0.3960 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3816 - acc: 0.8624 - precision: 0.1376 - recall: 0.9689 - f1_metric: 0.2362 - val_loss: 0.3903 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 19 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 12ms/step - loss: 0.5024 - acc: 0.8386 - precision: 0.1329 - recall: 0.8646 - f1_metric: 0.2220 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4168 - acc: 0.8563 - precision: 0.1436 - recall: 0.9606 - f1_metric: 0.2443 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4093 - acc: 0.8586 - precision: 0.1413 - recall: 0.9916 - f1_metric: 0.2433 - val_loss: 0.4183 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4087 - acc: 0.8596 - precision: 0.1404 - recall: 0.9972 - f1_metric: 0.2416 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4043 - acc: 0.8613 - precision: 0.1387 - recall: 0.9986 - f1_metric: 0.2395 - val_loss: 0.4174 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4027 - acc: 0.8602 - precision: 0.1398 - recall: 1.0000 - f1_metric: 0.2402 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4173 - acc: 0.8524 - precision: 0.1476 - recall: 0.9978 - f1_metric: 0.2521 - val_loss: 0.4165 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3907 - acc: 0.8667 - precision: 0.1333 - recall: 0.9972 - f1_metric: 0.2297 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4032 - acc: 0.8609 - precision: 0.1390 - recall: 0.9978 - f1_metric: 0.2390 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4032 - acc: 0.8606 - precision: 0.1394 - recall: 0.9885 - f1_metric: 0.2395 - val_loss: 0.4152 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3873 - acc: 0.8678 - precision: 0.1322 - recall: 0.9981 - f1_metric: 0.2295 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4078 - acc: 0.8560 - precision: 0.1440 - recall: 0.9819 - f1_metric: 0.2470 - val_loss: 0.4179 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4054 - acc: 0.8594 - precision: 0.1406 - recall: 0.9725 - f1_metric: 0.2410 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3925 - acc: 0.8652 - precision: 0.1347 - recall: 0.9933 - f1_metric: 0.2318 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4091 - acc: 0.8562 - precision: 0.1438 - recall: 0.9883 - f1_metric: 0.2471 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3992 - acc: 0.8621 - precision: 0.1379 - recall: 0.9997 - f1_metric: 0.23 - 1s 3ms/step - loss: 0.3994 - acc: 0.8619 - precision: 0.1381 - recall: 0.9992 - f1_metric: 0.2378 - val_loss: 0.4155 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3990 - acc: 0.8604 - precision: 0.1396 - recall: 0.9951 - f1_metric: 0.2396 - val_loss: 0.4126 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4064 - acc: 0.8571 - precision: 0.1429 - recall: 1.0000 - f1_metric: 0.2455 - val_loss: 0.4116 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4065 - acc: 0.8564 - precision: 0.1436 - recall: 0.9952 - f1_metric: 0.2460 - val_loss: 0.4112 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8595 - precision: 0.1405 - recall: 1.0000 - f1_metric: 0.2412 - val_loss: 0.4102 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4023 - acc: 0.8577 - precision: 0.1423 - recall: 0.9984 - f1_metric: 0.2444 - val_loss: 0.4136 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4007 - acc: 0.8581 - precision: 0.1419 - recall: 0.9993 - f1_metric: 0.2437 - val_loss: 0.4074 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3964 - acc: 0.8589 - precision: 0.1411 - recall: 0.9856 - f1_metric: 0.2423 - val_loss: 0.4055 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3868 - acc: 0.8635 - precision: 0.1365 - recall: 0.9955 - f1_metric: 0.2356 - val_loss: 0.4045 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3911 - acc: 0.8593 - precision: 0.1407 - recall: 0.9989 - f1_metric: 0.2428 - val_loss: 0.4028 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3887 - acc: 0.8614 - precision: 0.1386 - recall: 0.9913 - f1_metric: 0.2377 - val_loss: 0.3980 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3984 - acc: 0.8536 - precision: 0.1464 - recall: 0.9990 - f1_metric: 0.2512 - val_loss: 0.3973 - val_acc: 0.8507 - val_precision: 0.1468 - val_recall: 0.9808 - val_f1_metric: 0.2490\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3906 - acc: 0.8565 - precision: 0.1436 - recall: 0.9933 - f1_metric: 0.2470 - val_loss: 0.3911 - val_acc: 0.8507 - val_precision: 0.1468 - val_recall: 0.9808 - val_f1_metric: 0.2490\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3825 - acc: 0.8611 - precision: 0.1389 - recall: 0.9946 - f1_metric: 0.2388 - val_loss: 0.3937 - val_acc: 0.8507 - val_precision: 0.1468 - val_recall: 0.9808 - val_f1_metric: 0.2490\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3725 - acc: 0.8628 - precision: 0.1372 - recall: 1.0000 - f1_metric: 0.2376 - val_loss: 0.3840 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 20 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 16ms/step - loss: 0.4839 - acc: 0.8464 - precision: 0.1399 - recall: 1.2005 - f1_metric: 0.2446 - val_loss: 0.4212 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4152 - acc: 0.8571 - precision: 0.1423 - recall: 1.0031 - f1_metric: 0.2442 - val_loss: 0.4193 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3966 - acc: 0.8656 - precision: 0.1361 - recall: 1.0123 - f1_metric: 0.2362 - val_loss: 0.4170 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4082 - acc: 0.8612 - precision: 0.1375 - recall: 1.0023 - f1_metric: 0.2366 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4082 - acc: 0.8599 - precision: 0.1393 - recall: 1.0059 - f1_metric: 0.2400 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3999 - acc: 0.8642 - precision: 0.1360 - recall: 1.0034 - f1_metric: 0.2348 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4242 - acc: 0.8515 - precision: 0.1484 - recall: 0.9924 - f1_metric: 0.2534 - val_loss: 0.4163 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4079 - acc: 0.8577 - precision: 0.1425 - recall: 0.9911 - f1_metric: 0.2450 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4105 - acc: 0.8565 - precision: 0.1435 - recall: 1.0000 - f1_metric: 0.2458 - val_loss: 0.4155 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4023 - acc: 0.8626 - precision: 0.1374 - recall: 1.0019 - f1_metric: 0.2380 - val_loss: 0.4150 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4085 - acc: 0.8581 - precision: 0.1418 - recall: 1.0002 - f1_metric: 0.2446 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4032 - acc: 0.8597 - precision: 0.1402 - recall: 0.9778 - f1_metric: 0.2408 - val_loss: 0.4144 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4043 - acc: 0.8591 - precision: 0.1408 - recall: 1.0005 - f1_metric: 0.2424 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3999 - acc: 0.8603 - precision: 0.1393 - recall: 0.9970 - f1_metric: 0.2394 - val_loss: 0.4135 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4007 - acc: 0.8595 - precision: 0.1406 - recall: 0.9961 - f1_metric: 0.2414 - val_loss: 0.4131 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4004 - acc: 0.8615 - precision: 0.1385 - recall: 0.9969 - f1_metric: 0.2387 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3919 - acc: 0.8640 - precision: 0.1360 - recall: 0.9906 - f1_metric: 0.2343 - val_loss: 0.4123 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4011 - acc: 0.8575 - precision: 0.1425 - recall: 1.0000 - f1_metric: 0.2443 - val_loss: 0.4113 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3933 - acc: 0.8634 - precision: 0.1366 - recall: 0.9753 - f1_metric: 0.2352 - val_loss: 0.4097 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3873 - acc: 0.8665 - precision: 0.1335 - recall: 1.0000 - f1_metric: 0.2302 - val_loss: 0.4095 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3919 - acc: 0.8625 - precision: 0.1375 - recall: 0.9845 - f1_metric: 0.2361 - val_loss: 0.4073 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3890 - acc: 0.8631 - precision: 0.1369 - recall: 0.9999 - f1_metric: 0.2353 - val_loss: 0.4090 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3875 - acc: 0.8645 - precision: 0.1355 - recall: 0.9651 - f1_metric: 0.2331 - val_loss: 0.4043 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3845 - acc: 0.8659 - precision: 0.1341 - recall: 0.9961 - f1_metric: 0.2315 - val_loss: 0.4039 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3955 - acc: 0.8582 - precision: 0.1417 - recall: 0.9984 - f1_metric: 0.2430 - val_loss: 0.4015 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3888 - acc: 0.8596 - precision: 0.1404 - recall: 1.0000 - f1_metric: 0.2419 - val_loss: 0.4038 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3945 - acc: 0.8573 - precision: 0.1427 - recall: 0.9907 - f1_metric: 0.2447 - val_loss: 0.3990 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3789 - acc: 0.8647 - precision: 0.1353 - recall: 0.9937 - f1_metric: 0.2332 - val_loss: 0.3945 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3880 - acc: 0.8552 - precision: 0.1448 - recall: 0.9992 - f1_metric: 0.2481 - val_loss: 0.3897 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3894 - acc: 0.8549 - precision: 0.1451 - recall: 0.9910 - f1_metric: 0.2471 - val_loss: 0.3867 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 21 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.5337 - acc: 0.7650 - precision: 0.1413 - recall: 0.9095 - f1_metric: 0.2389 - val_loss: 0.4198 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4287 - acc: 0.8530 - precision: 0.1468 - recall: 0.8751 - f1_metric: 0.2469 - val_loss: 0.4180 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4071 - acc: 0.8623 - precision: 0.1333 - recall: 0.8316 - f1_metric: 0.2240 - val_loss: 0.4177 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4147 - acc: 0.8583 - precision: 0.1467 - recall: 0.8636 - f1_metric: 0.2443 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4110 - acc: 0.8595 - precision: 0.1443 - recall: 0.8581 - f1_metric: 0.2413 - val_loss: 0.4169 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4176 - acc: 0.8559 - precision: 0.1449 - recall: 0.8563 - f1_metric: 0.2415 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4125 - acc: 0.8580 - precision: 0.1414 - recall: 0.9292 - f1_metric: 0.2413 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4019 - acc: 0.8616 - precision: 0.1393 - recall: 0.9608 - f1_metric: 0.2393 - val_loss: 0.4157 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4299 - acc: 0.8454 - precision: 0.1561 - recall: 0.9321 - f1_metric: 0.2613 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3950 - acc: 0.8651 - precision: 0.1353 - recall: 0.9433 - f1_metric: 0.2323 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3956 - acc: 0.8648 - precision: 0.1357 - recall: 0.9639 - f1_metric: 0.23 - 0s 2ms/step - loss: 0.3964 - acc: 0.8643 - precision: 0.1362 - recall: 0.9647 - f1_metric: 0.2327 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8594 - precision: 0.1405 - recall: 0.9807 - f1_metric: 0.2409 - val_loss: 0.4150 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8613 - precision: 0.1388 - recall: 0.9857 - f1_metric: 0.2387 - val_loss: 0.4144 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3992 - acc: 0.8615 - precision: 0.1388 - recall: 0.9752 - f1_metric: 0.2387 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3881 - acc: 0.8682 - precision: 0.1319 - recall: 0.9972 - f1_metric: 0.2283 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4085 - acc: 0.8555 - precision: 0.1440 - recall: 0.9935 - f1_metric: 0.2467 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3917 - acc: 0.8633 - precision: 0.1370 - recall: 0.9925 - f1_metric: 0.2359 - val_loss: 0.4130 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4071 - acc: 0.8575 - precision: 0.1426 - recall: 0.9864 - f1_metric: 0.2451 - val_loss: 0.4136 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3990 - acc: 0.8596 - precision: 0.1404 - recall: 0.9955 - f1_metric: 0.2416 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3999 - acc: 0.8600 - precision: 0.1400 - recall: 0.9845 - f1_metric: 0.2413 - val_loss: 0.4131 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4134 - acc: 0.8521 - precision: 0.1479 - recall: 1.0000 - f1_metric: 0.2534 - val_loss: 0.4115 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4102 - acc: 0.8539 - precision: 0.1461 - recall: 0.9992 - f1_metric: 0.2501 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8608 - precision: 0.1392 - recall: 1.0000 - f1_metric: 0.2395 - val_loss: 0.4101 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4075 - acc: 0.8554 - precision: 0.1446 - recall: 0.9956 - f1_metric: 0.2480 - val_loss: 0.4074 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3914 - acc: 0.8618 - precision: 0.1382 - recall: 0.9797 - f1_metric: 0.2380 - val_loss: 0.4093 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3782 - acc: 0.8696 - precision: 0.1304 - recall: 0.9949 - f1_metric: 0.2269 - val_loss: 0.4054 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3767 - acc: 0.8684 - precision: 0.1316 - recall: 0.9875 - f1_metric: 0.2283 - val_loss: 0.4034 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3836 - acc: 0.8635 - precision: 0.1364 - recall: 0.9700 - f1_metric: 0.2355 - val_loss: 0.4003 - val_acc: 0.8507 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3834 - acc: 0.8630 - precision: 0.1370 - recall: 0.9893 - f1_metric: 0.2353 - val_loss: 0.3959 - val_acc: 0.8507 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3817 - acc: 0.8601 - precision: 0.1394 - recall: 0.9880 - f1_metric: 0.2402 - val_loss: 0.3936 - val_acc: 0.8501 - val_precision: 0.1465 - val_recall: 0.9808 - val_f1_metric: 0.2486\n",
      "Model 22 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 16ms/step - loss: 0.5131 - acc: 0.7912 - precision: 0.1334 - recall: 0.8817 - f1_metric: 0.2265 - val_loss: 0.4186 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4057 - acc: 0.8620 - precision: 0.1381 - recall: 0.9808 - f1_metric: 0.2372 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4023 - acc: 0.8632 - precision: 0.1368 - recall: 0.9965 - f1_metric: 0.2359 - val_loss: 0.4178 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3924 - acc: 0.8671 - precision: 0.1329 - recall: 0.9975 - f1_metric: 0.2301 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4078 - acc: 0.8586 - precision: 0.1414 - recall: 0.9999 - f1_metric: 0.2430 - val_loss: 0.4162 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4057 - acc: 0.8599 - precision: 0.1401 - recall: 1.0000 - f1_metric: 0.2419 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4081 - acc: 0.8580 - precision: 0.1420 - recall: 0.9961 - f1_metric: 0.2449 - val_loss: 0.4163 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4085 - acc: 0.8583 - precision: 0.1416 - recall: 0.9835 - f1_metric: 0.2425 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4039 - acc: 0.8598 - precision: 0.1403 - recall: 0.9721 - f1_metric: 0.2400 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4067 - acc: 0.8572 - precision: 0.1428 - recall: 0.9978 - f1_metric: 0.2451 - val_loss: 0.4152 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4122 - acc: 0.8543 - precision: 0.1452 - recall: 0.9902 - f1_metric: 0.2485 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4070 - acc: 0.8583 - precision: 0.1418 - recall: 1.0000 - f1_metric: 0.2438 - val_loss: 0.4145 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4066 - acc: 0.8561 - precision: 0.1437 - recall: 0.9934 - f1_metric: 0.2464 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4123 - acc: 0.8553 - precision: 0.1448 - recall: 0.9989 - f1_metric: 0.2477 - val_loss: 0.4150 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8592 - precision: 0.1408 - recall: 0.9985 - f1_metric: 0.2412 - val_loss: 0.4132 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4048 - acc: 0.8585 - precision: 0.1415 - recall: 0.9925 - f1_metric: 0.2428 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3994 - acc: 0.8606 - precision: 0.1394 - recall: 1.0000 - f1_metric: 0.2390 - val_loss: 0.4122 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3982 - acc: 0.8608 - precision: 0.1392 - recall: 0.9902 - f1_metric: 0.2389 - val_loss: 0.4118 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4001 - acc: 0.8609 - precision: 0.1391 - recall: 0.9955 - f1_metric: 0.2390 - val_loss: 0.4107 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3911 - acc: 0.8633 - precision: 0.1367 - recall: 0.9924 - f1_metric: 0.2350 - val_loss: 0.4107 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4008 - acc: 0.8588 - precision: 0.1412 - recall: 1.0000 - f1_metric: 0.2425 - val_loss: 0.4093 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3990 - acc: 0.8592 - precision: 0.1408 - recall: 0.9985 - f1_metric: 0.2422 - val_loss: 0.4089 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4056 - acc: 0.8550 - precision: 0.1450 - recall: 1.0000 - f1_metric: 0.2489 - val_loss: 0.4073 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3989 - acc: 0.8579 - precision: 0.1421 - recall: 1.0000 - f1_metric: 0.2448 - val_loss: 0.4060 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3917 - acc: 0.8607 - precision: 0.1393 - recall: 0.9924 - f1_metric: 0.2388 - val_loss: 0.4053 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3952 - acc: 0.8583 - precision: 0.1417 - recall: 1.0000 - f1_metric: 0.2438 - val_loss: 0.4004 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3964 - acc: 0.8569 - precision: 0.1431 - recall: 0.9868 - f1_metric: 0.2450 - val_loss: 0.3993 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3962 - acc: 0.8556 - precision: 0.1444 - recall: 0.9860 - f1_metric: 0.2465 - val_loss: 0.3977 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3817 - acc: 0.8616 - precision: 0.1384 - recall: 0.9989 - f1_metric: 0.2381 - val_loss: 0.3932 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3779 - acc: 0.8635 - precision: 0.1365 - recall: 0.9839 - f1_metric: 0.2345 - val_loss: 0.3911 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 23 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.4894 - acc: 0.8465 - precision: 0.1359 - recall: 0.8100 - f1_metric: 0.2169 - val_loss: 0.4222 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4080 - acc: 0.8602 - precision: 0.1401 - recall: 0.9728 - f1_metric: 0.2412 - val_loss: 0.4187 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4036 - acc: 0.8639 - precision: 0.1360 - recall: 0.9949 - f1_metric: 0.2334 - val_loss: 0.4169 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4052 - acc: 0.8601 - precision: 0.1400 - recall: 0.9998 - f1_metric: 0.2407 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4130 - acc: 0.8571 - precision: 0.1428 - recall: 0.9961 - f1_metric: 0.2452 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4012 - acc: 0.8647 - precision: 0.1355 - recall: 0.9713 - f1_metric: 0.2325 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4058 - acc: 0.8611 - precision: 0.1390 - recall: 1.0000 - f1_metric: 0.2395 - val_loss: 0.4165 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3995 - acc: 0.8638 - precision: 0.1362 - recall: 0.9894 - f1_metric: 0.2347 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4050 - acc: 0.8593 - precision: 0.1408 - recall: 0.9877 - f1_metric: 0.2425 - val_loss: 0.4174 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4127 - acc: 0.8545 - precision: 0.1455 - recall: 0.9826 - f1_metric: 0.2483 - val_loss: 0.4150 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3996 - acc: 0.8610 - precision: 0.1390 - recall: 0.9802 - f1_metric: 0.2389 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4051 - acc: 0.8593 - precision: 0.1407 - recall: 0.9976 - f1_metric: 0.2416 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3931 - acc: 0.8649 - precision: 0.1351 - recall: 0.9847 - f1_metric: 0.2332 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4074 - acc: 0.8561 - precision: 0.1441 - recall: 0.9738 - f1_metric: 0.2452 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4039 - acc: 0.8596 - precision: 0.1404 - recall: 1.0000 - f1_metric: 0.2413 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4001 - acc: 0.8607 - precision: 0.1393 - recall: 0.9989 - f1_metric: 0.2391 - val_loss: 0.4123 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4035 - acc: 0.8570 - precision: 0.1430 - recall: 0.9937 - f1_metric: 0.2458 - val_loss: 0.4129 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4155 - acc: 0.8516 - precision: 0.1484 - recall: 0.9989 - f1_metric: 0.2531 - val_loss: 0.4124 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3825 - acc: 0.8689 - precision: 0.1312 - recall: 0.9998 - f1_metric: 0.2279 - val_loss: 0.4163 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4065 - acc: 0.8583 - precision: 0.1417 - recall: 0.9855 - f1_metric: 0.2431 - val_loss: 0.4107 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4106 - acc: 0.8533 - precision: 0.1467 - recall: 1.0000 - f1_metric: 0.2521 - val_loss: 0.4084 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3865 - acc: 0.8657 - precision: 0.1343 - recall: 0.9931 - f1_metric: 0.2321 - val_loss: 0.4111 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3966 - acc: 0.8606 - precision: 0.1394 - recall: 0.9934 - f1_metric: 0.2397 - val_loss: 0.4054 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3980 - acc: 0.8574 - precision: 0.1426 - recall: 0.9926 - f1_metric: 0.2458 - val_loss: 0.4045 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3898 - acc: 0.8627 - precision: 0.1373 - recall: 0.9943 - f1_metric: 0.2365 - val_loss: 0.4033 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3780 - acc: 0.8676 - precision: 0.1324 - recall: 0.9906 - f1_metric: 0.2282 - val_loss: 0.4006 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3790 - acc: 0.8670 - precision: 0.1330 - recall: 0.9818 - f1_metric: 0.2299 - val_loss: 0.3968 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3891 - acc: 0.8582 - precision: 0.1418 - recall: 0.9841 - f1_metric: 0.2431 - val_loss: 0.3965 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3864 - acc: 0.8602 - precision: 0.1398 - recall: 0.9801 - f1_metric: 0.2404 - val_loss: 0.3933 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3814 - acc: 0.8583 - precision: 0.1417 - recall: 0.9962 - f1_metric: 0.2435 - val_loss: 0.3909 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 24 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 15ms/step - loss: 0.4959 - acc: 0.7969 - precision: 0.1330 - recall: 1.0391 - f1_metric: 0.2302 - val_loss: 0.4187 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4005 - acc: 0.8625 - precision: 0.1375 - recall: 0.9858 - f1_metric: 0.2360 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4025 - acc: 0.8631 - precision: 0.1369 - recall: 1.0000 - f1_metric: 0.2359 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4154 - acc: 0.8539 - precision: 0.1461 - recall: 1.0000 - f1_metric: 0.2509 - val_loss: 0.4176 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4002 - acc: 0.8648 - precision: 0.1352 - recall: 0.9920 - f1_metric: 0.2334 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.4032 - acc: 0.8607 - precision: 0.1393 - recall: 1.0000 - f1_metric: 0.2395 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4064 - acc: 0.8602 - precision: 0.1398 - recall: 0.9972 - f1_metric: 0.2399 - val_loss: 0.4185 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3913 - acc: 0.8672 - precision: 0.1328 - recall: 0.9878 - f1_metric: 0.2292 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4150 - acc: 0.8545 - precision: 0.1455 - recall: 0.9963 - f1_metric: 0.2496 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4091 - acc: 0.8565 - precision: 0.1435 - recall: 0.9878 - f1_metric: 0.2457 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4073 - acc: 0.8581 - precision: 0.1419 - recall: 0.9899 - f1_metric: 0.2436 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3986 - acc: 0.8636 - precision: 0.1364 - recall: 0.9983 - f1_metric: 0.2341 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3905 - acc: 0.8665 - precision: 0.1335 - recall: 0.9990 - f1_metric: 0.2319 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3959 - acc: 0.8626 - precision: 0.1374 - recall: 1.0000 - f1_metric: 0.2367 - val_loss: 0.4137 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3960 - acc: 0.8618 - precision: 0.1382 - recall: 0.9903 - f1_metric: 0.2372 - val_loss: 0.4153 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3992 - acc: 0.8608 - precision: 0.1391 - recall: 0.9961 - f1_metric: 0.2396 - val_loss: 0.4153 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.8580 - precision: 0.1420 - recall: 0.9915 - f1_metric: 0.2436 - val_loss: 0.4123 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3921 - acc: 0.8649 - precision: 0.1351 - recall: 0.9895 - f1_metric: 0.2329 - val_loss: 0.4116 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4010 - acc: 0.8579 - precision: 0.1421 - recall: 0.9969 - f1_metric: 0.2437 - val_loss: 0.4110 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4057 - acc: 0.8557 - precision: 0.1443 - recall: 1.0000 - f1_metric: 0.2476 - val_loss: 0.4111 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3844 - acc: 0.8683 - precision: 0.1317 - recall: 0.9816 - f1_metric: 0.2271 - val_loss: 0.4089 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4010 - acc: 0.8589 - precision: 0.1411 - recall: 0.9917 - f1_metric: 0.2433 - val_loss: 0.4087 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4106 - acc: 0.8518 - precision: 0.1482 - recall: 0.9880 - f1_metric: 0.2518 - val_loss: 0.4058 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8624 - precision: 0.1376 - recall: 0.9788 - f1_metric: 0.2365 - val_loss: 0.4038 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8569 - precision: 0.1431 - recall: 0.9825 - f1_metric: 0.2451 - val_loss: 0.4028 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4010 - acc: 0.8527 - precision: 0.1473 - recall: 0.9994 - f1_metric: 0.2521 - val_loss: 0.4020 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3817 - acc: 0.8637 - precision: 0.1363 - recall: 0.9986 - f1_metric: 0.2355 - val_loss: 0.4135 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3859 - acc: 0.8626 - precision: 0.1374 - recall: 0.9977 - f1_metric: 0.2365 - val_loss: 0.3936 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3887 - acc: 0.8574 - precision: 0.1426 - recall: 0.9981 - f1_metric: 0.2451 - val_loss: 0.3957 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3688 - acc: 0.8683 - precision: 0.1317 - recall: 0.9893 - f1_metric: 0.2281 - val_loss: 0.3860 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 25 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 12ms/step - loss: 0.5517 - acc: 0.7126 - precision: 0.1430 - recall: 1.0231 - f1_metric: 0.2454 - val_loss: 0.4198 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3994 - acc: 0.8664 - precision: 0.1337 - recall: 0.9774 - f1_metric: 0.2305 - val_loss: 0.4177 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4257 - acc: 0.8498 - precision: 0.1502 - recall: 0.9968 - f1_metric: 0.2563 - val_loss: 0.4186 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3973 - acc: 0.8650 - precision: 0.1349 - recall: 0.9986 - f1_metric: 0.2327 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4231 - acc: 0.8501 - precision: 0.1501 - recall: 1.0012 - f1_metric: 0.2560 - val_loss: 0.4192 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8656 - precision: 0.1344 - recall: 1.0000 - f1_metric: 0.2320 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4079 - acc: 0.8609 - precision: 0.1393 - recall: 0.9939 - f1_metric: 0.2386 - val_loss: 0.4193 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3984 - acc: 0.8635 - precision: 0.1365 - recall: 0.9795 - f1_metric: 0.2351 - val_loss: 0.4160 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4050 - acc: 0.8597 - precision: 0.1399 - recall: 0.9901 - f1_metric: 0.2397 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3990 - acc: 0.8634 - precision: 0.1367 - recall: 0.9987 - f1_metric: 0.2366 - val_loss: 0.4174 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4049 - acc: 0.8591 - precision: 0.1410 - recall: 0.9794 - f1_metric: 0.2417 - val_loss: 0.4153 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4122 - acc: 0.8557 - precision: 0.1443 - recall: 0.9993 - f1_metric: 0.2461 - val_loss: 0.4169 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4032 - acc: 0.8592 - precision: 0.1406 - recall: 0.9898 - f1_metric: 0.2412 - val_loss: 0.4149 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3916 - acc: 0.8663 - precision: 0.1337 - recall: 0.9913 - f1_metric: 0.2316 - val_loss: 0.4144 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3999 - acc: 0.8609 - precision: 0.1391 - recall: 0.9905 - f1_metric: 0.2391 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4108 - acc: 0.8550 - precision: 0.1450 - recall: 0.9958 - f1_metric: 0.2487 - val_loss: 0.4138 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3943 - acc: 0.8651 - precision: 0.1349 - recall: 1.0000 - f1_metric: 0.2330 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3883 - acc: 0.8651 - precision: 0.1349 - recall: 0.9880 - f1_metric: 0.2326 - val_loss: 0.4134 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3897 - acc: 0.8676 - precision: 0.1324 - recall: 0.9858 - f1_metric: 0.2289 - val_loss: 0.4124 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4020 - acc: 0.8591 - precision: 0.1409 - recall: 0.9807 - f1_metric: 0.2421 - val_loss: 0.4132 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4032 - acc: 0.8581 - precision: 0.1419 - recall: 0.9808 - f1_metric: 0.2439 - val_loss: 0.4127 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3957 - acc: 0.8619 - precision: 0.1381 - recall: 0.9984 - f1_metric: 0.2372 - val_loss: 0.4116 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3945 - acc: 0.8605 - precision: 0.1395 - recall: 1.0000 - f1_metric: 0.2402 - val_loss: 0.4095 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4033 - acc: 0.8554 - precision: 0.1446 - recall: 0.9987 - f1_metric: 0.2469 - val_loss: 0.4094 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3869 - acc: 0.8648 - precision: 0.1352 - recall: 0.9836 - f1_metric: 0.2333 - val_loss: 0.4079 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3883 - acc: 0.8647 - precision: 0.1353 - recall: 0.9938 - f1_metric: 0.2330 - val_loss: 0.4067 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3907 - acc: 0.8610 - precision: 0.1390 - recall: 1.0000 - f1_metric: 0.2382 - val_loss: 0.4039 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3897 - acc: 0.8609 - precision: 0.1391 - recall: 0.9985 - f1_metric: 0.2400 - val_loss: 0.4016 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3852 - acc: 0.8627 - precision: 0.1373 - recall: 0.9579 - f1_metric: 0.2350 - val_loss: 0.3987 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3777 - acc: 0.8648 - precision: 0.1352 - recall: 0.9949 - f1_metric: 0.2343 - val_loss: 0.3959 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 26 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 15ms/step - loss: 0.5124 - acc: 0.8082 - precision: 0.1334 - recall: 0.7733 - f1_metric: 0.2186 - val_loss: 0.4194 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4060 - acc: 0.8612 - precision: 0.1388 - recall: 0.9643 - f1_metric: 0.2380 - val_loss: 0.4178 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4159 - acc: 0.8546 - precision: 0.1458 - recall: 0.9986 - f1_metric: 0.2500 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4121 - acc: 0.8578 - precision: 0.1423 - recall: 0.9924 - f1_metric: 0.2438 - val_loss: 0.4169 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4150 - acc: 0.8565 - precision: 0.1435 - recall: 0.9892 - f1_metric: 0.2462 - val_loss: 0.4196 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4025 - acc: 0.8623 - precision: 0.1378 - recall: 0.9938 - f1_metric: 0.2369 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4121 - acc: 0.8563 - precision: 0.1439 - recall: 0.9873 - f1_metric: 0.2467 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3974 - acc: 0.8653 - precision: 0.1347 - recall: 0.9938 - f1_metric: 0.2323 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3962 - acc: 0.8630 - precision: 0.1370 - recall: 0.9846 - f1_metric: 0.2367 - val_loss: 0.4163 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4139 - acc: 0.8561 - precision: 0.1442 - recall: 0.9819 - f1_metric: 0.2472 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8627 - precision: 0.1373 - recall: 0.9800 - f1_metric: 0.2365 - val_loss: 0.4154 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3917 - acc: 0.8671 - precision: 0.1326 - recall: 0.9821 - f1_metric: 0.2284 - val_loss: 0.4153 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3847 - acc: 0.8713 - precision: 0.1287 - recall: 0.9827 - f1_metric: 0.2222 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3876 - acc: 0.8679 - precision: 0.1320 - recall: 0.9989 - f1_metric: 0.2286 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3968 - acc: 0.8647 - precision: 0.1353 - recall: 0.9626 - f1_metric: 0.2331 - val_loss: 0.4144 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4024 - acc: 0.8616 - precision: 0.1384 - recall: 1.0000 - f1_metric: 0.2383 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4145 - acc: 0.8528 - precision: 0.1472 - recall: 0.9923 - f1_metric: 0.2522 - val_loss: 0.4136 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4096 - acc: 0.8559 - precision: 0.1441 - recall: 0.9993 - f1_metric: 0.2475 - val_loss: 0.4136 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3898 - acc: 0.8666 - precision: 0.1334 - recall: 0.9985 - f1_metric: 0.2299 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3855 - acc: 0.8680 - precision: 0.1321 - recall: 0.9779 - f1_metric: 0.2272 - val_loss: 0.4125 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4071 - acc: 0.8562 - precision: 0.1438 - recall: 0.9814 - f1_metric: 0.2465 - val_loss: 0.4128 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3904 - acc: 0.8650 - precision: 0.1350 - recall: 0.9962 - f1_metric: 0.2342 - val_loss: 0.4109 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4008 - acc: 0.8580 - precision: 0.1420 - recall: 1.0000 - f1_metric: 0.2433 - val_loss: 0.4131 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3967 - acc: 0.8615 - precision: 0.1385 - recall: 0.9927 - f1_metric: 0.2390 - val_loss: 0.4094 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3912 - acc: 0.8639 - precision: 0.1361 - recall: 0.9944 - f1_metric: 0.2357 - val_loss: 0.4078 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4043 - acc: 0.8564 - precision: 0.1436 - recall: 0.9928 - f1_metric: 0.2457 - val_loss: 0.4066 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3964 - acc: 0.8572 - precision: 0.1428 - recall: 0.9952 - f1_metric: 0.2460 - val_loss: 0.4048 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4145 - acc: 0.8465 - precision: 0.1535 - recall: 0.9957 - f1_metric: 0.2608 - val_loss: 0.4116 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3967 - acc: 0.8583 - precision: 0.1417 - recall: 0.9952 - f1_metric: 0.2428 - val_loss: 0.4027 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4028 - acc: 0.8510 - precision: 0.1490 - recall: 1.0000 - f1_metric: 0.2548 - val_loss: 0.3978 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 27 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 16ms/step - loss: 0.5564 - acc: 0.7215 - precision: 0.1347 - recall: 1.1861 - f1_metric: 0.2356 - val_loss: 0.4192 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3995 - acc: 0.8652 - precision: 0.1348 - recall: 1.0000 - f1_metric: 0.2328 - val_loss: 0.4181 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4098 - acc: 0.8617 - precision: 0.1383 - recall: 0.9962 - f1_metric: 0.23 - 1s 3ms/step - loss: 0.4097 - acc: 0.8617 - precision: 0.1383 - recall: 0.9960 - f1_metric: 0.2388 - val_loss: 0.4171 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8675 - precision: 0.1325 - recall: 0.9839 - f1_metric: 0.2294 - val_loss: 0.4167 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4138 - acc: 0.8550 - precision: 0.1450 - recall: 0.9991 - f1_metric: 0.2488 - val_loss: 0.4187 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4076 - acc: 0.8595 - precision: 0.1405 - recall: 0.9774 - f1_metric: 0.2411 - val_loss: 0.4190 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4314 - acc: 0.8480 - precision: 0.1521 - recall: 1.0000 - f1_metric: 0.2589 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4038 - acc: 0.8604 - precision: 0.1396 - recall: 0.9925 - f1_metric: 0.2395 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4006 - acc: 0.8628 - precision: 0.1372 - recall: 0.9972 - f1_metric: 0.2377 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4042 - acc: 0.8600 - precision: 0.1400 - recall: 1.0000 - f1_metric: 0.2407 - val_loss: 0.4154 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3920 - acc: 0.8681 - precision: 0.1319 - recall: 0.9827 - f1_metric: 0.2285 - val_loss: 0.4152 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4202 - acc: 0.8506 - precision: 0.1494 - recall: 0.9945 - f1_metric: 0.2535 - val_loss: 0.4188 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4085 - acc: 0.8580 - precision: 0.1420 - recall: 0.9918 - f1_metric: 0.2438 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4087 - acc: 0.8572 - precision: 0.1428 - recall: 0.9984 - f1_metric: 0.2441 - val_loss: 0.4155 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3894 - acc: 0.8656 - precision: 0.1344 - recall: 0.9828 - f1_metric: 0.2321 - val_loss: 0.4143 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3868 - acc: 0.8683 - precision: 0.1317 - recall: 1.0000 - f1_metric: 0.2285 - val_loss: 0.4136 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3957 - acc: 0.8628 - precision: 0.1372 - recall: 0.9855 - f1_metric: 0.2369 - val_loss: 0.4131 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4004 - acc: 0.8596 - precision: 0.1404 - recall: 0.9905 - f1_metric: 0.2414 - val_loss: 0.4131 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3993 - acc: 0.8603 - precision: 0.1397 - recall: 0.9857 - f1_metric: 0.2390 - val_loss: 0.4145 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8583 - precision: 0.1417 - recall: 0.9961 - f1_metric: 0.2431 - val_loss: 0.4119 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3892 - acc: 0.8656 - precision: 0.1344 - recall: 0.9954 - f1_metric: 0.2325 - val_loss: 0.4111 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8618 - precision: 0.1382 - recall: 0.9745 - f1_metric: 0.2368 - val_loss: 0.4107 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8554 - precision: 0.1446 - recall: 0.9926 - f1_metric: 0.2474 - val_loss: 0.4090 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3942 - acc: 0.8609 - precision: 0.1391 - recall: 1.0000 - f1_metric: 0.2394 - val_loss: 0.4079 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3890 - acc: 0.8637 - precision: 0.1363 - recall: 0.9941 - f1_metric: 0.2340 - val_loss: 0.4062 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3830 - acc: 0.8661 - precision: 0.1339 - recall: 1.0000 - f1_metric: 0.2307 - val_loss: 0.4051 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3897 - acc: 0.8611 - precision: 0.1389 - recall: 0.9868 - f1_metric: 0.2397 - val_loss: 0.4036 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8565 - precision: 0.1435 - recall: 0.9949 - f1_metric: 0.2466 - val_loss: 0.4005 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3863 - acc: 0.8611 - precision: 0.1389 - recall: 1.0000 - f1_metric: 0.2400 - val_loss: 0.4000 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3869 - acc: 0.8633 - precision: 0.1367 - recall: 0.9880 - f1_metric: 0.2364 - val_loss: 0.3975 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 28 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.4912 - acc: 0.8157 - precision: 0.1468 - recall: 1.0340 - f1_metric: 0.2519 - val_loss: 0.4194 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4151 - acc: 0.8580 - precision: 0.1422 - recall: 0.9975 - f1_metric: 0.2440 - val_loss: 0.4193 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4022 - acc: 0.8646 - precision: 0.1356 - recall: 1.0000 - f1_metric: 0.2340 - val_loss: 0.4185 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4152 - acc: 0.8572 - precision: 0.1428 - recall: 0.9953 - f1_metric: 0.2446 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4088 - acc: 0.8571 - precision: 0.1429 - recall: 0.9871 - f1_metric: 0.2449 - val_loss: 0.4166 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4161 - acc: 0.8543 - precision: 0.1457 - recall: 0.9906 - f1_metric: 0.2484 - val_loss: 0.4165 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4177 - acc: 0.8531 - precision: 0.1469 - recall: 0.9906 - f1_metric: 0.2497 - val_loss: 0.4175 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4110 - acc: 0.8565 - precision: 0.1435 - recall: 0.9901 - f1_metric: 0.2448 - val_loss: 0.4161 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4043 - acc: 0.8607 - precision: 0.1393 - recall: 1.0000 - f1_metric: 0.2397 - val_loss: 0.4158 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4001 - acc: 0.8628 - precision: 0.1372 - recall: 1.0000 - f1_metric: 0.2361 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4023 - acc: 0.8612 - precision: 0.1388 - recall: 0.9853 - f1_metric: 0.2395 - val_loss: 0.4155 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8585 - precision: 0.1415 - recall: 0.9817 - f1_metric: 0.2419 - val_loss: 0.4149 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3984 - acc: 0.8629 - precision: 0.1371 - recall: 0.9823 - f1_metric: 0.2352 - val_loss: 0.4149 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3954 - acc: 0.8656 - precision: 0.1344 - recall: 1.0000 - f1_metric: 0.2327 - val_loss: 0.4146 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3957 - acc: 0.8641 - precision: 0.1359 - recall: 1.0000 - f1_metric: 0.2342 - val_loss: 0.4153 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4047 - acc: 0.8591 - precision: 0.1409 - recall: 1.0000 - f1_metric: 0.2421 - val_loss: 0.4140 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3987 - acc: 0.8620 - precision: 0.1380 - recall: 0.9950 - f1_metric: 0.2375 - val_loss: 0.4142 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3874 - acc: 0.8657 - precision: 0.1343 - recall: 0.9971 - f1_metric: 0.2328 - val_loss: 0.4135 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3856 - acc: 0.8682 - precision: 0.1318 - recall: 0.9752 - f1_metric: 0.2276 - val_loss: 0.4125 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4047 - acc: 0.8560 - precision: 0.1440 - recall: 1.0000 - f1_metric: 0.2459 - val_loss: 0.4119 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8610 - precision: 0.1390 - recall: 0.9883 - f1_metric: 0.2401 - val_loss: 0.4117 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3956 - acc: 0.8615 - precision: 0.1385 - recall: 0.9907 - f1_metric: 0.2395 - val_loss: 0.4099 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3975 - acc: 0.8598 - precision: 0.1402 - recall: 0.9968 - f1_metric: 0.2406 - val_loss: 0.4088 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3910 - acc: 0.8635 - precision: 0.1365 - recall: 1.0000 - f1_metric: 0.2348 - val_loss: 0.4101 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3967 - acc: 0.8587 - precision: 0.1413 - recall: 0.9947 - f1_metric: 0.2422 - val_loss: 0.4058 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3868 - acc: 0.8635 - precision: 0.1365 - recall: 0.9997 - f1_metric: 0.2361 - val_loss: 0.4045 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3942 - acc: 0.8588 - precision: 0.1412 - recall: 0.9966 - f1_metric: 0.2430 - val_loss: 0.4028 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3869 - acc: 0.8627 - precision: 0.1373 - recall: 0.9909 - f1_metric: 0.2365 - val_loss: 0.3997 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3935 - acc: 0.8592 - precision: 0.1408 - recall: 1.0000 - f1_metric: 0.2420 - val_loss: 0.3970 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3890 - acc: 0.8595 - precision: 0.1405 - recall: 1.0000 - f1_metric: 0.2410 - val_loss: 0.3975 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 29 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 17ms/step - loss: 0.4876 - acc: 0.8489 - precision: 0.1413 - recall: 1.0327 - f1_metric: 0.2446 - val_loss: 0.4210 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4134 - acc: 0.8592 - precision: 0.1413 - recall: 0.9842 - f1_metric: 0.2431 - val_loss: 0.4180 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3977 - acc: 0.8651 - precision: 0.1352 - recall: 0.9897 - f1_metric: 0.2332 - val_loss: 0.4173 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8595 - precision: 0.1404 - recall: 0.9896 - f1_metric: 0.2417 - val_loss: 0.4186 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3992 - acc: 0.8633 - precision: 0.1366 - recall: 0.9819 - f1_metric: 0.2355 - val_loss: 0.4172 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3905 - acc: 0.8666 - precision: 0.1334 - recall: 0.9963 - f1_metric: 0.2307 - val_loss: 0.4190 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4135 - acc: 0.8538 - precision: 0.1462 - recall: 0.9824 - f1_metric: 0.2504 - val_loss: 0.4169 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4090 - acc: 0.8589 - precision: 0.1413 - recall: 0.9965 - f1_metric: 0.2420 - val_loss: 0.4164 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4085 - acc: 0.8587 - precision: 0.1415 - recall: 1.0000 - f1_metric: 0.2429 - val_loss: 0.4162 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8656 - precision: 0.1345 - recall: 0.9786 - f1_metric: 0.2317 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4066 - acc: 0.8609 - precision: 0.1391 - recall: 1.0000 - f1_metric: 0.2399 - val_loss: 0.4154 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4052 - acc: 0.8586 - precision: 0.1414 - recall: 1.0000 - f1_metric: 0.2439 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3830 - acc: 0.8707 - precision: 0.1294 - recall: 1.0000 - f1_metric: 0.2249 - val_loss: 0.4159 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4002 - acc: 0.8619 - precision: 0.1382 - recall: 0.9979 - f1_metric: 0.2376 - val_loss: 0.4150 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8614 - precision: 0.1386 - recall: 0.9937 - f1_metric: 0.2394 - val_loss: 0.4147 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3976 - acc: 0.8640 - precision: 0.1360 - recall: 0.9900 - f1_metric: 0.2350 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8645 - precision: 0.1355 - recall: 0.9957 - f1_metric: 0.2343 - val_loss: 0.4144 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3921 - acc: 0.8636 - precision: 0.1364 - recall: 0.9755 - f1_metric: 0.23 - 1s 3ms/step - loss: 0.3929 - acc: 0.8632 - precision: 0.1368 - recall: 0.9766 - f1_metric: 0.2358 - val_loss: 0.4139 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3931 - acc: 0.8637 - precision: 0.1363 - recall: 0.9872 - f1_metric: 0.2334 - val_loss: 0.4132 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3873 - acc: 0.8666 - precision: 0.1334 - recall: 1.0000 - f1_metric: 0.2301 - val_loss: 0.4127 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4107 - acc: 0.8536 - precision: 0.1463 - recall: 0.9917 - f1_metric: 0.2512 - val_loss: 0.4133 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3906 - acc: 0.8639 - precision: 0.1361 - recall: 0.9865 - f1_metric: 0.2354 - val_loss: 0.4130 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4077 - acc: 0.8562 - precision: 0.1438 - recall: 0.9992 - f1_metric: 0.2472 - val_loss: 0.4148 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8596 - precision: 0.1404 - recall: 0.9855 - f1_metric: 0.2411 - val_loss: 0.4106 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3994 - acc: 0.8592 - precision: 0.1408 - recall: 0.9933 - f1_metric: 0.2410 - val_loss: 0.4087 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3905 - acc: 0.8628 - precision: 0.1372 - recall: 0.9766 - f1_metric: 0.2356 - val_loss: 0.4073 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4037 - acc: 0.8556 - precision: 0.1444 - recall: 0.9984 - f1_metric: 0.2475 - val_loss: 0.4056 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3773 - acc: 0.8686 - precision: 0.1314 - recall: 1.0000 - f1_metric: 0.2288 - val_loss: 0.4047 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4035 - acc: 0.8534 - precision: 0.1466 - recall: 0.9993 - f1_metric: 0.2508 - val_loss: 0.4019 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4038 - acc: 0.8527 - precision: 0.1473 - recall: 0.9965 - f1_metric: 0.2512 - val_loss: 0.4018 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Model 30 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.5216 - acc: 0.7852 - precision: 0.1482 - recall: 1.3332 - f1_metric: 0.2616 - val_loss: 0.4177 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4010 - acc: 0.8637 - precision: 0.1363 - recall: 0.9935 - f1_metric: 0.2358 - val_loss: 0.4169 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4156 - acc: 0.8552 - precision: 0.1440 - recall: 1.0076 - f1_metric: 0.2475 - val_loss: 0.4203 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4019 - acc: 0.8604 - precision: 0.1395 - recall: 1.0118 - f1_metric: 0.2414 - val_loss: 0.4168 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4069 - acc: 0.8607 - precision: 0.1404 - recall: 1.0099 - f1_metric: 0.2411 - val_loss: 0.4162 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4062 - acc: 0.8607 - precision: 0.1397 - recall: 1.0187 - f1_metric: 0.2406 - val_loss: 0.4183 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3964 - acc: 0.8646 - precision: 0.1354 - recall: 1.0181 - f1_metric: 0.2347 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3954 - acc: 0.8652 - precision: 0.1354 - recall: 1.0072 - f1_metric: 0.2345 - val_loss: 0.4155 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3966 - acc: 0.8636 - precision: 0.1379 - recall: 1.0246 - f1_metric: 0.2391 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3998 - acc: 0.8622 - precision: 0.1389 - recall: 1.0402 - f1_metric: 0.2403 - val_loss: 0.4152 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3911 - acc: 0.8664 - precision: 0.1339 - recall: 1.0280 - f1_metric: 0.2319 - val_loss: 0.4156 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4117 - acc: 0.8546 - precision: 0.1448 - recall: 1.0319 - f1_metric: 0.2495 - val_loss: 0.4151 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3986 - acc: 0.8613 - precision: 0.1393 - recall: 1.0217 - f1_metric: 0.2409 - val_loss: 0.4134 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4167 - acc: 0.8520 - precision: 0.1513 - recall: 1.0312 - f1_metric: 0.2583 - val_loss: 0.4131 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3994 - acc: 0.8604 - precision: 0.1398 - recall: 1.0051 - f1_metric: 0.2401 - val_loss: 0.4125 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3935 - acc: 0.8634 - precision: 0.1368 - recall: 1.0026 - f1_metric: 0.2362 - val_loss: 0.4132 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4030 - acc: 0.8582 - precision: 0.1424 - recall: 0.9977 - f1_metric: 0.2442 - val_loss: 0.4116 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3912 - acc: 0.8642 - precision: 0.1359 - recall: 1.0013 - f1_metric: 0.2341 - val_loss: 0.4109 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3817 - acc: 0.8701 - precision: 0.1299 - recall: 1.0008 - f1_metric: 0.2254 - val_loss: 0.4099 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3973 - acc: 0.8606 - precision: 0.1394 - recall: 1.0000 - f1_metric: 0.2397 - val_loss: 0.4091 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4059 - acc: 0.8559 - precision: 0.1441 - recall: 0.9735 - f1_metric: 0.2460 - val_loss: 0.4073 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3924 - acc: 0.8618 - precision: 0.1382 - recall: 0.9980 - f1_metric: 0.2382 - val_loss: 0.4061 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3814 - acc: 0.8657 - precision: 0.1343 - recall: 0.9920 - f1_metric: 0.2330 - val_loss: 0.4035 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8608 - precision: 0.1392 - recall: 1.0000 - f1_metric: 0.2390 - val_loss: 0.4023 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3850 - acc: 0.8613 - precision: 0.1387 - recall: 0.9941 - f1_metric: 0.2389 - val_loss: 0.4086 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3898 - acc: 0.8589 - precision: 0.1411 - recall: 1.0000 - f1_metric: 0.2422 - val_loss: 0.3960 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3768 - acc: 0.8648 - precision: 0.1352 - recall: 0.9940 - f1_metric: 0.2332 - val_loss: 0.3946 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3767 - acc: 0.8641 - precision: 0.1358 - recall: 0.9946 - f1_metric: 0.2340 - val_loss: 0.3979 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3762 - acc: 0.8635 - precision: 0.1365 - recall: 0.9997 - f1_metric: 0.2357 - val_loss: 0.3854 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3722 - acc: 0.8625 - precision: 0.1375 - recall: 0.9818 - f1_metric: 0.2348 - val_loss: 0.3830 - val_acc: 0.8507 - val_precision: 0.1466 - val_recall: 0.9808 - val_f1_metric: 0.2488\n"
     ]
    }
   ],
   "source": [
    "#Run the model\n",
    "\n",
    "print(\"Model 1 Fitting\")\n",
    "history1 = model1.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), class_weight = weights, callbacks=[tensorboard_callback])\n",
    "print(\"Model 2 Fitting\")\n",
    "history2 = model2.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 3 Fitting\")\n",
    "history3 = model3.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 4 Fitting\")\n",
    "history4 = model4.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 5 Fitting\")\n",
    "history5 = model5.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 6 Fitting\")\n",
    "history6 = model6.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 7 Fitting\")\n",
    "history7 = model7.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 8 Fitting\")\n",
    "history8 = model8.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 9 Fitting\")\n",
    "history9 = model9.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 10 Fitting\")\n",
    "history10 = model10.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 11 Fitting\")\n",
    "history11 = model11.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 12 Fitting\")\n",
    "history12 = model12.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 13 Fitting\")\n",
    "history13 = model13.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 14 Fitting\")\n",
    "history14 = model14.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 15 Fitting\")\n",
    "history15 = model15.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 16 Fitting\")\n",
    "history16 = model16.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 17 Fitting\")\n",
    "history17 = model17.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 18 Fitting\")\n",
    "history18 = model18.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 19 Fitting\")\n",
    "history19 = model19.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 20 Fitting\")\n",
    "history20 = model20.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 21 Fitting\")\n",
    "history21 = model21.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 22 Fitting\")\n",
    "history22 = model22.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 23 Fitting\")\n",
    "history23 = model23.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 24 Fitting\")\n",
    "history24 = model24.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 25 Fitting\")\n",
    "history25 = model25.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 26 Fitting\")\n",
    "history26 = model26.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 27 Fitting\")\n",
    "history27 = model27.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 28 Fitting\")\n",
    "history28 = model28.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 29 Fitting\")\n",
    "history29 = model29.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 30 Fitting\")\n",
    "history30 = model30.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GfHp9ww0FWvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6108 - acc: 0.7396 - precision: 0.1261 - recall: 0.9476 - f1_metric: 0.2157\n",
      "[0.6107940077781677, 0.7395986318588257, 0.12605886161327362, 0.9475942254066467, 0.21570830047130585]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3585 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.6107940077781677, 0.7395986318588257, 0.12605886161327362, 0.9475942254066467, 0.21570830047130585]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3637 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.6107940077781677, 0.7395986318588257, 0.12605886161327362, 0.9475942254066467, 0.21570830047130585]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3623 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.36231696605682373, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3508 - acc: 0.8678 - precision: 0.1308 - recall: 1.0000 - f1_metric: 0.2247\n",
      "[0.35083848237991333, 0.8678414225578308, 0.13081663846969604, 1.0, 0.2246522456407547]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3621 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3621315360069275, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3625 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3624584376811981, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3611 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.36106812953948975, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3620 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3619561791419983, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3610 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.36096644401550293, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3724 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3723892271518707, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3649 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.36494019627571106, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3671 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3670859932899475, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3577 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.35769766569137573, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3588 - acc: 0.8678 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.35878631472587585, 0.8678414225578308, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3658 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3658432960510254, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3596 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3595975637435913, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3627 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3626815378665924, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3583 - acc: 0.8688 - precision: 0.1310 - recall: 1.0000 - f1_metric: 0.2249\n",
      "[0.35827451944351196, 0.8688203692436218, 0.13102854788303375, 1.0, 0.22493287920951843]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3594 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.359443336725235, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3623 - acc: 0.8678 - precision: 0.1307 - recall: 1.0000 - f1_metric: 0.2245\n",
      "[0.3622572124004364, 0.8678414225578308, 0.1307426393032074, 1.0, 0.22454111278057098]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3589 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3589324653148651, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3655 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.36548274755477905, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3588 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.35881271958351135, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3668 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.366789847612381, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3651 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.36513465642929077, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3712 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.371206134557724, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3646 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.36464405059814453, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3687 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.36865124106407166, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3518 - acc: 0.8688 - precision: 0.1309 - recall: 1.0000 - f1_metric: 0.2248\n",
      "[0.3518458604812622, 0.8688203692436218, 0.13094979524612427, 1.0, 0.22481557726860046]\n"
     ]
    }
   ],
   "source": [
    "#Run model predictions\n",
    "\n",
    "# Model 1 \n",
    "prediction_features_1 = model1.predict(features_test)\n",
    "performance1 = model1.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 2 \n",
    "prediction_features_2 = model2.predict(features_test)\n",
    "performance2 = model2.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 3 \n",
    "prediction_features_3 = model3.predict(features_test)\n",
    "performance3 = model3.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 4 \n",
    "prediction_features_4 = model4.predict(features_test)\n",
    "performance4 = model4.evaluate(features_test, labels_test)\n",
    "print(performance4)\n",
    "# Model 5 \n",
    "prediction_features_5 = model5.predict(features_test)\n",
    "performance5 = model5.evaluate(features_test, labels_test)\n",
    "print(performance5)\n",
    "\n",
    "# Model 6 \n",
    "prediction_features_6 = model6.predict(features_test)\n",
    "performance6 = model6.evaluate(features_test, labels_test)\n",
    "print(performance6)\n",
    "# Model 7\n",
    "prediction_features_7 = model7.predict(features_test)\n",
    "performance7 = model7.evaluate(features_test, labels_test)\n",
    "print(performance7)\n",
    "# Model 8 \n",
    "prediction_features_8 = model8.predict(features_test)\n",
    "performance8 = model8.evaluate(features_test, labels_test)\n",
    "print(performance8)\n",
    "# Model 9\n",
    "prediction_features_9 = model9.predict(features_test)\n",
    "performance9 = model9.evaluate(features_test, labels_test)\n",
    "print(performance9)\n",
    "# Model 10\n",
    "prediction_features_10 = model10.predict(features_test)\n",
    "performance10 = model10.evaluate(features_test, labels_test)\n",
    "print(performance10)\n",
    "\n",
    "# Model 11 \n",
    "prediction_features_11 = model11.predict(features_test)\n",
    "performance11 = model11.evaluate(features_test, labels_test)\n",
    "print(performance11)\n",
    "# Model 12 \n",
    "prediction_features_12 = model12.predict(features_test)\n",
    "performance12 = model12.evaluate(features_test, labels_test)\n",
    "print(performance12)\n",
    "# Model 13 \n",
    "prediction_features_13 = model13.predict(features_test)\n",
    "performance13 = model13.evaluate(features_test, labels_test)\n",
    "print(performance13)\n",
    "# Model 14 \n",
    "prediction_features_14 = model14.predict(features_test)\n",
    "performance14 = model14.evaluate(features_test, labels_test)\n",
    "print(performance14)\n",
    "# Model 15 \n",
    "prediction_features_15 = model15.predict(features_test)\n",
    "performance15 = model15.evaluate(features_test, labels_test)\n",
    "print(performance15)\n",
    "\n",
    "# Model 16\n",
    "prediction_features_16 = model16.predict(features_test)\n",
    "performance16 = model16.evaluate(features_test, labels_test)\n",
    "print(performance16)\n",
    "# Model 17\n",
    "prediction_features_17 = model17.predict(features_test)\n",
    "performance17 = model17.evaluate(features_test, labels_test)\n",
    "print(performance17)\n",
    "# Model 18 \n",
    "prediction_features_18 = model18.predict(features_test)\n",
    "performance18 = model18.evaluate(features_test, labels_test)\n",
    "print(performance18)\n",
    "# Model 19\n",
    "prediction_features_19 = model19.predict(features_test)\n",
    "performance19 = model19.evaluate(features_test, labels_test)\n",
    "print(performance19)\n",
    "# Model 20 \n",
    "prediction_features_20 = model20.predict(features_test)\n",
    "performance20 = model20.evaluate(features_test, labels_test)\n",
    "print(performance20)\n",
    "\n",
    "# Model 21\n",
    "prediction_features_21 = model21.predict(features_test)\n",
    "performance21 = model21.evaluate(features_test, labels_test)\n",
    "print(performance21)\n",
    "# Model 22\n",
    "prediction_features_22 = model22.predict(features_test)\n",
    "performance22 = model22.evaluate(features_test, labels_test)\n",
    "print(performance22)\n",
    "# Model 23\n",
    "prediction_features_23 = model23.predict(features_test)\n",
    "performance23 = model23.evaluate(features_test, labels_test)\n",
    "print(performance23)\n",
    "# Model 24 \n",
    "prediction_features_24 = model24.predict(features_test)\n",
    "performance24 = model24.evaluate(features_test, labels_test)\n",
    "print(performance24)\n",
    "# Model 25\n",
    "prediction_features_25 = model25.predict(features_test)\n",
    "performance25 = model25.evaluate(features_test, labels_test)\n",
    "print(performance25)\n",
    "\n",
    "# Model 26 \n",
    "prediction_features_26 = model26.predict(features_test)\n",
    "performance26 = model26.evaluate(features_test, labels_test)\n",
    "print(performance26)\n",
    "# Model 27 \n",
    "prediction_features_27 = model27.predict(features_test)\n",
    "performance27 = model27.evaluate(features_test, labels_test)\n",
    "print(performance27)\n",
    "# Model 28 \n",
    "prediction_features_28 = model28.predict(features_test)\n",
    "performance28 = model28.evaluate(features_test, labels_test)\n",
    "print(performance28)\n",
    "# Model 29\n",
    "prediction_features_29 = model29.predict(features_test)\n",
    "performance29 = model29.evaluate(features_test, labels_test)\n",
    "print(performance29)\n",
    "# Model 30 \n",
    "prediction_features_30 = model30.predict(features_test)\n",
    "performance30 = model30.evaluate(features_test, labels_test)\n",
    "print(performance30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "-kn2xq4Ts7s-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Average:  0.37050689160823824\n",
      "Accuraccy Average:  0.8644150833288828\n",
      "Precision Average:  0.13077804545561472\n",
      "Recall Average:  0.9982531408468882\n",
      "F1 Average:  0.22450131823619207\n"
     ]
    }
   ],
   "source": [
    "# Averages\n",
    "\n",
    "# Loss\n",
    "loss_avg = (performance1[0] + performance2[0] + performance3[0] + performance4[0] + performance5[0] + performance6[0] + performance7[0] + performance8[0] + performance9[0] + performance10[0]\n",
    "            + performance11[0] + performance12[0] + performance13[0] + performance14[0] + performance15[0] + performance16[0] + performance17[0] + performance18[0] + performance19[0] + performance20[0]\n",
    "            + performance21[0] + performance22[0] + performance23[0] + performance24[0] + performance25[0] + performance26[0] + performance27[0] + performance28[0] + performance29[0] + performance30[0])/30\n",
    "print(\"Loss Average: \", loss_avg)\n",
    "\n",
    "# Accuracy\n",
    "acc_avg =(performance1[1] + performance2[1] + performance3[1] + performance4[1] + performance5[1] + performance6[1] + performance7[1] + performance8[1] + performance9[1] + performance10[1]\n",
    "            + performance11[1] + performance12[1] + performance13[1] + performance14[1] + performance15[1] + performance16[1] + performance17[1] + performance18[1] + performance19[1] + performance20[1]\n",
    "            + performance21[1] + performance22[1] + performance23[1] + performance24[1] + performance25[1] + performance26[1] + performance27[1] + performance28[1] + performance29[1] + performance30[1])/30\n",
    "print(\"Accuraccy Average: \", acc_avg)\n",
    "\n",
    "# Precision\n",
    "precision_avg = (performance1[2] + performance2[2] + performance3[2] + performance4[2] + performance5[2] + performance6[2] + performance7[2] + performance8[2] + performance9[2] + performance10[2]\n",
    "            + performance11[2] + performance12[2] + performance13[2] + performance14[2] + performance15[2] + performance16[2] + performance17[2] + performance18[2] + performance19[2] + performance20[2]\n",
    "            + performance21[2] + performance22[2] + performance23[2] + performance24[2] + performance25[2] + performance26[2] + performance27[2] + performance28[2] + performance29[2] + performance30[2])/30\n",
    "print(\"Precision Average: \", precision_avg)\n",
    "\n",
    "# Recall\n",
    "recall_avg = (performance1[3] + performance2[3] + performance3[3] + performance4[3] + performance5[3] + performance6[3] + performance7[3] + performance8[3] + performance9[3] + performance10[3]\n",
    "            + performance11[3] + performance12[3] + performance13[3] + performance14[3] + performance15[3] + performance16[3] + performance17[3] + performance18[3] + performance19[3] + performance20[3]\n",
    "            + performance21[3] + performance22[3] + performance23[3] + performance24[3] + performance25[3] + performance26[3] + performance27[3] + performance28[3] + performance29[3] + performance30[3])/30\n",
    "print(\"Recall Average: \", recall_avg)\n",
    "\n",
    "# f1_metric\n",
    "f1_avg = (performance1[4] + performance2[4] + performance3[4] + performance4[4] + performance5[4] + performance6[4] + performance7[4] + performance8[4] + performance9[4] + performance10[4]\n",
    "            + performance11[4] + performance12[4] + performance13[4] + performance14[4] + performance15[4] + performance16[4] + performance17[4] + performance18[4] + performance19[4] + performance20[4]\n",
    "            + performance21[4] + performance22[4] + performance23[4] + performance24[4] + performance25[4] + performance26[4] + performance27[4] + performance28[4] + performance29[4] + performance30[4])/30\n",
    "print(\"F1 Average: \", f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "XuwAUg5_KOz4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss SE: 0.045631894392276044\n",
      "Accuraccy SE:  0.02357594924616022\n",
      "Precision SE:  0.0008925580899546676\n",
      "Recall SE:  0.009567941629436904\n",
      "F1_Score_SE:  0.0016619006173885845\n"
     ]
    }
   ],
   "source": [
    "#Take the standard deviation of the model samples\n",
    "\n",
    "#Loss SE\n",
    "Loss_SE = statistics.stdev([performance1[0], performance2[0], performance3[0], performance4[0], performance5[0],\n",
    "                  performance6[0], performance7[0], performance8[0], performance9[0], performance10[0],\n",
    "                  performance11[0], performance12[0], performance13[0], performance14[0], performance15[0],\n",
    "                  performance16[0], performance17[0], performance18[0], performance19[0], performance20[0], \n",
    "                  performance21[0], performance22[0], performance23[0], performance24[0], performance25[0],\n",
    "                  performance26[0], performance27[0], performance28[0], performance29[0], performance30[0]])\n",
    "print(\"Loss SE:\", Loss_SE)\n",
    "\n",
    "#Accuracy SE\n",
    "Acc_SE = statistics.stdev([performance1[1], performance2[1], performance3[1], performance4[1], performance5[1],\n",
    "                  performance6[1], performance7[1], performance8[1], performance9[1], performance10[1],\n",
    "                  performance11[1], performance12[1], performance13[1], performance14[1], performance15[1],\n",
    "                  performance16[1], performance17[1], performance18[1], performance19[1], performance20[1], \n",
    "                  performance21[1], performance22[1], performance23[1], performance24[1], performance25[1],\n",
    "                  performance26[1], performance27[1], performance28[1], performance29[1], performance30[1]])\n",
    "print(\"Accuraccy SE: \", Acc_SE)\n",
    "\n",
    "#Precision SE\n",
    "precision_SE = statistics.stdev([performance1[2], performance2[2], performance3[2], performance4[2], performance5[2],\n",
    "                  performance6[2], performance7[2], performance8[2], performance9[2], performance10[2],\n",
    "                  performance11[2], performance12[2], performance13[2], performance14[2], performance15[2],\n",
    "                  performance16[2], performance17[2], performance18[2], performance19[2], performance20[2], \n",
    "                  performance21[2], performance22[2], performance23[2], performance24[2], performance25[2],\n",
    "                  performance26[2], performance27[2], performance28[2], performance29[2], performance30[2]])\n",
    "print(\"Precision SE: \", precision_SE)\n",
    "\n",
    "#Recall \n",
    "Recall_SE = statistics.stdev([performance1[3], performance2[3], performance3[3], performance4[3], performance5[3],\n",
    "                  performance6[3], performance7[3], performance8[3], performance9[3], performance10[3],\n",
    "                  performance11[3], performance12[3], performance13[3], performance14[3], performance15[3],\n",
    "                  performance16[3], performance17[3], performance18[3], performance19[3], performance20[3], \n",
    "                  performance21[3], performance22[3], performance23[3], performance24[3], performance25[3],\n",
    "                  performance26[3], performance27[3], performance28[3], performance29[3], performance30[3]])\n",
    "print(\"Recall SE: \", Recall_SE)\n",
    "\n",
    "#F1 Score\n",
    "F1_Score_SE = statistics.stdev([performance1[4], performance2[4], performance3[4], performance4[4], performance5[4],\n",
    "                  performance6[4], performance7[4], performance8[4], performance9[4], performance10[4],\n",
    "                  performance11[4], performance12[4], performance13[4], performance14[4], performance15[4],\n",
    "                  performance16[4], performance17[4], performance18[4], performance19[4], performance20[4], \n",
    "                  performance21[4], performance22[4], performance23[4], performance24[4], performance25[4],\n",
    "                  performance26[4], performance27[4], performance28[4], performance29[4], performance30[4]])\n",
    "print(\"F1_Score_SE: \", F1_Score_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "aMpLKJ4OKt-X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1032), started 3 days, 2:48:56 ago. (Use '!kill 1032' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3a5f06b09a3f73e3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3a5f06b09a3f73e3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tensorflow Graphics\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GA_ParliHeg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

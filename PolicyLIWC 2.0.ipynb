{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "g9kxq01msMg9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\alex\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (1.19.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (3.3.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (0.17.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (1.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alex\\anaconda3\\lib\\site-packages (from mlxtend) (50.3.1.post20201107)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\alex\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\alex\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\alex\\anaconda3\\lib\\site-packages (5.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\alex\\anaconda3\\lib\\site-packages (from h5py) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from h5py) (1.19.2)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\alex\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (50.3.1.post20201107)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (2.24.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.26.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.32.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.19.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (3.14.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (0.4.2)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (0.11.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (0.35.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard) (2.10)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.7)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\alex\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py~=2.10.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.26.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.1.post20201107)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\alex\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "### BEWARE:Tensorflow is stochastic - this means the model will not be replicated exactly. \n",
    "### Use GA_Load_Model for reproduction\n",
    "\n",
    "!pip install mlxtend\n",
    "\n",
    "!pip install h5py pyyaml\n",
    "\n",
    "!pip install tensorboard\n",
    "\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "tLftnb5sBe7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "#Load packages\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "iBsKz40OAGDU"
   },
   "outputs": [],
   "source": [
    "### Packages necessary for model construction \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks\n",
    "import datetime \n",
    "import statistics\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "FrEcBaVWVVkb"
   },
   "outputs": [],
   "source": [
    "#Read the Data\n",
    "\n",
    "UN_Data = pd.read_csv('GA_Query_CleanLIWC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 4369,
     "status": "ok",
     "timestamp": 1611639237251,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "FNUqnWTFS4y2",
    "outputId": "cf54ea17-a224-4830-8f6b-554ee9bafaf0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Policy Passed</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20075.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>822.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17021.0</td>\n",
       "      <td>98.45</td>\n",
       "      <td>...</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9289.0</td>\n",
       "      <td>98.94</td>\n",
       "      <td>...</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4059.0</td>\n",
       "      <td>98.95</td>\n",
       "      <td>...</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8210.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10210</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>98.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10211</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22700.0</td>\n",
       "      <td>98.26</td>\n",
       "      <td>...</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10212 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  Class M  Class S  Class I  Class P  Class B  Policy Passed  \\\n",
       "0      2012        1        0        0        0        0              0   \n",
       "1      2012        0        0        3        0        0              0   \n",
       "2      2003        0        0        0        0        0              0   \n",
       "3      1995        0        0        0        0        0              1   \n",
       "4      2007        0        0        0        0        0              0   \n",
       "...     ...      ...      ...      ...      ...      ...            ...   \n",
       "10207  2004        0        0        0        0        0              0   \n",
       "10208  1994        0        0        0        0        0              0   \n",
       "10209  2013        0        0        0        0        0              0   \n",
       "10210  2009        0        0        0        0        0              0   \n",
       "10211  2016        0        0        0        0        0              0   \n",
       "\n",
       "       Conflict Indicator       WC  Analytic  ...  Comma  Colon  SemiC  QMark  \\\n",
       "0                       1  20075.0     99.00  ...   4.34   0.03   0.04   0.00   \n",
       "1                       0    822.0     99.00  ...   3.04   1.70   0.00   0.00   \n",
       "2                       0    314.0     99.00  ...   3.50   0.96   0.00   0.00   \n",
       "3                       1  17021.0     98.45  ...   4.91   0.25   0.17   0.02   \n",
       "4                       0   9289.0     98.94  ...   3.80   0.16   0.15   0.00   \n",
       "...                   ...      ...       ...  ...    ...    ...    ...    ...   \n",
       "10207                   0   4059.0     98.95  ...   3.72   0.15   0.12   0.00   \n",
       "10208                   0   8210.0     99.00  ...   3.58   0.12   0.22   0.00   \n",
       "10209                   0    583.0     99.00  ...   3.09   0.86   0.00   0.69   \n",
       "10210                   0   1562.0     98.88  ...   2.82   0.19   0.45   0.00   \n",
       "10211                   0  22700.0     98.26  ...   3.69   0.04   0.03   0.04   \n",
       "\n",
       "       Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "0         0.0  1.23   0.07     0.64     0.82    0.60  \n",
       "1         0.0  0.85   0.49     0.12     4.38    1.46  \n",
       "2         0.0  2.23   0.00     0.64     3.18    2.87  \n",
       "3         0.0  1.33   0.22     0.16     0.64    2.18  \n",
       "4         0.0  0.93   0.28     0.75     1.42    1.52  \n",
       "...       ...   ...    ...      ...      ...     ...  \n",
       "10207     0.0  1.18   0.00     0.76     1.72    1.23  \n",
       "10208     0.0  1.06   0.02     0.29     1.05    1.75  \n",
       "10209     0.0  1.89   0.34     0.00     4.80    4.63  \n",
       "10210     0.0  1.34   0.00     0.77     1.66    2.18  \n",
       "10211     0.0  1.44   0.10     0.48     1.01    1.58  \n",
       "\n",
       "[10212 rows x 101 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect and Clean the Data\n",
    "\n",
    "UN_Data.head(5)\n",
    "\n",
    "UN_Data.drop(['Unnamed: 0'], axis = 1, inplace= True)\n",
    "\n",
    "UN_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.581151832460733, 1: 3.5806451612903225}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "#Balance Policy Passage\n",
    "\n",
    "# Count samples per class\n",
    "classes_zero = UN_Data[UN_Data['Policy Passed'] == 0]\n",
    "classes_one = UN_Data[UN_Data['Policy Passed'] == 1]\n",
    "\n",
    "# Convert parts into NumPy arrays for weight computation\n",
    "zero_numpy = classes_zero['Policy Passed'].to_numpy()\n",
    "one_numpy = classes_one['Policy Passed'].to_numpy()\n",
    "all_together = np.concatenate((zero_numpy, one_numpy))\n",
    "unique_classes = np.unique(all_together)\n",
    "\n",
    "# Compute weights\n",
    "weights = sklearn.utils.class_weight.compute_class_weight('balanced', unique_classes, all_together)\n",
    "weights = dict(enumerate(weights))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 4368,
     "status": "ok",
     "timestamp": 1611639237252,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "tkn1c4RqeEdO",
    "outputId": "d56ecbce-990a-427d-e279-772f8546aeac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Policy Passed</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.00000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2005.852135</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>0.009205</td>\n",
       "      <td>0.168625</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.13964</td>\n",
       "      <td>0.457697</td>\n",
       "      <td>9442.204220</td>\n",
       "      <td>98.190282</td>\n",
       "      <td>...</td>\n",
       "      <td>5.193233</td>\n",
       "      <td>0.303105</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>1.154469</td>\n",
       "      <td>0.260464</td>\n",
       "      <td>0.397916</td>\n",
       "      <td>1.744508</td>\n",
       "      <td>1.768528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.542111</td>\n",
       "      <td>0.229542</td>\n",
       "      <td>0.099521</td>\n",
       "      <td>0.861341</td>\n",
       "      <td>0.307019</td>\n",
       "      <td>0.156540</td>\n",
       "      <td>0.34663</td>\n",
       "      <td>0.498232</td>\n",
       "      <td>7786.325195</td>\n",
       "      <td>1.085791</td>\n",
       "      <td>...</td>\n",
       "      <td>3.290078</td>\n",
       "      <td>0.466117</td>\n",
       "      <td>0.190811</td>\n",
       "      <td>0.105541</td>\n",
       "      <td>0.713727</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>0.439789</td>\n",
       "      <td>1.273977</td>\n",
       "      <td>1.466890</td>\n",
       "      <td>4.337472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1993.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3838.500000</td>\n",
       "      <td>97.880000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2006.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7439.500000</td>\n",
       "      <td>98.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2012.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12438.750000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>2.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>74776.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>47.760000</td>\n",
       "      <td>22.710000</td>\n",
       "      <td>11.150000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>49.920000</td>\n",
       "      <td>22.880000</td>\n",
       "      <td>14.030000</td>\n",
       "      <td>89.620000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>213.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       Class M       Class S       Class I       Class P  \\\n",
       "count  10212.000000  10212.000000  10212.000000  10212.000000  10212.000000   \n",
       "mean    2005.852135      0.032805      0.009205      0.168625      0.051900   \n",
       "std        7.542111      0.229542      0.099521      0.861341      0.307019   \n",
       "min     1993.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     1999.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     2006.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%     2012.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max     2020.000000      9.000000      2.000000     28.000000      9.000000   \n",
       "\n",
       "            Class B  Policy Passed  Conflict Indicator            WC  \\\n",
       "count  10212.000000    10212.00000        10212.000000  10190.000000   \n",
       "mean       0.021641        0.13964            0.457697   9442.204220   \n",
       "std        0.156540        0.34663            0.498232   7786.325195   \n",
       "min        0.000000        0.00000            0.000000     44.000000   \n",
       "25%        0.000000        0.00000            0.000000   3838.500000   \n",
       "50%        0.000000        0.00000            0.000000   7439.500000   \n",
       "75%        0.000000        0.00000            1.000000  12438.750000   \n",
       "max        3.000000        1.00000            1.000000  74776.000000   \n",
       "\n",
       "           Analytic  ...         Comma         Colon         SemiC  \\\n",
       "count  10190.000000  ...  10190.000000  10190.000000  10190.000000   \n",
       "mean      98.190282  ...      5.193233      0.303105      0.144628   \n",
       "std        1.085791  ...      3.290078      0.466117      0.190811   \n",
       "min       80.460000  ...      0.210000      0.000000      0.000000   \n",
       "25%       97.880000  ...      3.980000      0.090000      0.060000   \n",
       "50%       98.440000  ...      4.440000      0.160000      0.110000   \n",
       "75%       99.000000  ...      4.990000      0.320000      0.190000   \n",
       "max       99.000000  ...     47.760000     22.710000     11.150000   \n",
       "\n",
       "              QMark        Exclam          Dash         Quote       Apostro  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.031364      0.014043      1.154469      0.260464      0.397916   \n",
       "std        0.105541      0.713727      0.643019      0.439789      1.273977   \n",
       "min        0.000000      0.000000      0.020000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.800000      0.050000      0.230000   \n",
       "50%        0.000000      0.000000      1.040000      0.130000      0.340000   \n",
       "75%        0.020000      0.000000      1.380000      0.300000      0.490000   \n",
       "max        3.400000     49.920000     22.880000     14.030000     89.620000   \n",
       "\n",
       "            Parenth        OtherP  \n",
       "count  10190.000000  10190.000000  \n",
       "mean       1.744508      1.768528  \n",
       "std        1.466890      4.337472  \n",
       "min        0.130000      0.010000  \n",
       "25%        0.860000      0.740000  \n",
       "50%        1.330000      1.170000  \n",
       "75%        2.090000      2.070000  \n",
       "max       17.900000    213.040000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect the data by key descriptive statistics\n",
    "\n",
    "UN_Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 4367,
     "status": "ok",
     "timestamp": 1611639237253,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "1c56fLBjJKA3",
    "outputId": "8b5f5028-9f16-4543-a585-e014b90e8ef7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Passed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>...</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>...</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  Class M  Class S  Class I  Class P  Class B  \\\n",
       "Policy Passed                                                      \n",
       "0              8786     8786     8786     8786     8786     8786   \n",
       "1              1426     1426     1426     1426     1426     1426   \n",
       "\n",
       "               Conflict Indicator    WC  Analytic  Clout  ...  Comma  Colon  \\\n",
       "Policy Passed                                             ...                 \n",
       "0                            8786  8764      8764   8764  ...   8764   8764   \n",
       "1                            1426  1426      1426   1426  ...   1426   1426   \n",
       "\n",
       "               SemiC  QMark  Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "Policy Passed                                                               \n",
       "0               8764   8764    8764  8764   8764     8764     8764    8764  \n",
       "1               1426   1426    1426  1426   1426     1426     1426    1426  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group the data by our label (dependent variable) of policy passage\n",
    "\n",
    "UN_Data.groupby(['Policy Passed']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Policy Passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>1.019000e+04</td>\n",
       "      <td>10212.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.345714</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.881989</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>4.345803e-04</td>\n",
       "      <td>0.13964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.252698</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.193529</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>1.748792e-03</td>\n",
       "      <td>0.34663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.026924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.473887e-07</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.159519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885374</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>6.634241e-05</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.260213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965297</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1.553934e-04</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.463364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.987100</td>\n",
       "      <td>0.022704</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>3.987904e-04</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997075</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.049344</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>9.570280e-02</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       Class M       Class S       Class I       Class P  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.345714      0.000004      0.000001      0.000021      0.000007   \n",
       "std        0.252698      0.000030      0.000017      0.000110      0.000050   \n",
       "min        0.026924      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.159519      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.260213      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.463364      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.997075      0.001033      0.000412      0.002388      0.001449   \n",
       "\n",
       "            Class B  Conflict Indicator            WC      Analytic  \\\n",
       "count  10190.000000        10190.000000  10190.000000  10190.000000   \n",
       "mean       0.000002            0.000056      0.881989      0.016961   \n",
       "std        0.000021            0.000086      0.193529      0.012448   \n",
       "min        0.000000            0.000000      0.021901      0.001303   \n",
       "25%        0.000000            0.000000      0.885374      0.007792   \n",
       "50%        0.000000            0.000000      0.965297      0.012765   \n",
       "75%        0.000000            0.000094      0.987100      0.022704   \n",
       "max        0.000514            0.000492      0.999635      0.049344   \n",
       "\n",
       "              Clout  ...         Colon         SemiC         QMark  \\\n",
       "count  10190.000000  ...  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.011428  ...      0.000081      0.000027      0.000006   \n",
       "std        0.008500  ...      0.000179      0.000070      0.000029   \n",
       "min        0.001053  ...      0.000000      0.000000      0.000000   \n",
       "25%        0.005150  ...      0.000008      0.000004      0.000000   \n",
       "50%        0.008537  ...      0.000020      0.000012      0.000000   \n",
       "75%        0.015068  ...      0.000066      0.000029      0.000001   \n",
       "max        0.040877  ...      0.005398      0.004388      0.001033   \n",
       "\n",
       "             Exclam          Dash         Quote       Apostro       Parenth  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.000003      0.000222      0.000051      0.000066      0.000417   \n",
       "std        0.000154      0.000308      0.000150      0.000281      0.000665   \n",
       "min        0.000000      0.000003      0.000000      0.000000      0.000003   \n",
       "25%        0.000000      0.000069      0.000004      0.000020      0.000073   \n",
       "50%        0.000000      0.000129      0.000013      0.000040      0.000180   \n",
       "75%        0.000000      0.000252      0.000041      0.000077      0.000431   \n",
       "max        0.010615      0.010232      0.006303      0.019591      0.008041   \n",
       "\n",
       "             OtherP  Policy Passed  \n",
       "count  1.019000e+04    10212.00000  \n",
       "mean   4.345803e-04        0.13964  \n",
       "std    1.748792e-03        0.34663  \n",
       "min    4.473887e-07        0.00000  \n",
       "25%    6.634241e-05        0.00000  \n",
       "50%    1.553934e-04        0.00000  \n",
       "75%    3.987904e-04        0.00000  \n",
       "max    9.570280e-02        1.00000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize the data \n",
    "\n",
    "UN_Data1 = tf.keras.utils.normalize(UN_Data.drop(columns = ['Policy Passed']))\n",
    "\n",
    "UN_Data1[\"Policy Passed\"] = UN_Data['Policy Passed']\n",
    "\n",
    "UN_Data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "r6BF8hmXGEnF"
   },
   "outputs": [],
   "source": [
    "#Divide our variables between the independent variables (features) and dependent variables (policy passage)\n",
    "\n",
    "labels = UN_Data1 ['Policy Passed']\n",
    "features = UN_Data1.drop(columns= ['Policy Passed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Null Values\n",
    "\n",
    "features = features.fillna(0)\n",
    "labels = labels.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1611639237254,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "4FJRWpUNH8Dk",
    "outputId": "fe514d19-f576-4030-c91f-fc2b562d2213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10212, 100)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect shape of features\n",
    "\n",
    "features = pd.get_dummies(features)\n",
    "features.shape[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "-oGgPi4CG0xx"
   },
   "outputs": [],
   "source": [
    "#Define type of feature and label values\n",
    "\n",
    "features = features.values.astype('float32')\n",
    "labels = labels.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "XOea_aP_HPSE"
   },
   "outputs": [],
   "source": [
    "#Data Sets for Training\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)\n",
    "features_train, features_validation, labels_train, labels_validation = train_test_split(features_train, labels_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "vHSYOIsh5vq4"
   },
   "outputs": [],
   "source": [
    "#Define Precision, Recall, and F1 score metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    weights = sklearn.utils.class_weight.compute_class_weight('balanced', unique_classes, all_together)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "4VQ_OqkCHkLM"
   },
   "outputs": [],
   "source": [
    "#Create your model\n",
    "\n",
    "model1 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model2 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model3 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model4 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model5 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model6 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model7 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model8 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model9 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model10 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model11 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model12 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model13 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model14 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model15 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model16 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model17 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model18 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model19 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model20 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model21 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model22 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model23 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model24 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model25 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model26 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model27 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model28 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model29 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model30 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1611639239019,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "7JTlq8aH8mzi",
    "outputId": "a7831e8c-a787-4b03-8006-c9aebd129183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "### Inspect form of model\n",
    "\n",
    "tf.keras.utils.plot_model(model1, to_file='model.png', show_shapes = True, show_dtype=True, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), \\n', 'for `pydotprint` to work.')\\n\\n\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), \n",
    "', 'for `pydotprint` to work.')\n",
    "\n",
    "'''\n",
    "\n",
    "# Make sure to have Pydot install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1611639239020,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "52Vb8b-qSIQ4",
    "outputId": "dd0ce729-358b-43ac-8d65-6c1f00ba812a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 3,794\n",
      "Trainable params: 3,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Check Trainable Parameters\n",
    "# Note: All the models are similarly structured\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Vyx2tehuTPfe"
   },
   "outputs": [],
   "source": [
    "#Set checkpoints, metrics, loss, and optimizer functions for the model\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model1.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model3.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model4.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model5.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model6.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model7.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model8.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model9.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model10.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model11.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model12.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model13.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model14.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model15.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model16.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model17.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model18.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model19.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model20.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model21.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model22.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model23.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model24.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model25.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model26.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model27.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model28.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model29.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model30.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 150145,
     "status": "error",
     "timestamp": 1611639383055,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "RhwA7DthFS72",
    "outputId": "64a70fe2-c811-4102-e25e-f708c8ee64e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Fitting\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:390: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 8s 32ms/step - loss: 0.6727 - acc: 0.6030 - precision: 0.1258 - recall: 0.4936 - f1_metric: 0.1889 - val_loss: 0.7085 - val_acc: 0.3501 - val_precision: 0.1549 - val_recall: 0.4904 - val_f1_metric: 0.2263\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6931 - acc: 0.4287 - precision: 0.1323 - recall: 0.5468 - f1_metric: 0.2036 - val_loss: 0.6705 - val_acc: 0.4186 - val_precision: 0.1441 - val_recall: 0.9657 - val_f1_metric: 0.2458\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6608 - acc: 0.5269 - precision: 0.1270 - recall: 0.8671 - f1_metric: 0.2165 - val_loss: 0.6943 - val_acc: 0.3654 - val_precision: 0.1451 - val_recall: 0.7539 - val_f1_metric: 0.2370\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6794 - acc: 0.5051 - precision: 0.1318 - recall: 0.7074 - f1_metric: 0.2172 - val_loss: 0.6885 - val_acc: 0.3929 - val_precision: 0.1441 - val_recall: 0.8364 - val_f1_metric: 0.2404\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6853 - acc: 0.4629 - precision: 0.1369 - recall: 0.6886 - f1_metric: 0.2222 - val_loss: 0.6925 - val_acc: 0.3684 - val_precision: 0.1415 - val_recall: 0.8381 - val_f1_metric: 0.2365\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6730 - acc: 0.4367 - precision: 0.1327 - recall: 0.7444 - f1_metric: 0.2199 - val_loss: 0.6652 - val_acc: 0.4694 - val_precision: 0.1425 - val_recall: 1.0206 - val_f1_metric: 0.2451\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6789 - acc: 0.4577 - precision: 0.1359 - recall: 0.6481 - f1_metric: 0.2171 - val_loss: 0.6663 - val_acc: 0.5979 - val_precision: 0.1091 - val_recall: 0.2081 - val_f1_metric: 0.1339\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6706 - acc: 0.5165 - precision: 0.1193 - recall: 0.3331 - f1_metric: 0.1693 - val_loss: 0.6709 - val_acc: 0.4957 - val_precision: 0.1462 - val_recall: 0.6187 - val_f1_metric: 0.2282\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6776 - acc: 0.5087 - precision: 0.1347 - recall: 0.5618 - f1_metric: 0.2089 - val_loss: 0.6845 - val_acc: 0.4339 - val_precision: 0.1575 - val_recall: 0.5910 - val_f1_metric: 0.2399\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6825 - acc: 0.4185 - precision: 0.1410 - recall: 0.4631 - f1_metric: 0.2063 - val_loss: 0.6761 - val_acc: 0.4510 - val_precision: 0.1397 - val_recall: 0.4062 - val_f1_metric: 0.1984\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6745 - acc: 0.4544 - precision: 0.1196 - recall: 0.3436 - f1_metric: 0.1655 - val_loss: 0.6563 - val_acc: 0.5514 - val_precision: 0.1158 - val_recall: 0.2397 - val_f1_metric: 0.1471\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6711 - acc: 0.5848 - precision: 0.1101 - recall: 0.2523 - f1_metric: 0.1406 - val_loss: 0.7135 - val_acc: 0.3623 - val_precision: 0.1616 - val_recall: 0.5465 - val_f1_metric: 0.2406\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6693 - acc: 0.4524 - precision: 0.1145 - recall: 0.2665 - f1_metric: 0.1444 - val_loss: 0.6669 - val_acc: 0.5122 - val_precision: 0.1169 - val_recall: 0.1562 - val_f1_metric: 0.1258\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6874 - acc: 0.4617 - precision: 0.1312 - recall: 0.3025 - f1_metric: 0.1719 - val_loss: 0.6829 - val_acc: 0.4394 - val_precision: 0.1133 - val_recall: 0.1192 - val_f1_metric: 0.1062\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6791 - acc: 0.4825 - precision: 0.1227 - recall: 0.2604 - f1_metric: 0.1515 - val_loss: 0.6302 - val_acc: 0.7228 - val_precision: 0.1120 - val_recall: 0.1140 - val_f1_metric: 0.1030\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6883 - acc: 0.5630 - precision: 0.0991 - recall: 0.2030 - f1_metric: 0.1224 - val_loss: 0.6738 - val_acc: 0.4774 - val_precision: 0.1349 - val_recall: 0.3351 - val_f1_metric: 0.1835\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6711 - acc: 0.5423 - precision: 0.1084 - recall: 0.2267 - f1_metric: 0.1338 - val_loss: 0.6934 - val_acc: 0.4235 - val_precision: 0.1477 - val_recall: 0.4407 - val_f1_metric: 0.2110\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6834 - acc: 0.4788 - precision: 0.1228 - recall: 0.3269 - f1_metric: 0.1711 - val_loss: 0.6510 - val_acc: 0.5838 - val_precision: 0.1181 - val_recall: 0.2973 - val_f1_metric: 0.1608\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6618 - acc: 0.6084 - precision: 0.1085 - recall: 0.3605 - f1_metric: 0.1603 - val_loss: 0.6712 - val_acc: 0.4865 - val_precision: 0.1447 - val_recall: 0.4229 - val_f1_metric: 0.2065\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6785 - acc: 0.5307 - precision: 0.1318 - recall: 0.5318 - f1_metric: 0.2040 - val_loss: 0.6265 - val_acc: 0.7264 - val_precision: 0.1143 - val_recall: 0.4170 - val_f1_metric: 0.1737\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6812 - acc: 0.5603 - precision: 0.1229 - recall: 0.4557 - f1_metric: 0.1873 - val_loss: 0.6381 - val_acc: 0.6805 - val_precision: 0.1145 - val_recall: 0.3276 - val_f1_metric: 0.1626\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6794 - acc: 0.5705 - precision: 0.1250 - recall: 0.3639 - f1_metric: 0.1806 - val_loss: 0.6447 - val_acc: 0.6420 - val_precision: 0.1121 - val_recall: 0.3276 - val_f1_metric: 0.1599\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6788 - acc: 0.5785 - precision: 0.1154 - recall: 0.3966 - f1_metric: 0.1727 - val_loss: 0.6879 - val_acc: 0.4364 - val_precision: 0.1611 - val_recall: 0.6127 - val_f1_metric: 0.2465\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6689 - acc: 0.5154 - precision: 0.1239 - recall: 0.4496 - f1_metric: 0.1863 - val_loss: 0.6595 - val_acc: 0.5453 - val_precision: 0.1437 - val_recall: 0.5362 - val_f1_metric: 0.2172\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6760 - acc: 0.5530 - precision: 0.1293 - recall: 0.5625 - f1_metric: 0.2034 - val_loss: 0.6448 - val_acc: 0.6261 - val_precision: 0.1429 - val_recall: 0.7785 - val_f1_metric: 0.2353\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6964 - acc: 0.5138 - precision: 0.1449 - recall: 0.7809 - f1_metric: 0.2381 - val_loss: 0.6492 - val_acc: 0.5716 - val_precision: 0.1500 - val_recall: 0.7570 - val_f1_metric: 0.2429\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6726 - acc: 0.5560 - precision: 0.1238 - recall: 0.6819 - f1_metric: 0.2050 - val_loss: 0.6511 - val_acc: 0.5845 - val_precision: 0.1472 - val_recall: 0.7297 - val_f1_metric: 0.2375\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6574 - acc: 0.6056 - precision: 0.1288 - recall: 0.6942 - f1_metric: 0.2116 - val_loss: 0.7196 - val_acc: 0.3856 - val_precision: 0.1531 - val_recall: 0.7840 - val_f1_metric: 0.2505\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6739 - acc: 0.4770 - precision: 0.1324 - recall: 0.6741 - f1_metric: 0.2155 - val_loss: 0.6099 - val_acc: 0.7326 - val_precision: 0.1023 - val_recall: 0.4233 - val_f1_metric: 0.1601\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.6712 - acc: 0.5579 - precision: 0.1287 - recall: 0.4965 - f1_metric: 0.1955 - val_loss: 0.6585 - val_acc: 0.5600 - val_precision: 0.1082 - val_recall: 0.2508 - val_f1_metric: 0.1455\n",
      "Model 2 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 18ms/step - loss: 0.4805 - acc: 0.8501 - precision: 0.1381 - recall: 1.0178 - f1_metric: 0.2374 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4173 - acc: 0.8544 - precision: 0.1456 - recall: 0.9944 - f1_metric: 0.2495 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3969 - acc: 0.8648 - precision: 0.1352 - recall: 0.9991 - f1_metric: 0.2338 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3842 - acc: 0.8726 - precision: 0.1274 - recall: 0.9986 - f1_metric: 0.2205 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3942 - acc: 0.8683 - precision: 0.1317 - recall: 1.0000 - f1_metric: 0.2282 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3828 - acc: 0.8726 - precision: 0.1274 - recall: 0.9809 - f1_metric: 0.2206 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4267 - acc: 0.8476 - precision: 0.1524 - recall: 0.9947 - f1_metric: 0.2599 - val_loss: 0.4201 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4005 - acc: 0.8644 - precision: 0.1356 - recall: 0.9564 - f1_metric: 0.2325 - val_loss: 0.4160 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4016 - acc: 0.8611 - precision: 0.1389 - recall: 0.9916 - f1_metric: 0.2391 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8657 - precision: 0.1343 - recall: 0.9514 - f1_metric: 0.2317 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.8597 - precision: 0.1403 - recall: 0.9967 - f1_metric: 0.2415 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3972 - acc: 0.8637 - precision: 0.1363 - recall: 0.9882 - f1_metric: 0.2358 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3985 - acc: 0.8625 - precision: 0.1375 - recall: 0.9940 - f1_metric: 0.2366 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3900 - acc: 0.8665 - precision: 0.1335 - recall: 0.9919 - f1_metric: 0.2311 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3821 - acc: 0.8696 - precision: 0.1304 - recall: 0.9996 - f1_metric: 0.2247 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3830 - acc: 0.8712 - precision: 0.1288 - recall: 0.9969 - f1_metric: 0.2235 - val_loss: 0.4124 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3936 - acc: 0.8638 - precision: 0.1362 - recall: 0.9922 - f1_metric: 0.2352 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4027 - acc: 0.8608 - precision: 0.1392 - recall: 0.9702 - f1_metric: 0.2380 - val_loss: 0.4147 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3966 - acc: 0.8622 - precision: 0.1378 - recall: 0.9732 - f1_metric: 0.2377 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3887 - acc: 0.8661 - precision: 0.1339 - recall: 0.9712 - f1_metric: 0.2298 - val_loss: 0.4090 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3823 - acc: 0.8706 - precision: 0.1294 - recall: 0.9973 - f1_metric: 0.2248 - val_loss: 0.4088 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3963 - acc: 0.8603 - precision: 0.1397 - recall: 0.9856 - f1_metric: 0.2409 - val_loss: 0.4103 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3829 - acc: 0.8682 - precision: 0.1318 - recall: 1.0000 - f1_metric: 0.2282 - val_loss: 0.4071 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3947 - acc: 0.8614 - precision: 0.1386 - recall: 0.9941 - f1_metric: 0.2390 - val_loss: 0.4075 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3896 - acc: 0.8636 - precision: 0.1364 - recall: 0.9955 - f1_metric: 0.2348 - val_loss: 0.4072 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8615 - precision: 0.1385 - recall: 0.9977 - f1_metric: 0.2387 - val_loss: 0.4040 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3873 - acc: 0.8641 - precision: 0.1359 - recall: 0.9855 - f1_metric: 0.2341 - val_loss: 0.4062 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3940 - acc: 0.8593 - precision: 0.1407 - recall: 0.9833 - f1_metric: 0.2404 - val_loss: 0.4008 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3924 - acc: 0.8594 - precision: 0.1406 - recall: 0.9884 - f1_metric: 0.2415 - val_loss: 0.4014 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3945 - acc: 0.8559 - precision: 0.1441 - recall: 1.0000 - f1_metric: 0.2464 - val_loss: 0.3949 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 3 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.4965 - acc: 0.8194 - precision: 0.1354 - recall: 1.0656 - f1_metric: 0.2358 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4014 - acc: 0.8636 - precision: 0.1363 - recall: 0.9981 - f1_metric: 0.2351 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3976 - acc: 0.8664 - precision: 0.1336 - recall: 0.9621 - f1_metric: 0.2299 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4067 - acc: 0.8596 - precision: 0.1404 - recall: 0.9947 - f1_metric: 0.2415 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3951 - acc: 0.8654 - precision: 0.1346 - recall: 0.9953 - f1_metric: 0.2319 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3910 - acc: 0.8665 - precision: 0.1335 - recall: 0.9623 - f1_metric: 0.2302 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3939 - acc: 0.8655 - precision: 0.1345 - recall: 0.9829 - f1_metric: 0.2326 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3962 - acc: 0.8652 - precision: 0.1348 - recall: 1.0000 - f1_metric: 0.2320 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4053 - acc: 0.8609 - precision: 0.1391 - recall: 0.9999 - f1_metric: 0.2393 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4011 - acc: 0.8609 - precision: 0.1391 - recall: 0.9895 - f1_metric: 0.2382 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3838 - acc: 0.8692 - precision: 0.1308 - recall: 0.9846 - f1_metric: 0.2268 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3973 - acc: 0.8634 - precision: 0.1366 - recall: 0.9942 - f1_metric: 0.2355 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3936 - acc: 0.8652 - precision: 0.1348 - recall: 0.9897 - f1_metric: 0.2310 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3927 - acc: 0.8648 - precision: 0.1352 - recall: 0.9885 - f1_metric: 0.2332 - val_loss: 0.4116 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4047 - acc: 0.8579 - precision: 0.1421 - recall: 0.9820 - f1_metric: 0.2436 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8614 - precision: 0.1386 - recall: 0.9944 - f1_metric: 0.2386 - val_loss: 0.4105 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8628 - precision: 0.1372 - recall: 0.9931 - f1_metric: 0.2366 - val_loss: 0.4098 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3773 - acc: 0.8730 - precision: 0.1270 - recall: 0.9898 - f1_metric: 0.2199 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3887 - acc: 0.8668 - precision: 0.1332 - recall: 0.9832 - f1_metric: 0.2304 - val_loss: 0.4092 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3942 - acc: 0.8619 - precision: 0.1381 - recall: 0.9843 - f1_metric: 0.2359 - val_loss: 0.4081 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3827 - acc: 0.8673 - precision: 0.1327 - recall: 1.0000 - f1_metric: 0.2312 - val_loss: 0.4073 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3822 - acc: 0.8672 - precision: 0.1328 - recall: 0.9907 - f1_metric: 0.2295 - val_loss: 0.4061 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3963 - acc: 0.8612 - precision: 0.1388 - recall: 0.9763 - f1_metric: 0.2378 - val_loss: 0.4052 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3776 - acc: 0.8700 - precision: 0.1300 - recall: 0.9918 - f1_metric: 0.2248 - val_loss: 0.4041 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3884 - acc: 0.8624 - precision: 0.1376 - recall: 0.9583 - f1_metric: 0.2362 - val_loss: 0.4025 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3858 - acc: 0.8636 - precision: 0.1364 - recall: 1.0000 - f1_metric: 0.2354 - val_loss: 0.4003 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3749 - acc: 0.8689 - precision: 0.1311 - recall: 0.9944 - f1_metric: 0.2275 - val_loss: 0.4160 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3759 - acc: 0.8670 - precision: 0.1330 - recall: 1.0000 - f1_metric: 0.2300 - val_loss: 0.3950 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3875 - acc: 0.8600 - precision: 0.1400 - recall: 0.9998 - f1_metric: 0.2406 - val_loss: 0.3922 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3919 - acc: 0.8544 - precision: 0.1456 - recall: 0.9900 - f1_metric: 0.2486 - val_loss: 0.3891 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 4 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 18ms/step - loss: 0.5673 - acc: 0.6779 - precision: 0.1345 - recall: 1.3018 - f1_metric: 0.2385 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3998 - acc: 0.8658 - precision: 0.1342 - recall: 0.9943 - f1_metric: 0.2321 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3940 - acc: 0.8684 - precision: 0.1316 - recall: 0.9883 - f1_metric: 0.2268 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3930 - acc: 0.8682 - precision: 0.1318 - recall: 0.9840 - f1_metric: 0.2288 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8627 - precision: 0.1373 - recall: 0.9757 - f1_metric: 0.2355 - val_loss: 0.4206 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4061 - acc: 0.8594 - precision: 0.1406 - recall: 0.9825 - f1_metric: 0.2397 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8631 - precision: 0.1369 - recall: 0.9819 - f1_metric: 0.2346 - val_loss: 0.4158 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4114 - acc: 0.8575 - precision: 0.1425 - recall: 0.9934 - f1_metric: 0.2442 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4029 - acc: 0.8596 - precision: 0.1404 - recall: 1.0000 - f1_metric: 0.2416 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3933 - acc: 0.8659 - precision: 0.1341 - recall: 1.0000 - f1_metric: 0.2322 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3973 - acc: 0.8623 - precision: 0.1377 - recall: 0.9770 - f1_metric: 0.2365 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4014 - acc: 0.8596 - precision: 0.1404 - recall: 0.9763 - f1_metric: 0.2410 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3926 - acc: 0.8654 - precision: 0.1346 - recall: 0.9845 - f1_metric: 0.2328 - val_loss: 0.4108 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3951 - acc: 0.8634 - precision: 0.1366 - recall: 0.9902 - f1_metric: 0.2352 - val_loss: 0.4112 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3788 - acc: 0.8710 - precision: 0.1290 - recall: 0.9851 - f1_metric: 0.2246 - val_loss: 0.4100 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3970 - acc: 0.8629 - precision: 0.1371 - recall: 1.0000 - f1_metric: 0.2358 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8639 - precision: 0.1361 - recall: 0.9986 - f1_metric: 0.2354 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4054 - acc: 0.8572 - precision: 0.1428 - recall: 0.9866 - f1_metric: 0.2453 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3844 - acc: 0.8654 - precision: 0.1346 - recall: 0.9805 - f1_metric: 0.2319 - val_loss: 0.4072 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3891 - acc: 0.8645 - precision: 0.1355 - recall: 0.9943 - f1_metric: 0.2342 - val_loss: 0.4066 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3764 - acc: 0.8703 - precision: 0.1297 - recall: 0.9851 - f1_metric: 0.2244 - val_loss: 0.4090 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3862 - acc: 0.8664 - precision: 0.1336 - recall: 1.0000 - f1_metric: 0.2320 - val_loss: 0.4050 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3877 - acc: 0.8633 - precision: 0.1367 - recall: 1.0000 - f1_metric: 0.2354 - val_loss: 0.4036 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3789 - acc: 0.8694 - precision: 0.1306 - recall: 0.9756 - f1_metric: 0.2263 - val_loss: 0.4018 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3958 - acc: 0.8594 - precision: 0.1406 - recall: 0.9905 - f1_metric: 0.2417 - val_loss: 0.4079 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3770 - acc: 0.8679 - precision: 0.1321 - recall: 0.9816 - f1_metric: 0.2279 - val_loss: 0.3976 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3897 - acc: 0.8577 - precision: 0.1423 - recall: 0.9914 - f1_metric: 0.24 - 1s 3ms/step - loss: 0.3886 - acc: 0.8584 - precision: 0.1416 - recall: 0.9917 - f1_metric: 0.2433 - val_loss: 0.3952 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3906 - acc: 0.8584 - precision: 0.1416 - recall: 1.0000 - f1_metric: 0.2440 - val_loss: 0.3932 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3823 - acc: 0.8630 - precision: 0.1370 - recall: 0.9924 - f1_metric: 0.2360 - val_loss: 0.3901 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3716 - acc: 0.8662 - precision: 0.1339 - recall: 0.9770 - f1_metric: 0.2300 - val_loss: 0.3889 - val_acc: 0.8537 - val_precision: 0.1437 - val_recall: 0.9808 - val_f1_metric: 0.2458\n",
      "Model 5 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 17ms/step - loss: 0.5095 - acc: 0.7764 - precision: 0.1354 - recall: 1.0501 - f1_metric: 0.2354 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4066 - acc: 0.8595 - precision: 0.1405 - recall: 0.9716 - f1_metric: 0.2397 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4001 - acc: 0.8630 - precision: 0.1370 - recall: 0.9985 - f1_metric: 0.2356 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3888 - acc: 0.8698 - precision: 0.1302 - recall: 0.9877 - f1_metric: 0.2253 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3951 - acc: 0.8667 - precision: 0.1333 - recall: 0.9981 - f1_metric: 0.2308 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4017 - acc: 0.8608 - precision: 0.1392 - recall: 0.9946 - f1_metric: 0.2399 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3987 - acc: 0.8626 - precision: 0.1374 - recall: 0.9928 - f1_metric: 0.2371 - val_loss: 0.4171 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3949 - acc: 0.8655 - precision: 0.1345 - recall: 0.9850 - f1_metric: 0.2324 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3781 - acc: 0.8728 - precision: 0.1272 - recall: 0.9633 - f1_metric: 0.2204 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4010 - acc: 0.8622 - precision: 0.1378 - recall: 0.9946 - f1_metric: 0.2373 - val_loss: 0.4160 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4051 - acc: 0.8586 - precision: 0.1414 - recall: 0.9937 - f1_metric: 0.2423 - val_loss: 0.4148 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3920 - acc: 0.8641 - precision: 0.1359 - recall: 0.9842 - f1_metric: 0.2338 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3836 - acc: 0.8704 - precision: 0.1296 - recall: 0.9996 - f1_metric: 0.2240 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3984 - acc: 0.8621 - precision: 0.1379 - recall: 0.9945 - f1_metric: 0.2365 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3871 - acc: 0.8669 - precision: 0.1331 - recall: 0.9860 - f1_metric: 0.2296 - val_loss: 0.4107 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3881 - acc: 0.8653 - precision: 0.1347 - recall: 0.9931 - f1_metric: 0.2333 - val_loss: 0.4109 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3977 - acc: 0.8599 - precision: 0.1401 - recall: 1.0000 - f1_metric: 0.2411 - val_loss: 0.4100 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3982 - acc: 0.8609 - precision: 0.1391 - recall: 0.9919 - f1_metric: 0.2386 - val_loss: 0.4104 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8605 - precision: 0.1395 - recall: 0.9904 - f1_metric: 0.2398 - val_loss: 0.4099 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3823 - acc: 0.8690 - precision: 0.1310 - recall: 1.0000 - f1_metric: 0.2272 - val_loss: 0.4078 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3983 - acc: 0.8608 - precision: 0.1392 - recall: 0.9467 - f1_metric: 0.2384 - val_loss: 0.4083 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3801 - acc: 0.8699 - precision: 0.1301 - recall: 0.9624 - f1_metric: 0.2251 - val_loss: 0.4075 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3841 - acc: 0.8671 - precision: 0.1329 - recall: 1.0000 - f1_metric: 0.2308 - val_loss: 0.4044 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8618 - precision: 0.1382 - recall: 0.9794 - f1_metric: 0.2372 - val_loss: 0.4042 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3822 - acc: 0.8651 - precision: 0.1349 - recall: 0.9939 - f1_metric: 0.2330 - val_loss: 0.4011 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3800 - acc: 0.8658 - precision: 0.1342 - recall: 0.9894 - f1_metric: 0.2318 - val_loss: 0.3983 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3814 - acc: 0.8629 - precision: 0.1371 - recall: 1.0000 - f1_metric: 0.2377 - val_loss: 0.3962 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3859 - acc: 0.8608 - precision: 0.1392 - recall: 0.9960 - f1_metric: 0.2384 - val_loss: 0.3948 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3725 - acc: 0.8656 - precision: 0.1344 - recall: 0.9963 - f1_metric: 0.2320 - val_loss: 0.3908 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3697 - acc: 0.8675 - precision: 0.1325 - recall: 0.9885 - f1_metric: 0.2284 - val_loss: 0.3924 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 6 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.5290 - acc: 0.8179 - precision: 0.1256 - recall: 1.2577 - f1_metric: 0.2216 - val_loss: 0.4151 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3957 - acc: 0.8662 - precision: 0.1326 - recall: 0.9979 - f1_metric: 0.2294 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3972 - acc: 0.8661 - precision: 0.1329 - recall: 0.9917 - f1_metric: 0.2300 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4034 - acc: 0.8613 - precision: 0.1402 - recall: 1.0046 - f1_metric: 0.2414 - val_loss: 0.4146 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3991 - acc: 0.8660 - precision: 0.1338 - recall: 0.9821 - f1_metric: 0.2311 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4068 - acc: 0.8581 - precision: 0.1419 - recall: 0.9845 - f1_metric: 0.2435 - val_loss: 0.4171 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8638 - precision: 0.1364 - recall: 0.9994 - f1_metric: 0.2355 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4041 - acc: 0.8604 - precision: 0.1394 - recall: 0.9953 - f1_metric: 0.2408 - val_loss: 0.4152 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8619 - precision: 0.1383 - recall: 0.9965 - f1_metric: 0.2378 - val_loss: 0.4148 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3963 - acc: 0.8650 - precision: 0.1350 - recall: 0.9901 - f1_metric: 0.2330 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4007 - acc: 0.8614 - precision: 0.1394 - recall: 0.9944 - f1_metric: 0.2406 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4024 - acc: 0.8605 - precision: 0.1400 - recall: 0.9879 - f1_metric: 0.2407 - val_loss: 0.4125 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3964 - acc: 0.8625 - precision: 0.1376 - recall: 0.9831 - f1_metric: 0.2377 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3969 - acc: 0.8632 - precision: 0.1364 - recall: 0.9889 - f1_metric: 0.2364 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3966 - acc: 0.8625 - precision: 0.1375 - recall: 0.9999 - f1_metric: 0.2376 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4065 - acc: 0.8588 - precision: 0.1412 - recall: 0.9980 - f1_metric: 0.2423 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3776 - acc: 0.8735 - precision: 0.1264 - recall: 0.9878 - f1_metric: 0.2189 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3919 - acc: 0.8658 - precision: 0.1342 - recall: 0.9802 - f1_metric: 0.2313 - val_loss: 0.4114 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3965 - acc: 0.8626 - precision: 0.1374 - recall: 0.9921 - f1_metric: 0.2370 - val_loss: 0.4152 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3931 - acc: 0.8646 - precision: 0.1354 - recall: 0.9885 - f1_metric: 0.2343 - val_loss: 0.4111 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8630 - precision: 0.1370 - recall: 0.9944 - f1_metric: 0.2359 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3886 - acc: 0.8670 - precision: 0.1330 - recall: 0.9898 - f1_metric: 0.2294 - val_loss: 0.4100 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3898 - acc: 0.8648 - precision: 0.1351 - recall: 0.9990 - f1_metric: 0.2336 - val_loss: 0.4096 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3861 - acc: 0.8659 - precision: 0.1341 - recall: 0.9989 - f1_metric: 0.2322 - val_loss: 0.4110 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3959 - acc: 0.8600 - precision: 0.1400 - recall: 0.9969 - f1_metric: 0.2403 - val_loss: 0.4083 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3806 - acc: 0.8703 - precision: 0.1297 - recall: 1.0000 - f1_metric: 0.2248 - val_loss: 0.4075 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3832 - acc: 0.8680 - precision: 0.1320 - recall: 0.9951 - f1_metric: 0.2285 - val_loss: 0.4082 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4085 - acc: 0.8528 - precision: 0.1472 - recall: 0.9977 - f1_metric: 0.2517 - val_loss: 0.4054 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3897 - acc: 0.8634 - precision: 0.1366 - recall: 0.9795 - f1_metric: 0.2342 - val_loss: 0.4050 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3715 - acc: 0.8716 - precision: 0.1284 - recall: 0.9975 - f1_metric: 0.2226 - val_loss: 0.4011 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 7 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 19ms/step - loss: 0.4860 - acc: 0.8461 - precision: 0.1461 - recall: 0.9223 - f1_metric: 0.2441 - val_loss: 0.4164 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3913 - acc: 0.8686 - precision: 0.1314 - recall: 0.9857 - f1_metric: 0.2277 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3975 - acc: 0.8650 - precision: 0.1350 - recall: 0.9978 - f1_metric: 0.2339 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3996 - acc: 0.8641 - precision: 0.1359 - recall: 0.9967 - f1_metric: 0.2350 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3780 - acc: 0.8720 - precision: 0.1280 - recall: 0.9836 - f1_metric: 0.2223 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3947 - acc: 0.8656 - precision: 0.1344 - recall: 0.9801 - f1_metric: 0.2314 - val_loss: 0.4161 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4119 - acc: 0.8559 - precision: 0.1441 - recall: 0.9894 - f1_metric: 0.2465 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3955 - acc: 0.8633 - precision: 0.1367 - recall: 0.9933 - f1_metric: 0.2360 - val_loss: 0.4192 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3908 - acc: 0.8672 - precision: 0.1328 - recall: 0.9938 - f1_metric: 0.2293 - val_loss: 0.4125 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3873 - acc: 0.8684 - precision: 0.1316 - recall: 1.0000 - f1_metric: 0.2287 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3878 - acc: 0.8685 - precision: 0.1315 - recall: 0.9759 - f1_metric: 0.2272 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3999 - acc: 0.8613 - precision: 0.1387 - recall: 0.9963 - f1_metric: 0.2390 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3866 - acc: 0.8671 - precision: 0.1329 - recall: 1.0000 - f1_metric: 0.2303 - val_loss: 0.4117 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4090 - acc: 0.8551 - precision: 0.1449 - recall: 0.9934 - f1_metric: 0.2480 - val_loss: 0.4113 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3943 - acc: 0.8639 - precision: 0.1361 - recall: 0.9903 - f1_metric: 0.2346 - val_loss: 0.4120 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8651 - precision: 0.1349 - recall: 0.9924 - f1_metric: 0.2320 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3821 - acc: 0.8708 - precision: 0.1292 - recall: 0.9960 - f1_metric: 0.2243 - val_loss: 0.4112 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3945 - acc: 0.8626 - precision: 0.1374 - recall: 0.9770 - f1_metric: 0.2364 - val_loss: 0.4095 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3888 - acc: 0.8662 - precision: 0.1338 - recall: 0.9922 - f1_metric: 0.2305 - val_loss: 0.4093 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3843 - acc: 0.8666 - precision: 0.1334 - recall: 0.9999 - f1_metric: 0.2311 - val_loss: 0.4080 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3952 - acc: 0.8590 - precision: 0.1410 - recall: 0.9883 - f1_metric: 0.2421 - val_loss: 0.4078 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4021 - acc: 0.8572 - precision: 0.1428 - recall: 0.9800 - f1_metric: 0.2450 - val_loss: 0.4055 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3781 - acc: 0.8684 - precision: 0.1316 - recall: 0.9834 - f1_metric: 0.2280 - val_loss: 0.4043 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4056 - acc: 0.8535 - precision: 0.1465 - recall: 0.9937 - f1_metric: 0.2490 - val_loss: 0.4027 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3898 - acc: 0.8613 - precision: 0.1387 - recall: 0.9975 - f1_metric: 0.2380 - val_loss: 0.4036 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3798 - acc: 0.8675 - precision: 0.1325 - recall: 0.9784 - f1_metric: 0.2290 - val_loss: 0.3992 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3866 - acc: 0.8626 - precision: 0.1374 - recall: 0.9954 - f1_metric: 0.2373 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3846 - acc: 0.8628 - precision: 0.1372 - recall: 0.9931 - f1_metric: 0.2366 - val_loss: 0.3962 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3859 - acc: 0.8584 - precision: 0.1416 - recall: 0.9851 - f1_metric: 0.2428 - val_loss: 0.3911 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3716 - acc: 0.8646 - precision: 0.1354 - recall: 0.9942 - f1_metric: 0.2342 - val_loss: 0.3874 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 8 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 3s 11ms/step - loss: 0.4584 - acc: 0.8660 - precision: 0.1291 - recall: 0.9715 - f1_metric: 0.2221 - val_loss: 0.4148 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4180 - acc: 0.8545 - precision: 0.1455 - recall: 1.0000 - f1_metric: 0.2493 - val_loss: 0.4156 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.4045 - acc: 0.8623 - precision: 0.1377 - recall: 0.9910 - f1_metric: 0.2377 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4040 - acc: 0.8620 - precision: 0.1380 - recall: 0.9895 - f1_metric: 0.2365 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3930 - acc: 0.8644 - precision: 0.1357 - recall: 0.9877 - f1_metric: 0.2344 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4011 - acc: 0.8642 - precision: 0.1358 - recall: 0.9957 - f1_metric: 0.2351 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4219 - acc: 0.8521 - precision: 0.1479 - recall: 0.9879 - f1_metric: 0.2529 - val_loss: 0.4166 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.8620 - precision: 0.1380 - recall: 0.9958 - f1_metric: 0.2377 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3951 - acc: 0.8649 - precision: 0.1351 - recall: 0.9847 - f1_metric: 0.2327 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3737 - acc: 0.8751 - precision: 0.1249 - recall: 0.9630 - f1_metric: 0.2163 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3996 - acc: 0.8622 - precision: 0.1378 - recall: 1.0000 - f1_metric: 0.2382 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3920 - acc: 0.8656 - precision: 0.1344 - recall: 0.9736 - f1_metric: 0.2321 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8628 - precision: 0.1372 - recall: 1.0000 - f1_metric: 0.2359 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8638 - precision: 0.1362 - recall: 0.9870 - f1_metric: 0.2355 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3975 - acc: 0.8635 - precision: 0.1365 - recall: 1.0000 - f1_metric: 0.2351 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4096 - acc: 0.8556 - precision: 0.1444 - recall: 0.9964 - f1_metric: 0.2457 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8662 - precision: 0.1338 - recall: 0.9859 - f1_metric: 0.2312 - val_loss: 0.4108 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4039 - acc: 0.8591 - precision: 0.1409 - recall: 0.9976 - f1_metric: 0.2422 - val_loss: 0.4150 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3950 - acc: 0.8642 - precision: 0.1358 - recall: 0.9785 - f1_metric: 0.2340 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3898 - acc: 0.8653 - precision: 0.1347 - recall: 0.9994 - f1_metric: 0.2342 - val_loss: 0.4113 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4101 - acc: 0.8562 - precision: 0.1438 - recall: 0.9843 - f1_metric: 0.2456 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3876 - acc: 0.8640 - precision: 0.1360 - recall: 0.9971 - f1_metric: 0.2346 - val_loss: 0.4088 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3816 - acc: 0.8683 - precision: 0.1317 - recall: 0.9999 - f1_metric: 0.2275 - val_loss: 0.4088 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4010 - acc: 0.8585 - precision: 0.1415 - recall: 0.9933 - f1_metric: 0.2433 - val_loss: 0.4063 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3787 - acc: 0.8688 - precision: 0.1312 - recall: 1.0000 - f1_metric: 0.2270 - val_loss: 0.4050 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3975 - acc: 0.8589 - precision: 0.1411 - recall: 0.9857 - f1_metric: 0.2420 - val_loss: 0.4055 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8601 - precision: 0.1399 - recall: 1.0000 - f1_metric: 0.2406 - val_loss: 0.4044 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3831 - acc: 0.8644 - precision: 0.1356 - recall: 0.9869 - f1_metric: 0.2344 - val_loss: 0.4016 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3904 - acc: 0.8604 - precision: 0.1396 - recall: 0.9996 - f1_metric: 0.2389 - val_loss: 0.3986 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3887 - acc: 0.8597 - precision: 0.1403 - recall: 1.0000 - f1_metric: 0.2405 - val_loss: 0.3997 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 9 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 3s 13ms/step - loss: 0.4992 - acc: 0.8136 - precision: 0.1334 - recall: 0.9796 - f1_metric: 0.2301 - val_loss: 0.4152 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3976 - acc: 0.8666 - precision: 0.1333 - recall: 0.9934 - f1_metric: 0.2308 - val_loss: 0.4157 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4069 - acc: 0.8582 - precision: 0.1426 - recall: 1.0006 - f1_metric: 0.2439 - val_loss: 0.4149 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4034 - acc: 0.8626 - precision: 0.1374 - recall: 1.0000 - f1_metric: 0.2373 - val_loss: 0.4156 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3888 - acc: 0.8695 - precision: 0.1305 - recall: 0.9997 - f1_metric: 0.2260 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3886 - acc: 0.8680 - precision: 0.1320 - recall: 1.0000 - f1_metric: 0.2281 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3981 - acc: 0.8644 - precision: 0.1356 - recall: 0.9887 - f1_metric: 0.2339 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3842 - acc: 0.8711 - precision: 0.1289 - recall: 0.9934 - f1_metric: 0.2235 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4002 - acc: 0.8626 - precision: 0.1374 - recall: 0.9755 - f1_metric: 0.2365 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8668 - precision: 0.1332 - recall: 0.9949 - f1_metric: 0.2303 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3976 - acc: 0.8621 - precision: 0.1379 - recall: 0.9953 - f1_metric: 0.2383 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3970 - acc: 0.8637 - precision: 0.1363 - recall: 0.9970 - f1_metric: 0.2357 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3737 - acc: 0.8763 - precision: 0.1237 - recall: 0.9948 - f1_metric: 0.2153 - val_loss: 0.4116 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3858 - acc: 0.8679 - precision: 0.1321 - recall: 0.9844 - f1_metric: 0.2285 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4059 - acc: 0.8589 - precision: 0.1411 - recall: 1.0000 - f1_metric: 0.2437 - val_loss: 0.4120 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4029 - acc: 0.8604 - precision: 0.1396 - recall: 0.9982 - f1_metric: 0.2390 - val_loss: 0.4105 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3899 - acc: 0.8654 - precision: 0.1346 - recall: 0.9865 - f1_metric: 0.2321 - val_loss: 0.4104 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3914 - acc: 0.8647 - precision: 0.1353 - recall: 0.9778 - f1_metric: 0.2339 - val_loss: 0.4096 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3862 - acc: 0.8674 - precision: 0.1326 - recall: 0.9917 - f1_metric: 0.2298 - val_loss: 0.4093 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3996 - acc: 0.8596 - precision: 0.1404 - recall: 0.9963 - f1_metric: 0.2415 - val_loss: 0.4084 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3959 - acc: 0.8614 - precision: 0.1386 - recall: 1.0000 - f1_metric: 0.2385 - val_loss: 0.4098 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3891 - acc: 0.8636 - precision: 0.1364 - recall: 0.9856 - f1_metric: 0.2346 - val_loss: 0.4068 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3871 - acc: 0.8639 - precision: 0.1361 - recall: 0.9903 - f1_metric: 0.2352 - val_loss: 0.4081 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3809 - acc: 0.8688 - precision: 0.1312 - recall: 0.9657 - f1_metric: 0.2265 - val_loss: 0.4076 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3882 - acc: 0.8624 - precision: 0.1376 - recall: 0.9707 - f1_metric: 0.2369 - val_loss: 0.4042 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3932 - acc: 0.8598 - precision: 0.1402 - recall: 0.9949 - f1_metric: 0.2413 - val_loss: 0.4005 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3861 - acc: 0.8616 - precision: 0.1384 - recall: 1.0000 - f1_metric: 0.2396 - val_loss: 0.3993 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3621 - acc: 0.8720 - precision: 0.1280 - recall: 0.9991 - f1_metric: 0.2212 - val_loss: 0.3955 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3689 - acc: 0.8705 - precision: 0.1295 - recall: 0.9914 - f1_metric: 0.2247 - val_loss: 0.3931 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3753 - acc: 0.8653 - precision: 0.1347 - recall: 0.9936 - f1_metric: 0.2334 - val_loss: 0.3913 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 10 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 6s 21ms/step - loss: 0.4967 - acc: 0.8314 - precision: 0.1441 - recall: 0.9634 - f1_metric: 0.2430 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4149 - acc: 0.8576 - precision: 0.1424 - recall: 0.9787 - f1_metric: 0.2446 - val_loss: 0.4146 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4025 - acc: 0.8632 - precision: 0.1368 - recall: 0.9814 - f1_metric: 0.2346 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4030 - acc: 0.8624 - precision: 0.1376 - recall: 1.0000 - f1_metric: 0.2379 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.4005 - acc: 0.8620 - precision: 0.1380 - recall: 0.9859 - f1_metric: 0.2375 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3856 - acc: 0.8699 - precision: 0.1301 - recall: 0.9703 - f1_metric: 0.2248 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4007 - acc: 0.8630 - precision: 0.1370 - recall: 0.9991 - f1_metric: 0.2363 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3960 - acc: 0.8636 - precision: 0.1364 - recall: 0.9840 - f1_metric: 0.2349 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3968 - acc: 0.8647 - precision: 0.1353 - recall: 0.9770 - f1_metric: 0.2326 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4021 - acc: 0.8613 - precision: 0.1387 - recall: 0.9982 - f1_metric: 0.2390 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8629 - precision: 0.1371 - recall: 0.9918 - f1_metric: 0.2369 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3989 - acc: 0.8621 - precision: 0.1379 - recall: 0.9722 - f1_metric: 0.2367 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3987 - acc: 0.8626 - precision: 0.1374 - recall: 0.9899 - f1_metric: 0.2365 - val_loss: 0.4156 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3974 - acc: 0.8619 - precision: 0.1381 - recall: 0.9921 - f1_metric: 0.2378 - val_loss: 0.4111 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4036 - acc: 0.8592 - precision: 0.1408 - recall: 0.9844 - f1_metric: 0.2411 - val_loss: 0.4109 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4013 - acc: 0.8592 - precision: 0.1408 - recall: 0.9975 - f1_metric: 0.2426 - val_loss: 0.4109 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4004 - acc: 0.8604 - precision: 0.1396 - recall: 0.9853 - f1_metric: 0.24 - 1s 3ms/step - loss: 0.3998 - acc: 0.8606 - precision: 0.1394 - recall: 0.9858 - f1_metric: 0.2398 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3959 - acc: 0.8621 - precision: 0.1379 - recall: 0.9762 - f1_metric: 0.2370 - val_loss: 0.4102 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3891 - acc: 0.8646 - precision: 0.1354 - recall: 0.9882 - f1_metric: 0.2342 - val_loss: 0.4090 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3745 - acc: 0.8749 - precision: 0.1251 - recall: 0.9985 - f1_metric: 0.2176 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8634 - precision: 0.1366 - recall: 1.0000 - f1_metric: 0.2354 - val_loss: 0.4082 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3887 - acc: 0.8671 - precision: 0.1329 - recall: 0.9839 - f1_metric: 0.2294 - val_loss: 0.4066 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3881 - acc: 0.8670 - precision: 0.1330 - recall: 0.9972 - f1_metric: 0.2310 - val_loss: 0.4096 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3904 - acc: 0.8628 - precision: 0.1372 - recall: 0.9949 - f1_metric: 0.2368 - val_loss: 0.4048 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3853 - acc: 0.8645 - precision: 0.1355 - recall: 0.9825 - f1_metric: 0.2342 - val_loss: 0.4037 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3836 - acc: 0.8671 - precision: 0.1329 - recall: 0.9897 - f1_metric: 0.2303 - val_loss: 0.4071 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3716 - acc: 0.8701 - precision: 0.1299 - recall: 0.9995 - f1_metric: 0.2253 - val_loss: 0.3991 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3820 - acc: 0.8646 - precision: 0.1354 - recall: 0.9989 - f1_metric: 0.2332 - val_loss: 0.3974 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3903 - acc: 0.8588 - precision: 0.1412 - recall: 0.9708 - f1_metric: 0.2416 - val_loss: 0.3966 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3828 - acc: 0.8623 - precision: 0.1377 - recall: 0.9990 - f1_metric: 0.2380 - val_loss: 0.3960 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 11 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.5029 - acc: 0.8285 - precision: 0.1381 - recall: 1.3692 - f1_metric: 0.2458 - val_loss: 0.4144 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3933 - acc: 0.8696 - precision: 0.1296 - recall: 1.0490 - f1_metric: 0.2265 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3949 - acc: 0.8658 - precision: 0.1351 - recall: 1.0686 - f1_metric: 0.2364 - val_loss: 0.4165 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3935 - acc: 0.8650 - precision: 0.1348 - recall: 1.0543 - f1_metric: 0.2346 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3892 - acc: 0.8684 - precision: 0.1343 - recall: 1.1015 - f1_metric: 0.2338 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3886 - acc: 0.8687 - precision: 0.1322 - recall: 1.1355 - f1_metric: 0.2324 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4020 - acc: 0.8607 - precision: 0.1395 - recall: 1.0781 - f1_metric: 0.2434 - val_loss: 0.4161 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3980 - acc: 0.8647 - precision: 0.1362 - recall: 1.0970 - f1_metric: 0.2366 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3963 - acc: 0.8641 - precision: 0.1374 - recall: 1.0888 - f1_metric: 0.2403 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3980 - acc: 0.8615 - precision: 0.1407 - recall: 1.1432 - f1_metric: 0.2461 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3962 - acc: 0.8633 - precision: 0.1374 - recall: 1.1796 - f1_metric: 0.2427 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3933 - acc: 0.8648 - precision: 0.1317 - recall: 1.1601 - f1_metric: 0.2321 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4004 - acc: 0.8612 - precision: 0.1370 - recall: 1.1986 - f1_metric: 0.2418 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3957 - acc: 0.8638 - precision: 0.1340 - recall: 1.1434 - f1_metric: 0.2351 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3990 - acc: 0.8604 - precision: 0.1338 - recall: 1.0983 - f1_metric: 0.2337 - val_loss: 0.4113 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4042 - acc: 0.8567 - precision: 0.1402 - recall: 1.0846 - f1_metric: 0.2435 - val_loss: 0.4116 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3911 - acc: 0.8657 - precision: 0.1296 - recall: 1.1116 - f1_metric: 0.2282 - val_loss: 0.4117 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3904 - acc: 0.8676 - precision: 0.1300 - recall: 1.1060 - f1_metric: 0.2285 - val_loss: 0.4103 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4113 - acc: 0.8549 - precision: 0.1442 - recall: 1.0714 - f1_metric: 0.2491 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3875 - acc: 0.8663 - precision: 0.1319 - recall: 1.0681 - f1_metric: 0.2301 - val_loss: 0.4113 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4037 - acc: 0.8579 - precision: 0.1399 - recall: 1.0215 - f1_metric: 0.2402 - val_loss: 0.4095 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3928 - acc: 0.8640 - precision: 0.1352 - recall: 1.0075 - f1_metric: 0.2333 - val_loss: 0.4083 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3853 - acc: 0.8684 - precision: 0.1299 - recall: 0.9891 - f1_metric: 0.2257 - val_loss: 0.4074 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3924 - acc: 0.8634 - precision: 0.1354 - recall: 0.9916 - f1_metric: 0.2337 - val_loss: 0.4068 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3959 - acc: 0.8594 - precision: 0.1408 - recall: 1.0120 - f1_metric: 0.2424 - val_loss: 0.4055 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3897 - acc: 0.8634 - precision: 0.1360 - recall: 1.0000 - f1_metric: 0.2351 - val_loss: 0.4043 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3874 - acc: 0.8632 - precision: 0.1366 - recall: 0.9911 - f1_metric: 0.2353 - val_loss: 0.4082 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3762 - acc: 0.8689 - precision: 0.1311 - recall: 0.9832 - f1_metric: 0.2273 - val_loss: 0.3996 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3939 - acc: 0.8578 - precision: 0.1422 - recall: 0.9942 - f1_metric: 0.2432 - val_loss: 0.3972 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3682 - acc: 0.8706 - precision: 0.1294 - recall: 0.9912 - f1_metric: 0.2242 - val_loss: 0.3934 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 12 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.4482 - acc: 0.8640 - precision: 0.1324 - recall: 1.0034 - f1_metric: 0.2304 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4088 - acc: 0.8627 - precision: 0.1373 - recall: 0.9934 - f1_metric: 0.2361 - val_loss: 0.4146 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4126 - acc: 0.8586 - precision: 0.1414 - recall: 0.9951 - f1_metric: 0.2426 - val_loss: 0.4149 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3927 - acc: 0.8682 - precision: 0.1319 - recall: 0.9958 - f1_metric: 0.2280 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8620 - precision: 0.1380 - recall: 0.9991 - f1_metric: 0.2380 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3963 - acc: 0.8650 - precision: 0.1350 - recall: 0.9864 - f1_metric: 0.2326 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4100 - acc: 0.8564 - precision: 0.1436 - recall: 0.9956 - f1_metric: 0.2465 - val_loss: 0.4156 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4135 - acc: 0.8544 - precision: 0.1456 - recall: 0.9929 - f1_metric: 0.2497 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3814 - acc: 0.8710 - precision: 0.1290 - recall: 0.9851 - f1_metric: 0.2233 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4013 - acc: 0.8608 - precision: 0.1392 - recall: 0.9908 - f1_metric: 0.2397 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3992 - acc: 0.8617 - precision: 0.1383 - recall: 0.9884 - f1_metric: 0.2384 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3906 - acc: 0.8646 - precision: 0.1354 - recall: 1.0000 - f1_metric: 0.2350 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3940 - acc: 0.8648 - precision: 0.1352 - recall: 1.0000 - f1_metric: 0.2333 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8644 - precision: 0.1356 - recall: 1.0000 - f1_metric: 0.2344 - val_loss: 0.4108 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3937 - acc: 0.8636 - precision: 0.1364 - recall: 0.9904 - f1_metric: 0.2350 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3914 - acc: 0.8637 - precision: 0.1363 - recall: 0.9980 - f1_metric: 0.2357 - val_loss: 0.4100 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3899 - acc: 0.8643 - precision: 0.1357 - recall: 1.0000 - f1_metric: 0.2351 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3845 - acc: 0.8688 - precision: 0.1312 - recall: 0.9952 - f1_metric: 0.2280 - val_loss: 0.4092 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4032 - acc: 0.8594 - precision: 0.1406 - recall: 0.9950 - f1_metric: 0.2418 - val_loss: 0.4081 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3836 - acc: 0.8681 - precision: 0.1319 - recall: 0.9842 - f1_metric: 0.2288 - val_loss: 0.4075 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4057 - acc: 0.8553 - precision: 0.1447 - recall: 0.9944 - f1_metric: 0.2479 - val_loss: 0.4073 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3804 - acc: 0.8677 - precision: 0.1324 - recall: 0.9545 - f1_metric: 0.2283 - val_loss: 0.4068 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3834 - acc: 0.8680 - precision: 0.1320 - recall: 0.9993 - f1_metric: 0.2285 - val_loss: 0.4042 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3940 - acc: 0.8601 - precision: 0.1399 - recall: 0.9903 - f1_metric: 0.2407 - val_loss: 0.4045 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3985 - acc: 0.8556 - precision: 0.1444 - recall: 0.9951 - f1_metric: 0.2467 - val_loss: 0.4007 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3814 - acc: 0.8639 - precision: 0.1361 - recall: 0.9769 - f1_metric: 0.2350 - val_loss: 0.3965 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3933 - acc: 0.8573 - precision: 0.1427 - recall: 1.0000 - f1_metric: 0.2451 - val_loss: 0.3994 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3803 - acc: 0.8645 - precision: 0.1355 - recall: 0.9983 - f1_metric: 0.2332 - val_loss: 0.3930 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3925 - acc: 0.8556 - precision: 0.1444 - recall: 0.9996 - f1_metric: 0.2476 - val_loss: 0.3934 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3785 - acc: 0.8617 - precision: 0.1383 - recall: 0.9855 - f1_metric: 0.2384 - val_loss: 0.3852 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 13 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 19ms/step - loss: 0.4660 - acc: 0.8604 - precision: 0.1292 - recall: 0.9805 - f1_metric: 0.2231 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4094 - acc: 0.8619 - precision: 0.1381 - recall: 0.9793 - f1_metric: 0.2377 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4133 - acc: 0.8581 - precision: 0.1419 - recall: 0.9896 - f1_metric: 0.2441 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3971 - acc: 0.8654 - precision: 0.1346 - recall: 0.9938 - f1_metric: 0.2321 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3865 - acc: 0.8698 - precision: 0.1302 - recall: 0.9993 - f1_metric: 0.2263 - val_loss: 0.4144 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8661 - precision: 0.1339 - recall: 0.9985 - f1_metric: 0.2312 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4043 - acc: 0.8605 - precision: 0.1395 - recall: 0.9992 - f1_metric: 0.2398 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8642 - precision: 0.1358 - recall: 0.9970 - f1_metric: 0.2350 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3904 - acc: 0.8686 - precision: 0.1313 - recall: 1.0000 - f1_metric: 0.2286 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3946 - acc: 0.8656 - precision: 0.1344 - recall: 0.9650 - f1_metric: 0.2317 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8657 - precision: 0.1343 - recall: 0.9838 - f1_metric: 0.2317 - val_loss: 0.4150 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3868 - acc: 0.8683 - precision: 0.1317 - recall: 0.9991 - f1_metric: 0.2281 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3976 - acc: 0.8628 - precision: 0.1372 - recall: 1.0000 - f1_metric: 0.2361 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3814 - acc: 0.8707 - precision: 0.1293 - recall: 0.9886 - f1_metric: 0.2237 - val_loss: 0.4108 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4034 - acc: 0.8586 - precision: 0.1414 - recall: 0.9990 - f1_metric: 0.2433 - val_loss: 0.4114 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3866 - acc: 0.8666 - precision: 0.1334 - recall: 0.9831 - f1_metric: 0.2303 - val_loss: 0.4102 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3947 - acc: 0.8621 - precision: 0.1379 - recall: 0.9956 - f1_metric: 0.2383 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3888 - acc: 0.8662 - precision: 0.1338 - recall: 0.9853 - f1_metric: 0.2315 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3924 - acc: 0.8639 - precision: 0.1361 - recall: 0.9921 - f1_metric: 0.2351 - val_loss: 0.4085 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3790 - acc: 0.8705 - precision: 0.1295 - recall: 0.9973 - f1_metric: 0.2234 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3809 - acc: 0.8684 - precision: 0.1316 - recall: 0.9846 - f1_metric: 0.2282 - val_loss: 0.4064 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3903 - acc: 0.8631 - precision: 0.1369 - recall: 0.9742 - f1_metric: 0.2348 - val_loss: 0.4062 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3876 - acc: 0.8654 - precision: 0.1346 - recall: 0.9976 - f1_metric: 0.2326 - val_loss: 0.4099 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3746 - acc: 0.8717 - precision: 0.1283 - recall: 0.9614 - f1_metric: 0.2221 - val_loss: 0.4063 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3913 - acc: 0.8610 - precision: 0.1390 - recall: 1.0000 - f1_metric: 0.2396 - val_loss: 0.4003 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3713 - acc: 0.8734 - precision: 0.1266 - recall: 1.0000 - f1_metric: 0.2204 - val_loss: 0.3985 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3818 - acc: 0.8635 - precision: 0.1365 - recall: 0.9928 - f1_metric: 0.2364 - val_loss: 0.3970 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3934 - acc: 0.8577 - precision: 0.1423 - recall: 0.9772 - f1_metric: 0.2444 - val_loss: 0.3961 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3875 - acc: 0.8595 - precision: 0.1405 - recall: 0.9984 - f1_metric: 0.2411 - val_loss: 0.3936 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3701 - acc: 0.8652 - precision: 0.1348 - recall: 0.9913 - f1_metric: 0.2326 - val_loss: 0.3879 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 14 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 12ms/step - loss: 0.4673 - acc: 0.8450 - precision: 0.1377 - recall: 0.9347 - f1_metric: 0.2338 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4155 - acc: 0.8584 - precision: 0.1416 - recall: 1.0000 - f1_metric: 0.2433 - val_loss: 0.4158 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3908 - acc: 0.8698 - precision: 0.1302 - recall: 0.9957 - f1_metric: 0.2269 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3894 - acc: 0.8688 - precision: 0.1312 - recall: 0.9987 - f1_metric: 0.2278 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4120 - acc: 0.8564 - precision: 0.1436 - recall: 0.9798 - f1_metric: 0.2459 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4062 - acc: 0.8594 - precision: 0.1406 - recall: 0.9965 - f1_metric: 0.2419 - val_loss: 0.4151 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3909 - acc: 0.8680 - precision: 0.1320 - recall: 0.9853 - f1_metric: 0.2274 - val_loss: 0.4143 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3886 - acc: 0.8701 - precision: 0.1299 - recall: 0.9891 - f1_metric: 0.2241 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3907 - acc: 0.8663 - precision: 0.1337 - recall: 0.9988 - f1_metric: 0.2306 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4121 - acc: 0.8549 - precision: 0.1451 - recall: 0.9834 - f1_metric: 0.2470 - val_loss: 0.4125 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8664 - precision: 0.1336 - recall: 1.0000 - f1_metric: 0.2307 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3914 - acc: 0.8668 - precision: 0.1332 - recall: 0.9939 - f1_metric: 0.2295 - val_loss: 0.4124 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3995 - acc: 0.8626 - precision: 0.1374 - recall: 0.9965 - f1_metric: 0.2374 - val_loss: 0.4125 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3780 - acc: 0.8723 - precision: 0.1277 - recall: 0.9768 - f1_metric: 0.2205 - val_loss: 0.4121 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4059 - acc: 0.8575 - precision: 0.1425 - recall: 0.9822 - f1_metric: 0.2442 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3933 - acc: 0.8638 - precision: 0.1362 - recall: 1.0000 - f1_metric: 0.2348 - val_loss: 0.4113 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3932 - acc: 0.8645 - precision: 0.1355 - recall: 0.9870 - f1_metric: 0.2342 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3974 - acc: 0.8605 - precision: 0.1395 - recall: 1.0000 - f1_metric: 0.2391 - val_loss: 0.4107 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4034 - acc: 0.8573 - precision: 0.1427 - recall: 1.0000 - f1_metric: 0.2457 - val_loss: 0.4110 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4005 - acc: 0.8611 - precision: 0.1389 - recall: 1.0000 - f1_metric: 0.2383 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3952 - acc: 0.8635 - precision: 0.1365 - recall: 0.9667 - f1_metric: 0.2344 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3860 - acc: 0.8682 - precision: 0.1318 - recall: 1.0000 - f1_metric: 0.2277 - val_loss: 0.4081 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3925 - acc: 0.8648 - precision: 0.1352 - recall: 0.9767 - f1_metric: 0.2326 - val_loss: 0.4085 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3903 - acc: 0.8632 - precision: 0.1368 - recall: 0.9761 - f1_metric: 0.2351 - val_loss: 0.4117 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3788 - acc: 0.8677 - precision: 0.1323 - recall: 0.9828 - f1_metric: 0.2290 - val_loss: 0.4101 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3893 - acc: 0.8642 - precision: 0.1358 - recall: 1.0000 - f1_metric: 0.2344 - val_loss: 0.4049 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3930 - acc: 0.8599 - precision: 0.1401 - recall: 0.9940 - f1_metric: 0.2397 - val_loss: 0.4022 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3770 - acc: 0.8702 - precision: 0.1298 - recall: 0.9929 - f1_metric: 0.2260 - val_loss: 0.4031 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3923 - acc: 0.8594 - precision: 0.1406 - recall: 0.9818 - f1_metric: 0.2407 - val_loss: 0.3980 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3758 - acc: 0.8668 - precision: 0.1332 - recall: 0.9898 - f1_metric: 0.2304 - val_loss: 0.3966 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 15 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 18ms/step - loss: 0.4754 - acc: 0.8485 - precision: 0.1476 - recall: 0.9847 - f1_metric: 0.2517 - val_loss: 0.4148 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8671 - precision: 0.1330 - recall: 0.9837 - f1_metric: 0.2302 - val_loss: 0.4163 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3998 - acc: 0.8638 - precision: 0.1362 - recall: 0.9934 - f1_metric: 0.2353 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3964 - acc: 0.8648 - precision: 0.1352 - recall: 1.0000 - f1_metric: 0.2345 - val_loss: 0.4156 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3951 - acc: 0.8642 - precision: 0.1358 - recall: 1.0000 - f1_metric: 0.2350 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8659 - precision: 0.1341 - recall: 1.0000 - f1_metric: 0.2317 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3928 - acc: 0.8654 - precision: 0.1346 - recall: 1.0000 - f1_metric: 0.2326 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4049 - acc: 0.8624 - precision: 0.1376 - recall: 0.9815 - f1_metric: 0.2359 - val_loss: 0.4156 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3830 - acc: 0.8707 - precision: 0.1293 - recall: 0.9968 - f1_metric: 0.2250 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3960 - acc: 0.8642 - precision: 0.1358 - recall: 0.9837 - f1_metric: 0.2340 - val_loss: 0.4120 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3972 - acc: 0.8637 - precision: 0.1363 - recall: 0.9883 - f1_metric: 0.2349 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3890 - acc: 0.8669 - precision: 0.1331 - recall: 0.9763 - f1_metric: 0.2292 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3946 - acc: 0.8643 - precision: 0.1357 - recall: 0.9949 - f1_metric: 0.2352 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3913 - acc: 0.8638 - precision: 0.1362 - recall: 0.9907 - f1_metric: 0.2347 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3822 - acc: 0.8702 - precision: 0.1298 - recall: 0.9736 - f1_metric: 0.2255 - val_loss: 0.4105 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3816 - acc: 0.8711 - precision: 0.1289 - recall: 0.9732 - f1_metric: 0.2230 - val_loss: 0.4103 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3897 - acc: 0.8658 - precision: 0.1342 - recall: 0.9952 - f1_metric: 0.2312 - val_loss: 0.4112 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3928 - acc: 0.8647 - precision: 0.1353 - recall: 0.9983 - f1_metric: 0.2322 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4012 - acc: 0.8615 - precision: 0.1385 - recall: 0.9635 - f1_metric: 0.2365 - val_loss: 0.4087 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3826 - acc: 0.8681 - precision: 0.1319 - recall: 0.9800 - f1_metric: 0.2282 - val_loss: 0.4101 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3823 - acc: 0.8681 - precision: 0.1319 - recall: 0.9784 - f1_metric: 0.2281 - val_loss: 0.4078 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3913 - acc: 0.8629 - precision: 0.1371 - recall: 0.9851 - f1_metric: 0.2360 - val_loss: 0.4065 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3966 - acc: 0.8600 - precision: 0.1400 - recall: 0.9961 - f1_metric: 0.2411 - val_loss: 0.4051 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3926 - acc: 0.8614 - precision: 0.1386 - recall: 0.9867 - f1_metric: 0.2379 - val_loss: 0.4061 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3962 - acc: 0.8582 - precision: 0.1418 - recall: 1.0000 - f1_metric: 0.2439 - val_loss: 0.4079 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3835 - acc: 0.8658 - precision: 0.1342 - recall: 0.9820 - f1_metric: 0.2321 - val_loss: 0.4021 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3759 - acc: 0.8674 - precision: 0.1326 - recall: 1.0000 - f1_metric: 0.2292 - val_loss: 0.3959 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3823 - acc: 0.8627 - precision: 0.1373 - recall: 0.9733 - f1_metric: 0.2361 - val_loss: 0.3929 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3667 - acc: 0.8708 - precision: 0.1292 - recall: 0.9805 - f1_metric: 0.2236 - val_loss: 0.3920 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3775 - acc: 0.8619 - precision: 0.1381 - recall: 0.9942 - f1_metric: 0.2375 - val_loss: 0.3945 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 16 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.4709 - acc: 0.8602 - precision: 0.1394 - recall: 1.0148 - f1_metric: 0.2396 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8668 - precision: 0.1332 - recall: 0.9835 - f1_metric: 0.2302 - val_loss: 0.4154 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3943 - acc: 0.8692 - precision: 0.1308 - recall: 1.0000 - f1_metric: 0.2250 - val_loss: 0.4152 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3959 - acc: 0.8651 - precision: 0.1348 - recall: 0.9988 - f1_metric: 0.2330 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8649 - precision: 0.1350 - recall: 0.9955 - f1_metric: 0.2326 - val_loss: 0.4159 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3981 - acc: 0.8644 - precision: 0.1355 - recall: 1.0000 - f1_metric: 0.2348 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3831 - acc: 0.8716 - precision: 0.1284 - recall: 1.0000 - f1_metric: 0.2235 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3880 - acc: 0.8692 - precision: 0.1307 - recall: 0.9921 - f1_metric: 0.2263 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8647 - precision: 0.1351 - recall: 0.9836 - f1_metric: 0.2331 - val_loss: 0.4125 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3878 - acc: 0.8669 - precision: 0.1331 - recall: 0.9896 - f1_metric: 0.2294 - val_loss: 0.4159 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4155 - acc: 0.8542 - precision: 0.1458 - recall: 0.9977 - f1_metric: 0.2492 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3944 - acc: 0.8644 - precision: 0.1356 - recall: 0.9874 - f1_metric: 0.2350 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3906 - acc: 0.8640 - precision: 0.1360 - recall: 0.9886 - f1_metric: 0.2346 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3855 - acc: 0.8684 - precision: 0.1316 - recall: 0.9925 - f1_metric: 0.2274 - val_loss: 0.4146 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3903 - acc: 0.8665 - precision: 0.1335 - recall: 0.9732 - f1_metric: 0.2304 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4010 - acc: 0.8615 - precision: 0.1385 - recall: 0.9979 - f1_metric: 0.2390 - val_loss: 0.4114 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4002 - acc: 0.8604 - precision: 0.1396 - recall: 0.9930 - f1_metric: 0.2397 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4088 - acc: 0.8572 - precision: 0.1428 - recall: 0.9933 - f1_metric: 0.2455 - val_loss: 0.4156 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3810 - acc: 0.8707 - precision: 0.1293 - recall: 0.9762 - f1_metric: 0.2248 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3968 - acc: 0.8636 - precision: 0.1364 - recall: 0.9802 - f1_metric: 0.2349 - val_loss: 0.4089 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3869 - acc: 0.8649 - precision: 0.1351 - recall: 0.9974 - f1_metric: 0.2331 - val_loss: 0.4081 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3942 - acc: 0.8613 - precision: 0.1387 - recall: 0.9960 - f1_metric: 0.2383 - val_loss: 0.4080 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4039 - acc: 0.8558 - precision: 0.1442 - recall: 0.9734 - f1_metric: 0.2464 - val_loss: 0.4064 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3963 - acc: 0.8592 - precision: 0.1408 - recall: 0.9873 - f1_metric: 0.2411 - val_loss: 0.4050 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3878 - acc: 0.8638 - precision: 0.1362 - recall: 0.9951 - f1_metric: 0.2344 - val_loss: 0.4030 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3808 - acc: 0.8651 - precision: 0.1349 - recall: 0.9921 - f1_metric: 0.2331 - val_loss: 0.3994 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3802 - acc: 0.8653 - precision: 0.1347 - recall: 0.9883 - f1_metric: 0.2325 - val_loss: 0.3977 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3791 - acc: 0.8664 - precision: 0.1336 - recall: 0.9843 - f1_metric: 0.2311 - val_loss: 0.3953 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3727 - acc: 0.8677 - precision: 0.1323 - recall: 0.9799 - f1_metric: 0.2283 - val_loss: 0.3921 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3710 - acc: 0.8673 - precision: 0.1327 - recall: 0.9871 - f1_metric: 0.2302 - val_loss: 0.3886 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 17 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 3s 8ms/step - loss: 0.4664 - acc: 0.8605 - precision: 0.1325 - recall: 0.9435 - f1_metric: 0.2288 - val_loss: 0.4152 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4064 - acc: 0.8615 - precision: 0.1385 - recall: 0.9997 - f1_metric: 0.2387 - val_loss: 0.4144 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4060 - acc: 0.8619 - precision: 0.1381 - recall: 0.9912 - f1_metric: 0.2379 - val_loss: 0.4146 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 983us/step - loss: 0.3964 - acc: 0.8668 - precision: 0.1332 - recall: 0.9747 - f1_metric: 0.2302 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3901 - acc: 0.8667 - precision: 0.1333 - recall: 0.9978 - f1_metric: 0.2312 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3830 - acc: 0.8726 - precision: 0.1274 - recall: 0.9817 - f1_metric: 0.2209 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4066 - acc: 0.8587 - precision: 0.1413 - recall: 0.9970 - f1_metric: 0.2431 - val_loss: 0.4232 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3933 - acc: 0.8650 - precision: 0.1350 - recall: 0.9800 - f1_metric: 0.2324 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3971 - acc: 0.8646 - precision: 0.1354 - recall: 1.0000 - f1_metric: 0.2336 - val_loss: 0.4124 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3887 - acc: 0.8681 - precision: 0.1319 - recall: 0.9840 - f1_metric: 0.2273 - val_loss: 0.4124 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4151 - acc: 0.8525 - precision: 0.1475 - recall: 0.9984 - f1_metric: 0.2506 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3845 - acc: 0.8699 - precision: 0.1301 - recall: 0.9874 - f1_metric: 0.2259 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3849 - acc: 0.8689 - precision: 0.1311 - recall: 0.9892 - f1_metric: 0.2267 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3987 - acc: 0.8620 - precision: 0.1380 - recall: 0.9871 - f1_metric: 0.2374 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3935 - acc: 0.8643 - precision: 0.1357 - recall: 0.9999 - f1_metric: 0.2336 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3807 - acc: 0.8721 - precision: 0.1279 - recall: 0.9451 - f1_metric: 0.2206 - val_loss: 0.4111 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3827 - acc: 0.8694 - precision: 0.1306 - recall: 0.9956 - f1_metric: 0.2262 - val_loss: 0.4108 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3840 - acc: 0.8694 - precision: 0.1306 - recall: 1.0000 - f1_metric: 0.2268 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8609 - precision: 0.1391 - recall: 1.0000 - f1_metric: 0.2390 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3876 - acc: 0.8653 - precision: 0.1347 - recall: 0.9900 - f1_metric: 0.2312 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3865 - acc: 0.8671 - precision: 0.1329 - recall: 0.9973 - f1_metric: 0.2297 - val_loss: 0.4087 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8621 - precision: 0.1379 - recall: 1.0000 - f1_metric: 0.2374 - val_loss: 0.4095 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3810 - acc: 0.8699 - precision: 0.1301 - recall: 0.9973 - f1_metric: 0.2262 - val_loss: 0.4095 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3884 - acc: 0.8650 - precision: 0.1350 - recall: 0.9945 - f1_metric: 0.2339 - val_loss: 0.4058 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3793 - acc: 0.8694 - precision: 0.1306 - recall: 1.0000 - f1_metric: 0.2265 - val_loss: 0.4040 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3806 - acc: 0.8686 - precision: 0.1314 - recall: 0.9961 - f1_metric: 0.2265 - val_loss: 0.4022 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3857 - acc: 0.8634 - precision: 0.1366 - recall: 0.9880 - f1_metric: 0.2356 - val_loss: 0.4012 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3975 - acc: 0.8565 - precision: 0.1435 - recall: 0.9969 - f1_metric: 0.2467 - val_loss: 0.4005 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3819 - acc: 0.8653 - precision: 0.1347 - recall: 0.9981 - f1_metric: 0.2320 - val_loss: 0.3950 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3885 - acc: 0.8592 - precision: 0.1408 - recall: 0.9994 - f1_metric: 0.2412 - val_loss: 0.3922 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 18 Fitting\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 5s 19ms/step - loss: 0.5337 - acc: 0.7314 - precision: 0.1400 - recall: 1.2402 - f1_metric: 0.2459 - val_loss: 0.4163 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4017 - acc: 0.8646 - precision: 0.1369 - recall: 1.0110 - f1_metric: 0.2362 - val_loss: 0.4178 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4039 - acc: 0.8618 - precision: 0.1380 - recall: 0.9793 - f1_metric: 0.2374 - val_loss: 0.4143 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8663 - precision: 0.1334 - recall: 1.0126 - f1_metric: 0.2308 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3973 - acc: 0.8635 - precision: 0.1360 - recall: 0.9770 - f1_metric: 0.2340 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8651 - precision: 0.1339 - recall: 1.0046 - f1_metric: 0.2314 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3923 - acc: 0.8672 - precision: 0.1326 - recall: 0.9870 - f1_metric: 0.2290 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4027 - acc: 0.8611 - precision: 0.1396 - recall: 1.0317 - f1_metric: 0.2420 - val_loss: 0.4150 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3898 - acc: 0.8672 - precision: 0.1329 - recall: 0.9863 - f1_metric: 0.2304 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3992 - acc: 0.8621 - precision: 0.1376 - recall: 1.0404 - f1_metric: 0.2379 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4030 - acc: 0.8617 - precision: 0.1383 - recall: 1.0392 - f1_metric: 0.2392 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3910 - acc: 0.8673 - precision: 0.1343 - recall: 1.0316 - f1_metric: 0.2333 - val_loss: 0.4121 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8656 - precision: 0.1338 - recall: 1.0067 - f1_metric: 0.2314 - val_loss: 0.4116 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3866 - acc: 0.8689 - precision: 0.1308 - recall: 0.9919 - f1_metric: 0.2276 - val_loss: 0.4124 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4030 - acc: 0.8587 - precision: 0.1412 - recall: 0.9976 - f1_metric: 0.2426 - val_loss: 0.4112 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4070 - acc: 0.8562 - precision: 0.1435 - recall: 0.9743 - f1_metric: 0.2461 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3929 - acc: 0.8660 - precision: 0.1335 - recall: 0.9892 - f1_metric: 0.2315 - val_loss: 0.4108 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3836 - acc: 0.8708 - precision: 0.1291 - recall: 0.9906 - f1_metric: 0.2237 - val_loss: 0.4101 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3949 - acc: 0.8629 - precision: 0.1371 - recall: 0.9987 - f1_metric: 0.2366 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3929 - acc: 0.8644 - precision: 0.1356 - recall: 0.9847 - f1_metric: 0.2342 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3947 - acc: 0.8622 - precision: 0.1378 - recall: 0.9867 - f1_metric: 0.2373 - val_loss: 0.4105 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3866 - acc: 0.8670 - precision: 0.1330 - recall: 1.0000 - f1_metric: 0.2298 - val_loss: 0.4080 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4032 - acc: 0.8555 - precision: 0.1445 - recall: 0.9904 - f1_metric: 0.2468 - val_loss: 0.4073 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3932 - acc: 0.8630 - precision: 0.1370 - recall: 1.0000 - f1_metric: 0.2361 - val_loss: 0.4065 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3904 - acc: 0.8633 - precision: 0.1367 - recall: 0.9911 - f1_metric: 0.2362 - val_loss: 0.4043 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3937 - acc: 0.8594 - precision: 0.1406 - recall: 0.9889 - f1_metric: 0.2415 - val_loss: 0.4029 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3681 - acc: 0.8740 - precision: 0.1260 - recall: 0.9907 - f1_metric: 0.2186 - val_loss: 0.4028 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3743 - acc: 0.8704 - precision: 0.1296 - recall: 0.9895 - f1_metric: 0.2255 - val_loss: 0.4004 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3992 - acc: 0.8534 - precision: 0.1466 - recall: 0.9995 - f1_metric: 0.2511 - val_loss: 0.4028 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3817 - acc: 0.8633 - precision: 0.1367 - recall: 1.0000 - f1_metric: 0.2348 - val_loss: 0.4021 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 19 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 14ms/step - loss: 0.4726 - acc: 0.8513 - precision: 0.1363 - recall: 0.9625 - f1_metric: 0.2338 - val_loss: 0.4143 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4086 - acc: 0.8615 - precision: 0.1385 - recall: 0.9904 - f1_metric: 0.2387 - val_loss: 0.4144 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3824 - acc: 0.8732 - precision: 0.1268 - recall: 0.9995 - f1_metric: 0.2194 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3956 - acc: 0.8662 - precision: 0.1338 - recall: 0.9911 - f1_metric: 0.2299 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3984 - acc: 0.8637 - precision: 0.1363 - recall: 1.0000 - f1_metric: 0.2349 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4055 - acc: 0.8599 - precision: 0.1401 - recall: 0.9888 - f1_metric: 0.2410 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4032 - acc: 0.8607 - precision: 0.1393 - recall: 0.9821 - f1_metric: 0.2396 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4038 - acc: 0.8620 - precision: 0.1380 - recall: 0.9969 - f1_metric: 0.2385 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8614 - precision: 0.1386 - recall: 0.9982 - f1_metric: 0.2376 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3991 - acc: 0.8626 - precision: 0.1374 - recall: 1.0000 - f1_metric: 0.2378 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4035 - acc: 0.8604 - precision: 0.1396 - recall: 0.9946 - f1_metric: 0.2401 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4079 - acc: 0.8575 - precision: 0.1425 - recall: 0.9704 - f1_metric: 0.2427 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3980 - acc: 0.8621 - precision: 0.1379 - recall: 0.9814 - f1_metric: 0.2370 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3970 - acc: 0.8618 - precision: 0.1382 - recall: 0.9969 - f1_metric: 0.2376 - val_loss: 0.4117 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3862 - acc: 0.8687 - precision: 0.1313 - recall: 0.9962 - f1_metric: 0.2268 - val_loss: 0.4108 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3857 - acc: 0.8690 - precision: 0.1310 - recall: 0.9762 - f1_metric: 0.2262 - val_loss: 0.4161 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3881 - acc: 0.8663 - precision: 0.1337 - recall: 1.0000 - f1_metric: 0.2312 - val_loss: 0.4101 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3823 - acc: 0.8698 - precision: 0.1302 - recall: 1.0000 - f1_metric: 0.2263 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3888 - acc: 0.8660 - precision: 0.1340 - recall: 0.9927 - f1_metric: 0.2310 - val_loss: 0.4093 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3889 - acc: 0.8657 - precision: 0.1344 - recall: 0.9798 - f1_metric: 0.2315 - val_loss: 0.4087 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3939 - acc: 0.8614 - precision: 0.1386 - recall: 1.0000 - f1_metric: 0.2395 - val_loss: 0.4085 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8630 - precision: 0.1370 - recall: 0.9917 - f1_metric: 0.2360 - val_loss: 0.4073 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3919 - acc: 0.8641 - precision: 0.1358 - recall: 1.0000 - f1_metric: 0.2342 - val_loss: 0.4065 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3948 - acc: 0.8613 - precision: 0.1388 - recall: 1.0000 - f1_metric: 0.2391 - val_loss: 0.4055 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3868 - acc: 0.8636 - precision: 0.1364 - recall: 0.9819 - f1_metric: 0.2354 - val_loss: 0.4033 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3962 - acc: 0.8590 - precision: 0.1410 - recall: 0.9936 - f1_metric: 0.2432 - val_loss: 0.4027 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3845 - acc: 0.8637 - precision: 0.1363 - recall: 0.9989 - f1_metric: 0.2352 - val_loss: 0.4010 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3901 - acc: 0.8591 - precision: 0.1409 - recall: 0.9985 - f1_metric: 0.2428 - val_loss: 0.4005 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3766 - acc: 0.8678 - precision: 0.1322 - recall: 0.9942 - f1_metric: 0.2285 - val_loss: 0.3944 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3850 - acc: 0.8594 - precision: 0.1406 - recall: 0.9998 - f1_metric: 0.2415 - val_loss: 0.3959 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 20 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 15ms/step - loss: 0.4866 - acc: 0.8646 - precision: 0.1277 - recall: 1.1259 - f1_metric: 0.2251 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3952 - acc: 0.8664 - precision: 0.1337 - recall: 0.9935 - f1_metric: 0.2305 - val_loss: 0.4151 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4082 - acc: 0.8592 - precision: 0.1408 - recall: 0.9986 - f1_metric: 0.2418 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4165 - acc: 0.8549 - precision: 0.1451 - recall: 0.9850 - f1_metric: 0.2480 - val_loss: 0.4162 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3971 - acc: 0.8643 - precision: 0.1357 - recall: 0.9855 - f1_metric: 0.2338 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4055 - acc: 0.8585 - precision: 0.1415 - recall: 0.9979 - f1_metric: 0.2419 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8627 - precision: 0.1373 - recall: 0.9913 - f1_metric: 0.2363 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3886 - acc: 0.8687 - precision: 0.1313 - recall: 0.9879 - f1_metric: 0.2273 - val_loss: 0.4125 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4109 - acc: 0.8576 - precision: 0.1424 - recall: 0.9910 - f1_metric: 0.2441 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3976 - acc: 0.8632 - precision: 0.1368 - recall: 0.9976 - f1_metric: 0.2348 - val_loss: 0.4125 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4000 - acc: 0.8624 - precision: 0.1376 - recall: 0.9824 - f1_metric: 0.2370 - val_loss: 0.4149 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4055 - acc: 0.8586 - precision: 0.1414 - recall: 0.9979 - f1_metric: 0.2435 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3940 - acc: 0.8657 - precision: 0.1344 - recall: 0.9968 - f1_metric: 0.2331 - val_loss: 0.4117 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3811 - acc: 0.8726 - precision: 0.1274 - recall: 0.9884 - f1_metric: 0.22 - 1s 3ms/step - loss: 0.3816 - acc: 0.8723 - precision: 0.1277 - recall: 0.9883 - f1_metric: 0.2213 - val_loss: 0.4121 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3917 - acc: 0.8657 - precision: 0.1343 - recall: 0.9960 - f1_metric: 0.2331 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3881 - acc: 0.8665 - precision: 0.1336 - recall: 0.9935 - f1_metric: 0.2309 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3924 - acc: 0.8647 - precision: 0.1353 - recall: 0.9914 - f1_metric: 0.2330 - val_loss: 0.4099 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8602 - precision: 0.1398 - recall: 1.0000 - f1_metric: 0.2401 - val_loss: 0.4098 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3942 - acc: 0.8616 - precision: 0.1384 - recall: 0.9997 - f1_metric: 0.2388 - val_loss: 0.4085 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3860 - acc: 0.8658 - precision: 0.1342 - recall: 0.9706 - f1_metric: 0.2310 - val_loss: 0.4090 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3883 - acc: 0.8665 - precision: 0.1335 - recall: 0.9845 - f1_metric: 0.2310 - val_loss: 0.4084 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3715 - acc: 0.8736 - precision: 0.1264 - recall: 1.0000 - f1_metric: 0.2195 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4006 - acc: 0.8575 - precision: 0.1425 - recall: 0.9871 - f1_metric: 0.2448 - val_loss: 0.4065 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3842 - acc: 0.8669 - precision: 0.1331 - recall: 0.9897 - f1_metric: 0.2304 - val_loss: 0.4084 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3771 - acc: 0.8684 - precision: 0.1316 - recall: 0.9882 - f1_metric: 0.2278 - val_loss: 0.4014 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3938 - acc: 0.8596 - precision: 0.1404 - recall: 0.9856 - f1_metric: 0.2412 - val_loss: 0.4004 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3817 - acc: 0.8646 - precision: 0.1354 - recall: 0.9761 - f1_metric: 0.2339 - val_loss: 0.3993 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3769 - acc: 0.8661 - precision: 0.1339 - recall: 0.9981 - f1_metric: 0.2316 - val_loss: 0.4007 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3696 - acc: 0.8700 - precision: 0.1300 - recall: 0.9946 - f1_metric: 0.2246 - val_loss: 0.3949 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3790 - acc: 0.8623 - precision: 0.1376 - recall: 0.9967 - f1_metric: 0.2380 - val_loss: 0.3930 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 21 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 6s 20ms/step - loss: 0.5759 - acc: 0.7226 - precision: 0.1413 - recall: 1.4865 - f1_metric: 0.2531 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4024 - acc: 0.8636 - precision: 0.1380 - recall: 1.0121 - f1_metric: 0.2370 - val_loss: 0.4156 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4107 - acc: 0.8573 - precision: 0.1435 - recall: 1.0144 - f1_metric: 0.2479 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4010 - acc: 0.8638 - precision: 0.1368 - recall: 1.0111 - f1_metric: 0.2362 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4034 - acc: 0.8620 - precision: 0.1375 - recall: 1.0115 - f1_metric: 0.2376 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4017 - acc: 0.8629 - precision: 0.1362 - recall: 0.9915 - f1_metric: 0.2356 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3995 - acc: 0.8650 - precision: 0.1347 - recall: 1.0068 - f1_metric: 0.2336 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3929 - acc: 0.8656 - precision: 0.1346 - recall: 1.0127 - f1_metric: 0.2333 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8672 - precision: 0.1327 - recall: 1.0112 - f1_metric: 0.2295 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3899 - acc: 0.8660 - precision: 0.1341 - recall: 1.0055 - f1_metric: 0.2324 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3874 - acc: 0.8675 - precision: 0.1320 - recall: 1.0044 - f1_metric: 0.2288 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4015 - acc: 0.8612 - precision: 0.1388 - recall: 0.9916 - f1_metric: 0.2390 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3900 - acc: 0.8674 - precision: 0.1325 - recall: 1.0038 - f1_metric: 0.2294 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3863 - acc: 0.8691 - precision: 0.1310 - recall: 1.0071 - f1_metric: 0.2273 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3921 - acc: 0.8674 - precision: 0.1325 - recall: 0.9776 - f1_metric: 0.2287 - val_loss: 0.4120 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3964 - acc: 0.8629 - precision: 0.1374 - recall: 1.0021 - f1_metric: 0.2369 - val_loss: 0.4116 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3900 - acc: 0.8656 - precision: 0.1343 - recall: 1.0009 - f1_metric: 0.2332 - val_loss: 0.4116 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4031 - acc: 0.8593 - precision: 0.1404 - recall: 0.9920 - f1_metric: 0.2409 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4000 - acc: 0.8611 - precision: 0.1388 - recall: 1.0015 - f1_metric: 0.2385 - val_loss: 0.4109 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4062 - acc: 0.8561 - precision: 0.1436 - recall: 0.9987 - f1_metric: 0.2464 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3958 - acc: 0.8624 - precision: 0.1375 - recall: 0.9970 - f1_metric: 0.2369 - val_loss: 0.4102 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4037 - acc: 0.8581 - precision: 0.1423 - recall: 1.0013 - f1_metric: 0.2442 - val_loss: 0.4117 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3988 - acc: 0.8596 - precision: 0.1404 - recall: 0.9932 - f1_metric: 0.2412 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3890 - acc: 0.8652 - precision: 0.1348 - recall: 0.9686 - f1_metric: 0.2319 - val_loss: 0.4089 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3790 - acc: 0.8696 - precision: 0.1304 - recall: 0.9897 - f1_metric: 0.2255 - val_loss: 0.4089 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3879 - acc: 0.8649 - precision: 0.1351 - recall: 0.9851 - f1_metric: 0.2336 - val_loss: 0.4076 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3950 - acc: 0.8621 - precision: 0.1379 - recall: 1.0000 - f1_metric: 0.2380 - val_loss: 0.4098 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3916 - acc: 0.8627 - precision: 0.1373 - recall: 0.9773 - f1_metric: 0.2362 - val_loss: 0.4053 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3941 - acc: 0.8614 - precision: 0.1386 - recall: 0.9973 - f1_metric: 0.2386 - val_loss: 0.4096 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4036 - acc: 0.8577 - precision: 0.1423 - recall: 0.9884 - f1_metric: 0.2434 - val_loss: 0.4059 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 22 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 3s 7ms/step - loss: 0.4274 - acc: 0.8681 - precision: 0.1321 - recall: 0.9954 - f1_metric: 0.2287 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4037 - acc: 0.8631 - precision: 0.1369 - recall: 0.9871 - f1_metric: 0.2359 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4017 - acc: 0.8618 - precision: 0.1381 - recall: 0.9981 - f1_metric: 0.2380 - val_loss: 0.4148 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3888 - acc: 0.8702 - precision: 0.1298 - recall: 1.0000 - f1_metric: 0.2253 - val_loss: 0.4151 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4037 - acc: 0.8635 - precision: 0.1360 - recall: 0.9860 - f1_metric: 0.2350 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3905 - acc: 0.8673 - precision: 0.1327 - recall: 0.9964 - f1_metric: 0.2298 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4020 - acc: 0.8611 - precision: 0.1389 - recall: 0.9832 - f1_metric: 0.2381 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4001 - acc: 0.8619 - precision: 0.1381 - recall: 0.9836 - f1_metric: 0.2376 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3912 - acc: 0.8679 - precision: 0.1321 - recall: 0.9833 - f1_metric: 0.2294 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4008 - acc: 0.8613 - precision: 0.1387 - recall: 0.9972 - f1_metric: 0.2383 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4010 - acc: 0.8599 - precision: 0.1401 - recall: 0.9975 - f1_metric: 0.2394 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4012 - acc: 0.8612 - precision: 0.1388 - recall: 0.9939 - f1_metric: 0.2387 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3900 - acc: 0.8669 - precision: 0.1331 - recall: 0.9940 - f1_metric: 0.2295 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3912 - acc: 0.8661 - precision: 0.1339 - recall: 0.9961 - f1_metric: 0.2318 - val_loss: 0.4102 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8615 - precision: 0.1385 - recall: 0.9945 - f1_metric: 0.2384 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3905 - acc: 0.8651 - precision: 0.1349 - recall: 0.9638 - f1_metric: 0.2323 - val_loss: 0.4114 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3910 - acc: 0.8647 - precision: 0.1353 - recall: 0.9817 - f1_metric: 0.2340 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3793 - acc: 0.8692 - precision: 0.1308 - recall: 0.9820 - f1_metric: 0.2263 - val_loss: 0.4074 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3930 - acc: 0.8638 - precision: 0.1362 - recall: 0.9942 - f1_metric: 0.2348 - val_loss: 0.4069 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3971 - acc: 0.8601 - precision: 0.1399 - recall: 0.9927 - f1_metric: 0.2403 - val_loss: 0.4049 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3796 - acc: 0.8682 - precision: 0.1318 - recall: 0.9943 - f1_metric: 0.2286 - val_loss: 0.4063 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3771 - acc: 0.8714 - precision: 0.1286 - recall: 0.9861 - f1_metric: 0.2223 - val_loss: 0.4026 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3906 - acc: 0.8621 - precision: 0.1379 - recall: 0.9972 - f1_metric: 0.2374 - val_loss: 0.4047 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3727 - acc: 0.8722 - precision: 0.1278 - recall: 0.9846 - f1_metric: 0.2202 - val_loss: 0.3986 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3830 - acc: 0.8630 - precision: 0.1370 - recall: 0.9985 - f1_metric: 0.2368 - val_loss: 0.3952 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3708 - acc: 0.8690 - precision: 0.1310 - recall: 0.9964 - f1_metric: 0.2268 - val_loss: 0.4036 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3743 - acc: 0.8667 - precision: 0.1333 - recall: 0.9890 - f1_metric: 0.2308 - val_loss: 0.3926 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3673 - acc: 0.8686 - precision: 0.1314 - recall: 0.9827 - f1_metric: 0.2279 - val_loss: 0.3979 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3663 - acc: 0.8667 - precision: 0.1333 - recall: 0.9980 - f1_metric: 0.2308 - val_loss: 0.3828 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3639 - acc: 0.8683 - precision: 0.1317 - recall: 0.9891 - f1_metric: 0.2281 - val_loss: 0.3805 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 23 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 2s 6ms/step - loss: 0.5051 - acc: 0.7839 - precision: 0.1402 - recall: 0.9480 - f1_metric: 0.2403 - val_loss: 0.4173 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3921 - acc: 0.8712 - precision: 0.1288 - recall: 0.9705 - f1_metric: 0.2225 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3829 - acc: 0.8729 - precision: 0.1271 - recall: 0.9726 - f1_metric: 0.2200 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3894 - acc: 0.8710 - precision: 0.1290 - recall: 0.9940 - f1_metric: 0.2240 - val_loss: 0.4161 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3988 - acc: 0.8656 - precision: 0.1344 - recall: 0.9955 - f1_metric: 0.2323 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3999 - acc: 0.8613 - precision: 0.1387 - recall: 0.9942 - f1_metric: 0.2379 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3981 - acc: 0.8627 - precision: 0.1373 - recall: 0.9890 - f1_metric: 0.2371 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3956 - acc: 0.8630 - precision: 0.1370 - recall: 0.9866 - f1_metric: 0.2361 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4178 - acc: 0.8530 - precision: 0.1470 - recall: 0.9849 - f1_metric: 0.2507 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3940 - acc: 0.8655 - precision: 0.1345 - recall: 0.9902 - f1_metric: 0.2330 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3931 - acc: 0.8655 - precision: 0.1345 - recall: 1.0000 - f1_metric: 0.2328 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3996 - acc: 0.8610 - precision: 0.1390 - recall: 0.9981 - f1_metric: 0.2391 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4027 - acc: 0.8600 - precision: 0.1400 - recall: 0.9936 - f1_metric: 0.2410 - val_loss: 0.4147 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4048 - acc: 0.8585 - precision: 0.1415 - recall: 0.9968 - f1_metric: 0.2423 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3884 - acc: 0.8662 - precision: 0.1338 - recall: 0.9999 - f1_metric: 0.2321 - val_loss: 0.4107 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3926 - acc: 0.8646 - precision: 0.1354 - recall: 0.9983 - f1_metric: 0.2338 - val_loss: 0.4105 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3862 - acc: 0.8655 - precision: 0.1345 - recall: 0.9663 - f1_metric: 0.2330 - val_loss: 0.4095 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 980us/step - loss: 0.3867 - acc: 0.8684 - precision: 0.1316 - recall: 0.9674 - f1_metric: 0.2269 - val_loss: 0.4117 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3940 - acc: 0.8624 - precision: 0.1376 - recall: 0.9887 - f1_metric: 0.2368 - val_loss: 0.4084 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3893 - acc: 0.8641 - precision: 0.1359 - recall: 0.9966 - f1_metric: 0.2342 - val_loss: 0.4078 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3846 - acc: 0.8668 - precision: 0.1332 - recall: 0.9903 - f1_metric: 0.2306 - val_loss: 0.4083 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3832 - acc: 0.8685 - precision: 0.1315 - recall: 0.9801 - f1_metric: 0.2276 - val_loss: 0.4064 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3894 - acc: 0.8645 - precision: 0.1355 - recall: 0.9864 - f1_metric: 0.2337 - val_loss: 0.4050 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3839 - acc: 0.8664 - precision: 0.1336 - recall: 0.9991 - f1_metric: 0.2313 - val_loss: 0.4032 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3919 - acc: 0.8609 - precision: 0.1391 - recall: 0.9992 - f1_metric: 0.2393 - val_loss: 0.3999 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 980us/step - loss: 0.3805 - acc: 0.8664 - precision: 0.1336 - recall: 0.9815 - f1_metric: 0.2308 - val_loss: 0.3998 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3878 - acc: 0.8609 - precision: 0.1391 - recall: 0.9926 - f1_metric: 0.2398 - val_loss: 0.3956 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3657 - acc: 0.8739 - precision: 0.1261 - recall: 0.9751 - f1_metric: 0.2180 - val_loss: 0.3922 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3659 - acc: 0.8699 - precision: 0.1301 - recall: 0.9656 - f1_metric: 0.2254 - val_loss: 0.3918 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3590 - acc: 0.8730 - precision: 0.1270 - recall: 0.9932 - f1_metric: 0.2206 - val_loss: 0.3867 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 24 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 3s 6ms/step - loss: 0.4812 - acc: 0.8391 - precision: 0.1369 - recall: 1.0045 - f1_metric: 0.2364 - val_loss: 0.4166 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4122 - acc: 0.8584 - precision: 0.1416 - recall: 1.0000 - f1_metric: 0.2442 - val_loss: 0.4146 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4031 - acc: 0.8612 - precision: 0.1387 - recall: 0.9705 - f1_metric: 0.2370 - val_loss: 0.4172 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3970 - acc: 0.8666 - precision: 0.1333 - recall: 0.9920 - f1_metric: 0.2307 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3889 - acc: 0.8677 - precision: 0.1323 - recall: 0.9974 - f1_metric: 0.2280 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3940 - acc: 0.8648 - precision: 0.1352 - recall: 1.0000 - f1_metric: 0.2341 - val_loss: 0.4163 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3903 - acc: 0.8683 - precision: 0.1317 - recall: 0.9997 - f1_metric: 0.2275 - val_loss: 0.4169 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3927 - acc: 0.8661 - precision: 0.1339 - recall: 0.9786 - f1_metric: 0.2305 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4065 - acc: 0.8603 - precision: 0.1397 - recall: 1.0000 - f1_metric: 0.2406 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3968 - acc: 0.8624 - precision: 0.1376 - recall: 0.9834 - f1_metric: 0.2372 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3949 - acc: 0.8644 - precision: 0.1356 - recall: 0.9813 - f1_metric: 0.2338 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3866 - acc: 0.8672 - precision: 0.1328 - recall: 0.9813 - f1_metric: 0.22 - 1s 3ms/step - loss: 0.3871 - acc: 0.8670 - precision: 0.1330 - recall: 0.9811 - f1_metric: 0.2292 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4022 - acc: 0.8594 - precision: 0.1406 - recall: 0.9808 - f1_metric: 0.2403 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4013 - acc: 0.8596 - precision: 0.1404 - recall: 0.9911 - f1_metric: 0.2419 - val_loss: 0.4150 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4033 - acc: 0.8590 - precision: 0.1410 - recall: 0.9930 - f1_metric: 0.2421 - val_loss: 0.4103 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3924 - acc: 0.8640 - precision: 0.1360 - recall: 0.9814 - f1_metric: 0.2338 - val_loss: 0.4111 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3899 - acc: 0.8648 - precision: 0.1352 - recall: 1.0000 - f1_metric: 0.2339 - val_loss: 0.4094 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8599 - precision: 0.1401 - recall: 0.9923 - f1_metric: 0.2406 - val_loss: 0.4092 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4027 - acc: 0.8575 - precision: 0.1425 - recall: 0.9945 - f1_metric: 0.2453 - val_loss: 0.4096 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4100 - acc: 0.8524 - precision: 0.1476 - recall: 0.9900 - f1_metric: 0.2515 - val_loss: 0.4069 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4022 - acc: 0.8576 - precision: 0.1424 - recall: 1.0000 - f1_metric: 0.2445 - val_loss: 0.4054 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8571 - precision: 0.1429 - recall: 0.9832 - f1_metric: 0.2456 - val_loss: 0.4052 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3819 - acc: 0.8680 - precision: 0.1320 - recall: 0.9692 - f1_metric: 0.2275 - val_loss: 0.4029 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3909 - acc: 0.8594 - precision: 0.1406 - recall: 0.9892 - f1_metric: 0.2415 - val_loss: 0.4020 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3732 - acc: 0.8694 - precision: 0.1306 - recall: 0.9974 - f1_metric: 0.2270 - val_loss: 0.3977 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3696 - acc: 0.8722 - precision: 0.1278 - recall: 0.9885 - f1_metric: 0.2216 - val_loss: 0.3976 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3803 - acc: 0.8636 - precision: 0.1364 - recall: 0.9914 - f1_metric: 0.2352 - val_loss: 0.3932 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3712 - acc: 0.8690 - precision: 0.1310 - recall: 0.9977 - f1_metric: 0.2262 - val_loss: 0.4050 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3796 - acc: 0.8611 - precision: 0.1389 - recall: 0.9992 - f1_metric: 0.2399 - val_loss: 0.3878 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3707 - acc: 0.8636 - precision: 0.1364 - recall: 0.9881 - f1_metric: 0.2351 - val_loss: 0.3835 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2456\n",
      "Model 25 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.4767 - acc: 0.8632 - precision: 0.1365 - recall: 1.1793 - f1_metric: 0.2385 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3892 - acc: 0.8700 - precision: 0.1295 - recall: 1.0297 - f1_metric: 0.2261 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4188 - acc: 0.8509 - precision: 0.1494 - recall: 1.0240 - f1_metric: 0.2556 - val_loss: 0.4169 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3761 - acc: 0.8748 - precision: 0.1258 - recall: 1.0505 - f1_metric: 0.2202 - val_loss: 0.4160 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4004 - acc: 0.8632 - precision: 0.1361 - recall: 1.0764 - f1_metric: 0.2365 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4107 - acc: 0.8598 - precision: 0.1400 - recall: 1.0739 - f1_metric: 0.2423 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3917 - acc: 0.8658 - precision: 0.1312 - recall: 1.0840 - f1_metric: 0.2301 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3946 - acc: 0.8646 - precision: 0.1327 - recall: 1.0553 - f1_metric: 0.2315 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8661 - precision: 0.1323 - recall: 1.0337 - f1_metric: 0.2304 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8648 - precision: 0.1340 - recall: 1.0803 - f1_metric: 0.2346 - val_loss: 0.4121 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3875 - acc: 0.8695 - precision: 0.1289 - recall: 1.0695 - f1_metric: 0.2257 - val_loss: 0.4116 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3919 - acc: 0.8649 - precision: 0.1345 - recall: 1.0269 - f1_metric: 0.2324 - val_loss: 0.4113 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8620 - precision: 0.1361 - recall: 1.0386 - f1_metric: 0.2363 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3921 - acc: 0.8635 - precision: 0.1360 - recall: 1.0332 - f1_metric: 0.2372 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4001 - acc: 0.8601 - precision: 0.1418 - recall: 1.0553 - f1_metric: 0.2448 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4117 - acc: 0.8541 - precision: 0.1457 - recall: 1.0366 - f1_metric: 0.2513 - val_loss: 0.4118 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3861 - acc: 0.8668 - precision: 0.1335 - recall: 1.0184 - f1_metric: 0.2320 - val_loss: 0.4087 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3786 - acc: 0.8697 - precision: 0.1308 - recall: 0.9967 - f1_metric: 0.2272 - val_loss: 0.4080 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3976 - acc: 0.8596 - precision: 0.1397 - recall: 1.0023 - f1_metric: 0.2402 - val_loss: 0.4070 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3972 - acc: 0.8605 - precision: 0.1391 - recall: 0.9977 - f1_metric: 0.2389 - val_loss: 0.4069 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3791 - acc: 0.8679 - precision: 0.1321 - recall: 1.0014 - f1_metric: 0.2295 - val_loss: 0.4047 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3802 - acc: 0.8678 - precision: 0.1321 - recall: 0.9831 - f1_metric: 0.2291 - val_loss: 0.4033 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4035 - acc: 0.8562 - precision: 0.1438 - recall: 0.9989 - f1_metric: 0.2463 - val_loss: 0.4059 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3879 - acc: 0.8628 - precision: 0.1370 - recall: 1.0000 - f1_metric: 0.2365 - val_loss: 0.4031 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3952 - acc: 0.8568 - precision: 0.1432 - recall: 0.9929 - f1_metric: 0.2441 - val_loss: 0.3978 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3727 - acc: 0.8685 - precision: 0.1314 - recall: 1.0000 - f1_metric: 0.2281 - val_loss: 0.3949 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3631 - acc: 0.8715 - precision: 0.1285 - recall: 0.9801 - f1_metric: 0.2230 - val_loss: 0.3908 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3692 - acc: 0.8660 - precision: 0.1340 - recall: 1.0000 - f1_metric: 0.2308 - val_loss: 0.3887 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3815 - acc: 0.8583 - precision: 0.1417 - recall: 0.9946 - f1_metric: 0.2435 - val_loss: 0.3838 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3676 - acc: 0.8654 - precision: 0.1346 - recall: 0.9980 - f1_metric: 0.2333 - val_loss: 0.3807 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 26 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 18ms/step - loss: 0.5056 - acc: 0.7795 - precision: 0.1337 - recall: 1.0023 - f1_metric: 0.2306 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3932 - acc: 0.8706 - precision: 0.1294 - recall: 0.9787 - f1_metric: 0.2241 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3989 - acc: 0.8663 - precision: 0.1337 - recall: 0.9839 - f1_metric: 0.2292 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3990 - acc: 0.8635 - precision: 0.1365 - recall: 0.9963 - f1_metric: 0.2347 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3919 - acc: 0.8679 - precision: 0.1321 - recall: 0.9985 - f1_metric: 0.2288 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4033 - acc: 0.8622 - precision: 0.1378 - recall: 0.9901 - f1_metric: 0.2372 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4003 - acc: 0.8619 - precision: 0.1381 - recall: 0.9917 - f1_metric: 0.2376 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3855 - acc: 0.8700 - precision: 0.1300 - recall: 0.9856 - f1_metric: 0.2256 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4000 - acc: 0.8610 - precision: 0.1390 - recall: 0.9833 - f1_metric: 0.2391 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4003 - acc: 0.8624 - precision: 0.1376 - recall: 1.0000 - f1_metric: 0.2375 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4052 - acc: 0.8596 - precision: 0.1404 - recall: 0.9948 - f1_metric: 0.2402 - val_loss: 0.4144 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3901 - acc: 0.8654 - precision: 0.1346 - recall: 0.9747 - f1_metric: 0.2320 - val_loss: 0.4121 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4085 - acc: 0.8566 - precision: 0.1434 - recall: 0.9925 - f1_metric: 0.2455 - val_loss: 0.4154 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4054 - acc: 0.8597 - precision: 0.1403 - recall: 0.9750 - f1_metric: 0.2415 - val_loss: 0.4113 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3846 - acc: 0.8671 - precision: 0.1329 - recall: 0.9710 - f1_metric: 0.2301 - val_loss: 0.4111 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8593 - precision: 0.1407 - recall: 0.9821 - f1_metric: 0.2416 - val_loss: 0.4114 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3961 - acc: 0.8641 - precision: 0.1359 - recall: 0.9997 - f1_metric: 0.2351 - val_loss: 0.4164 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4005 - acc: 0.8600 - precision: 0.1400 - recall: 0.9985 - f1_metric: 0.2407 - val_loss: 0.4106 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3916 - acc: 0.8634 - precision: 0.1366 - recall: 0.9846 - f1_metric: 0.2360 - val_loss: 0.4100 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3901 - acc: 0.8642 - precision: 0.1358 - recall: 1.0000 - f1_metric: 0.2354 - val_loss: 0.4092 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3932 - acc: 0.8641 - precision: 0.1359 - recall: 0.9753 - f1_metric: 0.2339 - val_loss: 0.4080 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3937 - acc: 0.8635 - precision: 0.1365 - recall: 0.9996 - f1_metric: 0.2355 - val_loss: 0.4074 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3877 - acc: 0.8662 - precision: 0.1338 - recall: 0.9762 - f1_metric: 0.2307 - val_loss: 0.4088 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3836 - acc: 0.8668 - precision: 0.1332 - recall: 0.9810 - f1_metric: 0.2302 - val_loss: 0.4052 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3837 - acc: 0.8658 - precision: 0.1342 - recall: 0.9963 - f1_metric: 0.2317 - val_loss: 0.4036 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3823 - acc: 0.8660 - precision: 0.1340 - recall: 0.9990 - f1_metric: 0.2312 - val_loss: 0.4056 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3696 - acc: 0.8708 - precision: 0.1290 - recall: 1.0000 - f1_metric: 0.2245 - val_loss: 0.4021 - val_acc: 0.8537 - val_precision: 0.1437 - val_recall: 0.9808 - val_f1_metric: 0.2458\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3884 - acc: 0.8604 - precision: 0.1394 - recall: 0.9979 - f1_metric: 0.2399 - val_loss: 0.3987 - val_acc: 0.8531 - val_precision: 0.1437 - val_recall: 0.9808 - val_f1_metric: 0.2458\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3799 - acc: 0.8670 - precision: 0.1330 - recall: 0.9909 - f1_metric: 0.2298 - val_loss: 0.3950 - val_acc: 0.8531 - val_precision: 0.1437 - val_recall: 0.9808 - val_f1_metric: 0.2458\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3815 - acc: 0.8616 - precision: 0.1381 - recall: 0.9854 - f1_metric: 0.2366 - val_loss: 0.3910 - val_acc: 0.8531 - val_precision: 0.1437 - val_recall: 0.9808 - val_f1_metric: 0.2458\n",
      "Model 27 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 13ms/step - loss: 0.5045 - acc: 0.8040 - precision: 0.1349 - recall: 1.2848 - f1_metric: 0.2389 - val_loss: 0.4145 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3986 - acc: 0.8656 - precision: 0.1342 - recall: 1.0029 - f1_metric: 0.23 - 1s 3ms/step - loss: 0.3987 - acc: 0.8655 - precision: 0.1344 - recall: 1.0034 - f1_metric: 0.2326 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3955 - acc: 0.8671 - precision: 0.1327 - recall: 1.0150 - f1_metric: 0.2309 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4166 - acc: 0.8559 - precision: 0.1426 - recall: 1.0567 - f1_metric: 0.2473 - val_loss: 0.4151 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3947 - acc: 0.8647 - precision: 0.1352 - recall: 1.0587 - f1_metric: 0.2352 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3956 - acc: 0.8651 - precision: 0.1333 - recall: 1.0582 - f1_metric: 0.2327 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8676 - precision: 0.1309 - recall: 1.0298 - f1_metric: 0.2272 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3985 - acc: 0.8649 - precision: 0.1343 - recall: 1.0615 - f1_metric: 0.2329 - val_loss: 0.4128 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4013 - acc: 0.8625 - precision: 0.1360 - recall: 1.0315 - f1_metric: 0.2357 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3979 - acc: 0.8634 - precision: 0.1353 - recall: 1.0746 - f1_metric: 0.2363 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.3947 - acc: 0.8659 - precision: 0.1309 - recall: 1.0699 - f1_metric: 0.2290 - val_loss: 0.4129 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3896 - acc: 0.8677 - precision: 0.1316 - recall: 1.0479 - f1_metric: 0.2291 - val_loss: 0.4124 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3950 - acc: 0.8656 - precision: 0.1342 - recall: 1.0553 - f1_metric: 0.2333 - val_loss: 0.4119 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4055 - acc: 0.8589 - precision: 0.1431 - recall: 1.0633 - f1_metric: 0.2479 - val_loss: 0.4114 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4001 - acc: 0.8597 - precision: 0.1391 - recall: 1.0465 - f1_metric: 0.2411 - val_loss: 0.4120 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3940 - acc: 0.8640 - precision: 0.1374 - recall: 1.0488 - f1_metric: 0.2388 - val_loss: 0.4112 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3935 - acc: 0.8646 - precision: 0.1364 - recall: 1.0197 - f1_metric: 0.2351 - val_loss: 0.4110 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3887 - acc: 0.8661 - precision: 0.1338 - recall: 1.0020 - f1_metric: 0.2311 - val_loss: 0.4105 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4028 - acc: 0.8595 - precision: 0.1403 - recall: 1.0002 - f1_metric: 0.2409 - val_loss: 0.4116 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3970 - acc: 0.8601 - precision: 0.1399 - recall: 1.0004 - f1_metric: 0.2397 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3793 - acc: 0.8711 - precision: 0.1289 - recall: 0.9802 - f1_metric: 0.2218 - val_loss: 0.4082 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3907 - acc: 0.8629 - precision: 0.1371 - recall: 0.9942 - f1_metric: 0.2370 - val_loss: 0.4102 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3851 - acc: 0.8654 - precision: 0.1348 - recall: 0.9680 - f1_metric: 0.2314 - val_loss: 0.4109 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3897 - acc: 0.8638 - precision: 0.1361 - recall: 0.9770 - f1_metric: 0.2341 - val_loss: 0.4044 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3858 - acc: 0.8654 - precision: 0.1346 - recall: 0.9842 - f1_metric: 0.2323 - val_loss: 0.4041 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8595 - precision: 0.1405 - recall: 0.9868 - f1_metric: 0.2419 - val_loss: 0.4019 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3848 - acc: 0.8636 - precision: 0.1364 - recall: 0.9999 - f1_metric: 0.2357 - val_loss: 0.4001 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3791 - acc: 0.8650 - precision: 0.1350 - recall: 0.9798 - f1_metric: 0.2331 - val_loss: 0.3960 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3867 - acc: 0.8588 - precision: 0.1412 - recall: 0.9952 - f1_metric: 0.2407 - val_loss: 0.3945 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3747 - acc: 0.8659 - precision: 0.1342 - recall: 0.9992 - f1_metric: 0.2319 - val_loss: 0.3969 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 28 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 4s 11ms/step - loss: 0.4734 - acc: 0.8624 - precision: 0.1326 - recall: 0.9939 - f1_metric: 0.2294 - val_loss: 0.4169 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3953 - acc: 0.8685 - precision: 0.1315 - recall: 0.9898 - f1_metric: 0.2270 - val_loss: 0.4139 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4006 - acc: 0.8648 - precision: 0.1352 - recall: 0.9655 - f1_metric: 0.2321 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4047 - acc: 0.8613 - precision: 0.1387 - recall: 0.9973 - f1_metric: 0.2383 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4060 - acc: 0.8606 - precision: 0.1394 - recall: 0.9840 - f1_metric: 0.2391 - val_loss: 0.4144 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3887 - acc: 0.8693 - precision: 0.1307 - recall: 0.9700 - f1_metric: 0.2272 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3875 - acc: 0.8699 - precision: 0.1301 - recall: 0.9501 - f1_metric: 0.2243 - val_loss: 0.4134 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3916 - acc: 0.8675 - precision: 0.1324 - recall: 0.9888 - f1_metric: 0.2290 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4076 - acc: 0.8576 - precision: 0.1424 - recall: 0.9712 - f1_metric: 0.2428 - val_loss: 0.4132 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3894 - acc: 0.8674 - precision: 0.1327 - recall: 0.9847 - f1_metric: 0.2292 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3998 - acc: 0.8612 - precision: 0.1388 - recall: 0.9829 - f1_metric: 0.2386 - val_loss: 0.4144 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3818 - acc: 0.8705 - precision: 0.1295 - recall: 0.9715 - f1_metric: 0.2249 - val_loss: 0.4122 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3987 - acc: 0.8617 - precision: 0.1383 - recall: 0.9913 - f1_metric: 0.2383 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3937 - acc: 0.8638 - precision: 0.1362 - recall: 0.9893 - f1_metric: 0.2351 - val_loss: 0.4111 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4034 - acc: 0.8585 - precision: 0.1415 - recall: 0.9897 - f1_metric: 0.2426 - val_loss: 0.4121 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4001 - acc: 0.8606 - precision: 0.1394 - recall: 0.9802 - f1_metric: 0.2398 - val_loss: 0.4133 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4022 - acc: 0.8604 - precision: 0.1396 - recall: 0.9973 - f1_metric: 0.2385 - val_loss: 0.4105 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3848 - acc: 0.8681 - precision: 0.1319 - recall: 0.9979 - f1_metric: 0.2286 - val_loss: 0.4094 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3830 - acc: 0.8706 - precision: 0.1293 - recall: 1.0000 - f1_metric: 0.2244 - val_loss: 0.4105 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3945 - acc: 0.8630 - precision: 0.1370 - recall: 0.9957 - f1_metric: 0.2363 - val_loss: 0.4082 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3851 - acc: 0.8682 - precision: 0.1318 - recall: 0.9931 - f1_metric: 0.2293 - val_loss: 0.4082 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3789 - acc: 0.8696 - precision: 0.1304 - recall: 0.9896 - f1_metric: 0.2259 - val_loss: 0.4088 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3784 - acc: 0.8710 - precision: 0.1290 - recall: 0.9744 - f1_metric: 0.2241 - val_loss: 0.4049 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3858 - acc: 0.8636 - precision: 0.1364 - recall: 0.9956 - f1_metric: 0.2347 - val_loss: 0.4031 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3782 - acc: 0.8685 - precision: 0.1315 - recall: 1.0000 - f1_metric: 0.2271 - val_loss: 0.4021 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4009 - acc: 0.8555 - precision: 0.1445 - recall: 1.0000 - f1_metric: 0.2481 - val_loss: 0.4002 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3723 - acc: 0.8693 - precision: 0.1307 - recall: 0.9931 - f1_metric: 0.2268 - val_loss: 0.3987 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3730 - acc: 0.8704 - precision: 0.1296 - recall: 0.9572 - f1_metric: 0.2227 - val_loss: 0.4003 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3853 - acc: 0.8625 - precision: 0.1375 - recall: 0.9945 - f1_metric: 0.2373 - val_loss: 0.3986 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3804 - acc: 0.8637 - precision: 0.1363 - recall: 1.0000 - f1_metric: 0.2349 - val_loss: 0.3913 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 29 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 5s 18ms/step - loss: 0.5050 - acc: 0.7783 - precision: 0.1340 - recall: 1.0534 - f1_metric: 0.2329 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3978 - acc: 0.8659 - precision: 0.1341 - recall: 1.0000 - f1_metric: 0.2319 - val_loss: 0.4158 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3977 - acc: 0.8643 - precision: 0.1357 - recall: 0.9955 - f1_metric: 0.2342 - val_loss: 0.4171 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3990 - acc: 0.8645 - precision: 0.1354 - recall: 0.9678 - f1_metric: 0.2319 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 1s 4ms/step - loss: 0.4019 - acc: 0.8604 - precision: 0.1396 - recall: 0.9892 - f1_metric: 0.2404 - val_loss: 0.4140 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3919 - acc: 0.8666 - precision: 0.1334 - recall: 1.0000 - f1_metric: 0.2309 - val_loss: 0.4163 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3975 - acc: 0.8629 - precision: 0.1371 - recall: 0.9918 - f1_metric: 0.2363 - val_loss: 0.4158 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4039 - acc: 0.8604 - precision: 0.1396 - recall: 0.9978 - f1_metric: 0.2405 - val_loss: 0.4138 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4099 - acc: 0.8569 - precision: 0.1431 - recall: 0.9884 - f1_metric: 0.2445 - val_loss: 0.4135 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4005 - acc: 0.8641 - precision: 0.1359 - recall: 0.9887 - f1_metric: 0.2338 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3945 - acc: 0.8638 - precision: 0.1362 - recall: 0.9905 - f1_metric: 0.2357 - val_loss: 0.4153 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4067 - acc: 0.8579 - precision: 0.1421 - recall: 0.9954 - f1_metric: 0.2437 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3972 - acc: 0.8619 - precision: 0.1381 - recall: 0.9892 - f1_metric: 0.2385 - val_loss: 0.4115 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3991 - acc: 0.8618 - precision: 0.1382 - recall: 0.9980 - f1_metric: 0.2385 - val_loss: 0.4110 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3890 - acc: 0.8675 - precision: 0.1325 - recall: 0.9737 - f1_metric: 0.2288 - val_loss: 0.4111 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4009 - acc: 0.8604 - precision: 0.1396 - recall: 0.9984 - f1_metric: 0.2404 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3835 - acc: 0.8693 - precision: 0.1307 - recall: 0.9840 - f1_metric: 0.2272 - val_loss: 0.4097 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3814 - acc: 0.8698 - precision: 0.1302 - recall: 0.9791 - f1_metric: 0.2254 - val_loss: 0.4088 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3912 - acc: 0.8658 - precision: 0.1342 - recall: 0.9931 - f1_metric: 0.2315 - val_loss: 0.4084 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3954 - acc: 0.8622 - precision: 0.1378 - recall: 0.9921 - f1_metric: 0.2375 - val_loss: 0.4091 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3952 - acc: 0.8602 - precision: 0.1398 - recall: 0.9987 - f1_metric: 0.2411 - val_loss: 0.4104 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3879 - acc: 0.8646 - precision: 0.1354 - recall: 0.9955 - f1_metric: 0.2337 - val_loss: 0.4053 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3849 - acc: 0.8644 - precision: 0.1356 - recall: 0.9681 - f1_metric: 0.2340 - val_loss: 0.4036 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3743 - acc: 0.8701 - precision: 0.1299 - recall: 0.9694 - f1_metric: 0.2246 - val_loss: 0.4015 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3832 - acc: 0.8671 - precision: 0.1329 - recall: 0.9686 - f1_metric: 0.2291 - val_loss: 0.4024 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3765 - acc: 0.8682 - precision: 0.1319 - recall: 0.9984 - f1_metric: 0.2277 - val_loss: 0.4034 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3988 - acc: 0.8541 - precision: 0.1459 - recall: 0.9856 - f1_metric: 0.2487 - val_loss: 0.3957 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3942 - acc: 0.8572 - precision: 0.1428 - recall: 0.9968 - f1_metric: 0.2458 - val_loss: 0.3985 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3707 - acc: 0.8687 - precision: 0.1313 - recall: 1.0000 - f1_metric: 0.2261 - val_loss: 0.3906 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3753 - acc: 0.8618 - precision: 0.1382 - recall: 0.9884 - f1_metric: 0.2381 - val_loss: 0.3877 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Model 30 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 3s 10ms/step - loss: 0.4781 - acc: 0.8489 - precision: 0.0960 - recall: 0.3928 - f1_metric: 0.1341 - val_loss: 0.4142 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4003 - acc: 0.8632 - precision: 0.1337 - recall: 0.8515 - f1_metric: 0.2256 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3953 - acc: 0.8658 - precision: 0.1289 - recall: 0.8259 - f1_metric: 0.2189 - val_loss: 0.4137 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3984 - acc: 0.8640 - precision: 0.1360 - recall: 0.9296 - f1_metric: 0.2326 - val_loss: 0.4136 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3990 - acc: 0.8653 - precision: 0.1363 - recall: 0.9526 - f1_metric: 0.2340 - val_loss: 0.4141 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4045 - acc: 0.8601 - precision: 0.1402 - recall: 0.9388 - f1_metric: 0.2399 - val_loss: 0.4131 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4008 - acc: 0.8620 - precision: 0.1364 - recall: 0.9496 - f1_metric: 0.2336 - val_loss: 0.4130 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3984 - acc: 0.8642 - precision: 0.1359 - recall: 0.9670 - f1_metric: 0.2336 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4095 - acc: 0.8565 - precision: 0.1424 - recall: 0.9502 - f1_metric: 0.2417 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4011 - acc: 0.8618 - precision: 0.1407 - recall: 0.9575 - f1_metric: 0.2409 - val_loss: 0.4124 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3953 - acc: 0.8645 - precision: 0.1366 - recall: 0.9240 - f1_metric: 0.2330 - val_loss: 0.4123 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3949 - acc: 0.8648 - precision: 0.1361 - recall: 0.9470 - f1_metric: 0.2323 - val_loss: 0.4125 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3981 - acc: 0.8628 - precision: 0.1377 - recall: 0.9925 - f1_metric: 0.2380 - val_loss: 0.4127 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4033 - acc: 0.8597 - precision: 0.1402 - recall: 0.9941 - f1_metric: 0.2421 - val_loss: 0.4126 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3858 - acc: 0.8683 - precision: 0.1319 - recall: 0.9954 - f1_metric: 0.2291 - val_loss: 0.4110 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4037 - acc: 0.8580 - precision: 0.1419 - recall: 0.9844 - f1_metric: 0.2434 - val_loss: 0.4160 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4015 - acc: 0.8604 - precision: 0.1397 - recall: 0.9554 - f1_metric: 0.2394 - val_loss: 0.4109 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8576 - precision: 0.1425 - recall: 0.9986 - f1_metric: 0.2446 - val_loss: 0.4110 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3763 - acc: 0.8719 - precision: 0.1281 - recall: 0.9783 - f1_metric: 0.2223 - val_loss: 0.4108 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3969 - acc: 0.8615 - precision: 0.1386 - recall: 1.0000 - f1_metric: 0.2384 - val_loss: 0.4087 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3790 - acc: 0.8705 - precision: 0.1295 - recall: 0.9647 - f1_metric: 0.2242 - val_loss: 0.4102 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3922 - acc: 0.8648 - precision: 0.1352 - recall: 0.9839 - f1_metric: 0.2327 - val_loss: 0.4077 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3915 - acc: 0.8633 - precision: 0.1367 - recall: 0.9951 - f1_metric: 0.2338 - val_loss: 0.4065 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3964 - acc: 0.8596 - precision: 0.1404 - recall: 1.0000 - f1_metric: 0.2428 - val_loss: 0.4082 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3935 - acc: 0.8605 - precision: 0.1395 - recall: 0.9865 - f1_metric: 0.2387 - val_loss: 0.4037 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3716 - acc: 0.8732 - precision: 0.1268 - recall: 0.9992 - f1_metric: 0.2197 - val_loss: 0.4099 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3838 - acc: 0.8659 - precision: 0.1341 - recall: 0.9813 - f1_metric: 0.2317 - val_loss: 0.4013 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3909 - acc: 0.8583 - precision: 0.1417 - recall: 0.9925 - f1_metric: 0.2432 - val_loss: 0.3973 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.3784 - acc: 0.8652 - precision: 0.1348 - recall: 0.9928 - f1_metric: 0.2322 - val_loss: 0.3957 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.3860 - acc: 0.8619 - precision: 0.1381 - recall: 0.9807 - f1_metric: 0.2373 - val_loss: 0.4011 - val_acc: 0.8537 - val_precision: 0.1436 - val_recall: 0.9808 - val_f1_metric: 0.2457\n"
     ]
    }
   ],
   "source": [
    "#Run the model\n",
    "\n",
    "print(\"Model 1 Fitting\")\n",
    "history1 = model1.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), class_weight = weights, callbacks=[tensorboard_callback])\n",
    "print(\"Model 2 Fitting\")\n",
    "history2 = model2.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 3 Fitting\")\n",
    "history3 = model3.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 4 Fitting\")\n",
    "history4 = model4.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 5 Fitting\")\n",
    "history5 = model5.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 6 Fitting\")\n",
    "history6 = model6.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 7 Fitting\")\n",
    "history7 = model7.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 8 Fitting\")\n",
    "history8 = model8.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 9 Fitting\")\n",
    "history9 = model9.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 10 Fitting\")\n",
    "history10 = model10.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 11 Fitting\")\n",
    "history11 = model11.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 12 Fitting\")\n",
    "history12 = model12.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 13 Fitting\")\n",
    "history13 = model13.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 14 Fitting\")\n",
    "history14 = model14.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 15 Fitting\")\n",
    "history15 = model15.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 16 Fitting\")\n",
    "history16 = model16.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 17 Fitting\")\n",
    "history17 = model17.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 18 Fitting\")\n",
    "history18 = model18.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 19 Fitting\")\n",
    "history19 = model19.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 20 Fitting\")\n",
    "history20 = model20.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 21 Fitting\")\n",
    "history21 = model21.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 22 Fitting\")\n",
    "history22 = model22.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 23 Fitting\")\n",
    "history23 = model23.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 24 Fitting\")\n",
    "history24 = model24.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 25 Fitting\")\n",
    "history25 = model25.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 26 Fitting\")\n",
    "history26 = model26.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 27 Fitting\")\n",
    "history27 = model27.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 28 Fitting\")\n",
    "history28 = model28.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 29 Fitting\")\n",
    "history29 = model29.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 30 Fitting\")\n",
    "history30 = model30.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "GfHp9ww0FWvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6599 - acc: 0.5487 - precision: 0.0948 - recall: 0.2292 - f1_metric: 0.1267      \n",
      "[0.6598657965660095, 0.5487028956413269, 0.09481371939182281, 0.22924105823040009, 0.1266539990901947]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3879 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.6598657965660095, 0.5487028956413269, 0.09481371939182281, 0.22924105823040009, 0.1266539990901947]\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3806 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.6598657965660095, 0.5487028956413269, 0.09481371939182281, 0.22924105823040009, 0.1266539990901947]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3791 - acc: 0.8551 - precision: 0.1450 - recall: 1.0000 - f1_metric: 0.2477\n",
      "[0.3790889084339142, 0.8551150560379028, 0.14500319957733154, 1.0, 0.24772460758686066]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3813 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3812548816204071, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3952 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3951749801635742, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3782 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3781825602054596, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3925 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.39253535866737366, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3829 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.38294661045074463, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 713us/step - loss: 0.3870 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.38701120018959045, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 712us/step - loss: 0.3855 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.38547244668006897, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4686 - acc: 0.7812 - precision: 0.2188 - recall: 1.0000 - f1_metric: 0.35 - 0s 697us/step - loss: 0.3766 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3766199052333832, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 681us/step - loss: 0.3796 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.37959030270576477, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 697us/step - loss: 0.3899 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.38988107442855835, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 665us/step - loss: 0.3854 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3853989839553833, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 665us/step - loss: 0.3806 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3805796205997467, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 697us/step - loss: 0.3845 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.38450586795806885, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 792us/step - loss: 0.3939 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.39393532276153564, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 871us/step - loss: 0.3872 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3871611952781677, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 918us/step - loss: 0.3835 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3835403621196747, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 617us/step - loss: 0.3992 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3992469906806946, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 665us/step - loss: 0.3695 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3695491552352905, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 681us/step - loss: 0.3779 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.37794846296310425, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 744us/step - loss: 0.3736 - acc: 0.8551 - precision: 0.1448 - recall: 1.0000 - f1_metric: 0.2474\n",
      "[0.373587429523468, 0.8551150560379028, 0.14478936791419983, 1.0, 0.2474367767572403]\n",
      "64/64 [==============================] - 0s 793us/step - loss: 0.3708 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3707828223705292, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 697us/step - loss: 0.3828 - acc: 0.8546 - precision: 0.1451 - recall: 1.0000 - f1_metric: 0.2478\n",
      "[0.382753849029541, 0.8546255230903625, 0.14508195221424103, 1.0, 0.24783647060394287]\n",
      "64/64 [==============================] - 0s 665us/step - loss: 0.3874 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3873736262321472, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 681us/step - loss: 0.3820 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.38202840089797974, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 681us/step - loss: 0.3792 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.3791505694389343, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n",
      "64/64 [==============================] - 0s 633us/step - loss: 0.3945 - acc: 0.8551 - precision: 0.1449 - recall: 1.0000 - f1_metric: 0.2476\n",
      "[0.39453840255737305, 0.8551150560379028, 0.14489294588565826, 1.0, 0.2475769966840744]\n"
     ]
    }
   ],
   "source": [
    "#Run model predictions\n",
    "\n",
    "# Model 1 \n",
    "prediction_features_1 = model1.predict(features_test)\n",
    "performance1 = model1.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 2 \n",
    "prediction_features_2 = model2.predict(features_test)\n",
    "performance2 = model2.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 3 \n",
    "prediction_features_3 = model3.predict(features_test)\n",
    "performance3 = model3.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 4 \n",
    "prediction_features_4 = model4.predict(features_test)\n",
    "performance4 = model4.evaluate(features_test, labels_test)\n",
    "print(performance4)\n",
    "# Model 5 \n",
    "prediction_features_5 = model5.predict(features_test)\n",
    "performance5 = model5.evaluate(features_test, labels_test)\n",
    "print(performance5)\n",
    "\n",
    "# Model 6 \n",
    "prediction_features_6 = model6.predict(features_test)\n",
    "performance6 = model6.evaluate(features_test, labels_test)\n",
    "print(performance6)\n",
    "# Model 7\n",
    "prediction_features_7 = model7.predict(features_test)\n",
    "performance7 = model7.evaluate(features_test, labels_test)\n",
    "print(performance7)\n",
    "# Model 8 \n",
    "prediction_features_8 = model8.predict(features_test)\n",
    "performance8 = model8.evaluate(features_test, labels_test)\n",
    "print(performance8)\n",
    "# Model 9\n",
    "prediction_features_9 = model9.predict(features_test)\n",
    "performance9 = model9.evaluate(features_test, labels_test)\n",
    "print(performance9)\n",
    "# Model 10\n",
    "prediction_features_10 = model10.predict(features_test)\n",
    "performance10 = model10.evaluate(features_test, labels_test)\n",
    "print(performance10)\n",
    "\n",
    "# Model 11 \n",
    "prediction_features_11 = model11.predict(features_test)\n",
    "performance11 = model11.evaluate(features_test, labels_test)\n",
    "print(performance11)\n",
    "# Model 12 \n",
    "prediction_features_12 = model12.predict(features_test)\n",
    "performance12 = model12.evaluate(features_test, labels_test)\n",
    "print(performance12)\n",
    "# Model 13 \n",
    "prediction_features_13 = model13.predict(features_test)\n",
    "performance13 = model13.evaluate(features_test, labels_test)\n",
    "print(performance13)\n",
    "# Model 14 \n",
    "prediction_features_14 = model14.predict(features_test)\n",
    "performance14 = model14.evaluate(features_test, labels_test)\n",
    "print(performance14)\n",
    "# Model 15 \n",
    "prediction_features_15 = model15.predict(features_test)\n",
    "performance15 = model15.evaluate(features_test, labels_test)\n",
    "print(performance15)\n",
    "\n",
    "# Model 16\n",
    "prediction_features_16 = model16.predict(features_test)\n",
    "performance16 = model16.evaluate(features_test, labels_test)\n",
    "print(performance16)\n",
    "# Model 17\n",
    "prediction_features_17 = model17.predict(features_test)\n",
    "performance17 = model17.evaluate(features_test, labels_test)\n",
    "print(performance17)\n",
    "# Model 18 \n",
    "prediction_features_18 = model18.predict(features_test)\n",
    "performance18 = model18.evaluate(features_test, labels_test)\n",
    "print(performance18)\n",
    "# Model 19\n",
    "prediction_features_19 = model19.predict(features_test)\n",
    "performance19 = model19.evaluate(features_test, labels_test)\n",
    "print(performance19)\n",
    "# Model 20 \n",
    "prediction_features_20 = model20.predict(features_test)\n",
    "performance20 = model20.evaluate(features_test, labels_test)\n",
    "print(performance20)\n",
    "\n",
    "# Model 21\n",
    "prediction_features_21 = model21.predict(features_test)\n",
    "performance21 = model21.evaluate(features_test, labels_test)\n",
    "print(performance21)\n",
    "# Model 22\n",
    "prediction_features_22 = model22.predict(features_test)\n",
    "performance22 = model22.evaluate(features_test, labels_test)\n",
    "print(performance22)\n",
    "# Model 23\n",
    "prediction_features_23 = model23.predict(features_test)\n",
    "performance23 = model23.evaluate(features_test, labels_test)\n",
    "print(performance23)\n",
    "# Model 24 \n",
    "prediction_features_24 = model24.predict(features_test)\n",
    "performance24 = model24.evaluate(features_test, labels_test)\n",
    "print(performance24)\n",
    "# Model 25\n",
    "prediction_features_25 = model25.predict(features_test)\n",
    "performance25 = model25.evaluate(features_test, labels_test)\n",
    "print(performance25)\n",
    "\n",
    "# Model 26 \n",
    "prediction_features_26 = model26.predict(features_test)\n",
    "performance26 = model26.evaluate(features_test, labels_test)\n",
    "print(performance26)\n",
    "# Model 27 \n",
    "prediction_features_27 = model27.predict(features_test)\n",
    "performance27 = model27.evaluate(features_test, labels_test)\n",
    "print(performance27)\n",
    "# Model 28 \n",
    "prediction_features_28 = model28.predict(features_test)\n",
    "performance28 = model28.evaluate(features_test, labels_test)\n",
    "print(performance28)\n",
    "# Model 29\n",
    "prediction_features_29 = model29.predict(features_test)\n",
    "performance29 = model29.evaluate(features_test, labels_test)\n",
    "print(performance29)\n",
    "# Model 30 \n",
    "prediction_features_30 = model30.predict(features_test)\n",
    "performance30 = model30.evaluate(features_test, labels_test)\n",
    "print(performance30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "-kn2xq4Ts7s-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Average:  0.3929402609666189\n",
      "Accuraccy Average:  0.8448849995930989\n",
      "Precision Average:  0.14323016107082367\n",
      "Recall Average:  0.9743080352743466\n",
      "F1 Average:  0.2435551255941391\n"
     ]
    }
   ],
   "source": [
    "# Averages\n",
    "\n",
    "# Loss\n",
    "loss_avg = (performance1[0] + performance2[0] + performance3[0] + performance4[0] + performance5[0] + performance6[0] + performance7[0] + performance8[0] + performance9[0] + performance10[0]\n",
    "            + performance11[0] + performance12[0] + performance13[0] + performance14[0] + performance15[0] + performance16[0] + performance17[0] + performance18[0] + performance19[0] + performance20[0]\n",
    "            + performance21[0] + performance22[0] + performance23[0] + performance24[0] + performance25[0] + performance26[0] + performance27[0] + performance28[0] + performance29[0] + performance30[0])/30\n",
    "print(\"Loss Average: \", loss_avg)\n",
    "\n",
    "# Accuracy\n",
    "acc_avg =(performance1[1] + performance2[1] + performance3[1] + performance4[1] + performance5[1] + performance6[1] + performance7[1] + performance8[1] + performance9[1] + performance10[1]\n",
    "            + performance11[1] + performance12[1] + performance13[1] + performance14[1] + performance15[1] + performance16[1] + performance17[1] + performance18[1] + performance19[1] + performance20[1]\n",
    "            + performance21[1] + performance22[1] + performance23[1] + performance24[1] + performance25[1] + performance26[1] + performance27[1] + performance28[1] + performance29[1] + performance30[1])/30\n",
    "print(\"Accuraccy Average: \", acc_avg)\n",
    "\n",
    "# Precision\n",
    "precision_avg = (performance1[2] + performance2[2] + performance3[2] + performance4[2] + performance5[2] + performance6[2] + performance7[2] + performance8[2] + performance9[2] + performance10[2]\n",
    "            + performance11[2] + performance12[2] + performance13[2] + performance14[2] + performance15[2] + performance16[2] + performance17[2] + performance18[2] + performance19[2] + performance20[2]\n",
    "            + performance21[2] + performance22[2] + performance23[2] + performance24[2] + performance25[2] + performance26[2] + performance27[2] + performance28[2] + performance29[2] + performance30[2])/30\n",
    "print(\"Precision Average: \", precision_avg)\n",
    "\n",
    "# Recall\n",
    "recall_avg = (performance1[3] + performance2[3] + performance3[3] + performance4[3] + performance5[3] + performance6[3] + performance7[3] + performance8[3] + performance9[3] + performance10[3]\n",
    "            + performance11[3] + performance12[3] + performance13[3] + performance14[3] + performance15[3] + performance16[3] + performance17[3] + performance18[3] + performance19[3] + performance20[3]\n",
    "            + performance21[3] + performance22[3] + performance23[3] + performance24[3] + performance25[3] + performance26[3] + performance27[3] + performance28[3] + performance29[3] + performance30[3])/30\n",
    "print(\"Recall Average: \", recall_avg)\n",
    "\n",
    "# f1_metric\n",
    "f1_avg = (performance1[4] + performance2[4] + performance3[4] + performance4[4] + performance5[4] + performance6[4] + performance7[4] + performance8[4] + performance9[4] + performance10[4]\n",
    "            + performance11[4] + performance12[4] + performance13[4] + performance14[4] + performance15[4] + performance16[4] + performance17[4] + performance18[4] + performance19[4] + performance20[4]\n",
    "            + performance21[4] + performance22[4] + performance23[4] + performance24[4] + performance25[4] + performance26[4] + performance27[4] + performance28[4] + performance29[4] + performance30[4])/30\n",
    "print(\"F1 Average: \", f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "XuwAUg5_KOz4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss SE: 0.050905684751565136\n",
      "Accuraccy SE:  0.0559399400944927\n",
      "Precision SE:  0.009144513964332853\n",
      "Recall SE:  0.14072068626867357\n",
      "F1_Score_SE:  0.022079180950958045\n"
     ]
    }
   ],
   "source": [
    "#Take the standard deviation of the model samples\n",
    "\n",
    "#Loss SE\n",
    "Loss_SE = statistics.stdev([performance1[0], performance2[0], performance3[0], performance4[0], performance5[0],\n",
    "                  performance6[0], performance7[0], performance8[0], performance9[0], performance10[0],\n",
    "                  performance11[0], performance12[0], performance13[0], performance14[0], performance15[0],\n",
    "                  performance16[0], performance17[0], performance18[0], performance19[0], performance20[0], \n",
    "                  performance21[0], performance22[0], performance23[0], performance24[0], performance25[0],\n",
    "                  performance26[0], performance27[0], performance28[0], performance29[0], performance30[0]])\n",
    "print(\"Loss SE:\", Loss_SE)\n",
    "\n",
    "#Accuracy SE\n",
    "Acc_SE = statistics.stdev([performance1[1], performance2[1], performance3[1], performance4[1], performance5[1],\n",
    "                  performance6[1], performance7[1], performance8[1], performance9[1], performance10[1],\n",
    "                  performance11[1], performance12[1], performance13[1], performance14[1], performance15[1],\n",
    "                  performance16[1], performance17[1], performance18[1], performance19[1], performance20[1], \n",
    "                  performance21[1], performance22[1], performance23[1], performance24[1], performance25[1],\n",
    "                  performance26[1], performance27[1], performance28[1], performance29[1], performance30[1]])\n",
    "print(\"Accuraccy SE: \", Acc_SE)\n",
    "\n",
    "#Precision SE\n",
    "precision_SE = statistics.stdev([performance1[2], performance2[2], performance3[2], performance4[2], performance5[2],\n",
    "                  performance6[2], performance7[2], performance8[2], performance9[2], performance10[2],\n",
    "                  performance11[2], performance12[2], performance13[2], performance14[2], performance15[2],\n",
    "                  performance16[2], performance17[2], performance18[2], performance19[2], performance20[2], \n",
    "                  performance21[2], performance22[2], performance23[2], performance24[2], performance25[2],\n",
    "                  performance26[2], performance27[2], performance28[2], performance29[2], performance30[2]])\n",
    "print(\"Precision SE: \", precision_SE)\n",
    "\n",
    "#Recall \n",
    "Recall_SE = statistics.stdev([performance1[3], performance2[3], performance3[3], performance4[3], performance5[3],\n",
    "                  performance6[3], performance7[3], performance8[3], performance9[3], performance10[3],\n",
    "                  performance11[3], performance12[3], performance13[3], performance14[3], performance15[3],\n",
    "                  performance16[3], performance17[3], performance18[3], performance19[3], performance20[3], \n",
    "                  performance21[3], performance22[3], performance23[3], performance24[3], performance25[3],\n",
    "                  performance26[3], performance27[3], performance28[3], performance29[3], performance30[3]])\n",
    "print(\"Recall SE: \", Recall_SE)\n",
    "\n",
    "#F1 Score\n",
    "F1_Score_SE = statistics.stdev([performance1[4], performance2[4], performance3[4], performance4[4], performance5[4],\n",
    "                  performance6[4], performance7[4], performance8[4], performance9[4], performance10[4],\n",
    "                  performance11[4], performance12[4], performance13[4], performance14[4], performance15[4],\n",
    "                  performance16[4], performance17[4], performance18[4], performance19[4], performance20[4], \n",
    "                  performance21[4], performance22[4], performance23[4], performance24[4], performance25[4],\n",
    "                  performance26[4], performance27[4], performance28[4], performance29[4], performance30[4]])\n",
    "print(\"F1_Score_SE: \", F1_Score_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "aMpLKJ4OKt-X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1032), started 3 days, 3:29:01 ago. (Use '!kill 1032' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1ccd6995b9187e96\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1ccd6995b9187e96\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tensorflow Graphics\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GA_ParliHeg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g9kxq01msMg9"
   },
   "outputs": [],
   "source": [
    "### BEWARE:Tensorflow is stochastic - this means the model will not be replicated exactly. \n",
    "### Use GA_Load_Model for reproduction\n",
    "\n",
    "#!pip install mlxtend\n",
    "\n",
    "#!pip install h5py pyyaml\n",
    "\n",
    "#!pip install tensorboard\n",
    "\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tLftnb5sBe7B"
   },
   "outputs": [],
   "source": [
    "#Load packages\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iBsKz40OAGDU"
   },
   "outputs": [],
   "source": [
    "### Packages necessary for model construction \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks\n",
    "import datetime \n",
    "import statistics\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FrEcBaVWVVkb"
   },
   "outputs": [],
   "source": [
    "#Read the Data\n",
    "\n",
    "UN_Data = pd.read_csv('GA_Query_CleanLIWC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 4369,
     "status": "ok",
     "timestamp": 1611639237251,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "FNUqnWTFS4y2",
    "outputId": "cf54ea17-a224-4830-8f6b-554ee9bafaf0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Policy Passed</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20075.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>822.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17021.0</td>\n",
       "      <td>98.45</td>\n",
       "      <td>...</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9289.0</td>\n",
       "      <td>98.94</td>\n",
       "      <td>...</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4059.0</td>\n",
       "      <td>98.95</td>\n",
       "      <td>...</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8210.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10210</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>98.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10211</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22700.0</td>\n",
       "      <td>98.26</td>\n",
       "      <td>...</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10212 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  Class M  Class S  Class I  Class P  Class B  Policy Passed  \\\n",
       "0      2012        1        0        0        0        0              0   \n",
       "1      2012        0        0        3        0        0              0   \n",
       "2      2003        0        0        0        0        0              0   \n",
       "3      1995        0        0        0        0        0              1   \n",
       "4      2007        0        0        0        0        0              0   \n",
       "...     ...      ...      ...      ...      ...      ...            ...   \n",
       "10207  2004        0        0        0        0        0              0   \n",
       "10208  1994        0        0        0        0        0              0   \n",
       "10209  2013        0        0        0        0        0              0   \n",
       "10210  2009        0        0        0        0        0              0   \n",
       "10211  2016        0        0        0        0        0              0   \n",
       "\n",
       "       Conflict Indicator       WC  Analytic  ...  Comma  Colon  SemiC  QMark  \\\n",
       "0                       1  20075.0     99.00  ...   4.34   0.03   0.04   0.00   \n",
       "1                       0    822.0     99.00  ...   3.04   1.70   0.00   0.00   \n",
       "2                       0    314.0     99.00  ...   3.50   0.96   0.00   0.00   \n",
       "3                       1  17021.0     98.45  ...   4.91   0.25   0.17   0.02   \n",
       "4                       0   9289.0     98.94  ...   3.80   0.16   0.15   0.00   \n",
       "...                   ...      ...       ...  ...    ...    ...    ...    ...   \n",
       "10207                   0   4059.0     98.95  ...   3.72   0.15   0.12   0.00   \n",
       "10208                   0   8210.0     99.00  ...   3.58   0.12   0.22   0.00   \n",
       "10209                   0    583.0     99.00  ...   3.09   0.86   0.00   0.69   \n",
       "10210                   0   1562.0     98.88  ...   2.82   0.19   0.45   0.00   \n",
       "10211                   0  22700.0     98.26  ...   3.69   0.04   0.03   0.04   \n",
       "\n",
       "       Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "0         0.0  1.23   0.07     0.64     0.82    0.60  \n",
       "1         0.0  0.85   0.49     0.12     4.38    1.46  \n",
       "2         0.0  2.23   0.00     0.64     3.18    2.87  \n",
       "3         0.0  1.33   0.22     0.16     0.64    2.18  \n",
       "4         0.0  0.93   0.28     0.75     1.42    1.52  \n",
       "...       ...   ...    ...      ...      ...     ...  \n",
       "10207     0.0  1.18   0.00     0.76     1.72    1.23  \n",
       "10208     0.0  1.06   0.02     0.29     1.05    1.75  \n",
       "10209     0.0  1.89   0.34     0.00     4.80    4.63  \n",
       "10210     0.0  1.34   0.00     0.77     1.66    2.18  \n",
       "10211     0.0  1.44   0.10     0.48     1.01    1.58  \n",
       "\n",
       "[10212 rows x 101 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect and Clean the Data\n",
    "\n",
    "UN_Data.head(5)\n",
    "\n",
    "UN_Data.drop(['Unnamed: 0'], axis = 1, inplace= True)\n",
    "\n",
    "UN_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.581151832460733, 1: 3.5806451612903225}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "#Balance Policy Passage\n",
    "\n",
    "# Count samples per class\n",
    "classes_zero = UN_Data[UN_Data['Policy Passed'] == 0]\n",
    "classes_one = UN_Data[UN_Data['Policy Passed'] == 1]\n",
    "\n",
    "# Convert parts into NumPy arrays for weight computation\n",
    "zero_numpy = classes_zero['Policy Passed'].to_numpy()\n",
    "one_numpy = classes_one['Policy Passed'].to_numpy()\n",
    "all_together = np.concatenate((zero_numpy, one_numpy))\n",
    "unique_classes = np.unique(all_together)\n",
    "\n",
    "# Compute weights\n",
    "weights = sklearn.utils.class_weight.compute_class_weight('balanced', unique_classes, all_together)\n",
    "weights = dict(enumerate(weights))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 4368,
     "status": "ok",
     "timestamp": 1611639237252,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "tkn1c4RqeEdO",
    "outputId": "d56ecbce-990a-427d-e279-772f8546aeac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Policy Passed</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.00000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2005.852135</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>0.009205</td>\n",
       "      <td>0.168625</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.13964</td>\n",
       "      <td>0.457697</td>\n",
       "      <td>9442.204220</td>\n",
       "      <td>98.190282</td>\n",
       "      <td>...</td>\n",
       "      <td>5.193233</td>\n",
       "      <td>0.303105</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>1.154469</td>\n",
       "      <td>0.260464</td>\n",
       "      <td>0.397916</td>\n",
       "      <td>1.744508</td>\n",
       "      <td>1.768528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.542111</td>\n",
       "      <td>0.229542</td>\n",
       "      <td>0.099521</td>\n",
       "      <td>0.861341</td>\n",
       "      <td>0.307019</td>\n",
       "      <td>0.156540</td>\n",
       "      <td>0.34663</td>\n",
       "      <td>0.498232</td>\n",
       "      <td>7786.325195</td>\n",
       "      <td>1.085791</td>\n",
       "      <td>...</td>\n",
       "      <td>3.290078</td>\n",
       "      <td>0.466117</td>\n",
       "      <td>0.190811</td>\n",
       "      <td>0.105541</td>\n",
       "      <td>0.713727</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>0.439789</td>\n",
       "      <td>1.273977</td>\n",
       "      <td>1.466890</td>\n",
       "      <td>4.337472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1993.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3838.500000</td>\n",
       "      <td>97.880000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2006.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7439.500000</td>\n",
       "      <td>98.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2012.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12438.750000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>2.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>74776.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>47.760000</td>\n",
       "      <td>22.710000</td>\n",
       "      <td>11.150000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>49.920000</td>\n",
       "      <td>22.880000</td>\n",
       "      <td>14.030000</td>\n",
       "      <td>89.620000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>213.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       Class M       Class S       Class I       Class P  \\\n",
       "count  10212.000000  10212.000000  10212.000000  10212.000000  10212.000000   \n",
       "mean    2005.852135      0.032805      0.009205      0.168625      0.051900   \n",
       "std        7.542111      0.229542      0.099521      0.861341      0.307019   \n",
       "min     1993.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     1999.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     2006.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%     2012.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max     2020.000000      9.000000      2.000000     28.000000      9.000000   \n",
       "\n",
       "            Class B  Policy Passed  Conflict Indicator            WC  \\\n",
       "count  10212.000000    10212.00000        10212.000000  10190.000000   \n",
       "mean       0.021641        0.13964            0.457697   9442.204220   \n",
       "std        0.156540        0.34663            0.498232   7786.325195   \n",
       "min        0.000000        0.00000            0.000000     44.000000   \n",
       "25%        0.000000        0.00000            0.000000   3838.500000   \n",
       "50%        0.000000        0.00000            0.000000   7439.500000   \n",
       "75%        0.000000        0.00000            1.000000  12438.750000   \n",
       "max        3.000000        1.00000            1.000000  74776.000000   \n",
       "\n",
       "           Analytic  ...         Comma         Colon         SemiC  \\\n",
       "count  10190.000000  ...  10190.000000  10190.000000  10190.000000   \n",
       "mean      98.190282  ...      5.193233      0.303105      0.144628   \n",
       "std        1.085791  ...      3.290078      0.466117      0.190811   \n",
       "min       80.460000  ...      0.210000      0.000000      0.000000   \n",
       "25%       97.880000  ...      3.980000      0.090000      0.060000   \n",
       "50%       98.440000  ...      4.440000      0.160000      0.110000   \n",
       "75%       99.000000  ...      4.990000      0.320000      0.190000   \n",
       "max       99.000000  ...     47.760000     22.710000     11.150000   \n",
       "\n",
       "              QMark        Exclam          Dash         Quote       Apostro  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.031364      0.014043      1.154469      0.260464      0.397916   \n",
       "std        0.105541      0.713727      0.643019      0.439789      1.273977   \n",
       "min        0.000000      0.000000      0.020000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.800000      0.050000      0.230000   \n",
       "50%        0.000000      0.000000      1.040000      0.130000      0.340000   \n",
       "75%        0.020000      0.000000      1.380000      0.300000      0.490000   \n",
       "max        3.400000     49.920000     22.880000     14.030000     89.620000   \n",
       "\n",
       "            Parenth        OtherP  \n",
       "count  10190.000000  10190.000000  \n",
       "mean       1.744508      1.768528  \n",
       "std        1.466890      4.337472  \n",
       "min        0.130000      0.010000  \n",
       "25%        0.860000      0.740000  \n",
       "50%        1.330000      1.170000  \n",
       "75%        2.090000      2.070000  \n",
       "max       17.900000    213.040000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect the data by key descriptive statistics\n",
    "\n",
    "UN_Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 4367,
     "status": "ok",
     "timestamp": 1611639237253,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "1c56fLBjJKA3",
    "outputId": "8b5f5028-9f16-4543-a585-e014b90e8ef7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Passed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>...</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>...</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  Class M  Class S  Class I  Class P  Class B  \\\n",
       "Policy Passed                                                      \n",
       "0              8786     8786     8786     8786     8786     8786   \n",
       "1              1426     1426     1426     1426     1426     1426   \n",
       "\n",
       "               Conflict Indicator    WC  Analytic  Clout  ...  Comma  Colon  \\\n",
       "Policy Passed                                             ...                 \n",
       "0                            8786  8764      8764   8764  ...   8764   8764   \n",
       "1                            1426  1426      1426   1426  ...   1426   1426   \n",
       "\n",
       "               SemiC  QMark  Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "Policy Passed                                                               \n",
       "0               8764   8764    8764  8764   8764     8764     8764    8764  \n",
       "1               1426   1426    1426  1426   1426     1426     1426    1426  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group the data by our label (dependent variable) of policy passage\n",
    "\n",
    "UN_Data.groupby(['Policy Passed']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Policy Passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>1.019000e+04</td>\n",
       "      <td>10212.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.345714</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.881989</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>4.345803e-04</td>\n",
       "      <td>0.13964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.252698</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.193529</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>1.748792e-03</td>\n",
       "      <td>0.34663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.026924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.473887e-07</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.159519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885374</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>6.634241e-05</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.260213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965297</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1.553934e-04</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.463364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.987100</td>\n",
       "      <td>0.022704</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>3.987904e-04</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997075</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.049344</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>9.570280e-02</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       Class M       Class S       Class I       Class P  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.345714      0.000004      0.000001      0.000021      0.000007   \n",
       "std        0.252698      0.000030      0.000017      0.000110      0.000050   \n",
       "min        0.026924      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.159519      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.260213      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.463364      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.997075      0.001033      0.000412      0.002388      0.001449   \n",
       "\n",
       "            Class B  Conflict Indicator            WC      Analytic  \\\n",
       "count  10190.000000        10190.000000  10190.000000  10190.000000   \n",
       "mean       0.000002            0.000056      0.881989      0.016961   \n",
       "std        0.000021            0.000086      0.193529      0.012448   \n",
       "min        0.000000            0.000000      0.021901      0.001303   \n",
       "25%        0.000000            0.000000      0.885374      0.007792   \n",
       "50%        0.000000            0.000000      0.965297      0.012765   \n",
       "75%        0.000000            0.000094      0.987100      0.022704   \n",
       "max        0.000514            0.000492      0.999635      0.049344   \n",
       "\n",
       "              Clout  ...         Colon         SemiC         QMark  \\\n",
       "count  10190.000000  ...  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.011428  ...      0.000081      0.000027      0.000006   \n",
       "std        0.008500  ...      0.000179      0.000070      0.000029   \n",
       "min        0.001053  ...      0.000000      0.000000      0.000000   \n",
       "25%        0.005150  ...      0.000008      0.000004      0.000000   \n",
       "50%        0.008537  ...      0.000020      0.000012      0.000000   \n",
       "75%        0.015068  ...      0.000066      0.000029      0.000001   \n",
       "max        0.040877  ...      0.005398      0.004388      0.001033   \n",
       "\n",
       "             Exclam          Dash         Quote       Apostro       Parenth  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.000003      0.000222      0.000051      0.000066      0.000417   \n",
       "std        0.000154      0.000308      0.000150      0.000281      0.000665   \n",
       "min        0.000000      0.000003      0.000000      0.000000      0.000003   \n",
       "25%        0.000000      0.000069      0.000004      0.000020      0.000073   \n",
       "50%        0.000000      0.000129      0.000013      0.000040      0.000180   \n",
       "75%        0.000000      0.000252      0.000041      0.000077      0.000431   \n",
       "max        0.010615      0.010232      0.006303      0.019591      0.008041   \n",
       "\n",
       "             OtherP  Policy Passed  \n",
       "count  1.019000e+04    10212.00000  \n",
       "mean   4.345803e-04        0.13964  \n",
       "std    1.748792e-03        0.34663  \n",
       "min    4.473887e-07        0.00000  \n",
       "25%    6.634241e-05        0.00000  \n",
       "50%    1.553934e-04        0.00000  \n",
       "75%    3.987904e-04        0.00000  \n",
       "max    9.570280e-02        1.00000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize the data \n",
    "\n",
    "UN_Data1 = tf.keras.utils.normalize(UN_Data.drop(columns = ['Policy Passed']))\n",
    "\n",
    "UN_Data1[\"Policy Passed\"] = UN_Data['Policy Passed']\n",
    "\n",
    "UN_Data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r6BF8hmXGEnF"
   },
   "outputs": [],
   "source": [
    "#Divide our variables between the independent variables (features) and dependent variables (policy passage)\n",
    "\n",
    "labels = UN_Data1 ['Policy Passed']\n",
    "features = UN_Data1.drop(columns= ['Policy Passed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Null Values\n",
    "\n",
    "features = features.fillna(0)\n",
    "labels = labels.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1611639237254,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "4FJRWpUNH8Dk",
    "outputId": "fe514d19-f576-4030-c91f-fc2b562d2213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10212, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect shape of features\n",
    "\n",
    "features = pd.get_dummies(features)\n",
    "features.shape[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-oGgPi4CG0xx"
   },
   "outputs": [],
   "source": [
    "#Define type of feature and label values\n",
    "\n",
    "features = features.values.astype('float32')\n",
    "labels = labels.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XOea_aP_HPSE"
   },
   "outputs": [],
   "source": [
    "#Data Sets for Training\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)\n",
    "features_train, features_validation, labels_train, labels_validation = train_test_split(features_train, labels_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vHSYOIsh5vq4"
   },
   "outputs": [],
   "source": [
    "#Define Precision, Recall, and F1 score metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    weights = sklearn.utils.class_weight.compute_class_weight('balanced', unique_classes, all_together)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4VQ_OqkCHkLM"
   },
   "outputs": [],
   "source": [
    "#Create your model\n",
    "\n",
    "model1 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model2 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model3 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model4 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model5 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model6 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model7 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model8 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model9 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model10 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model11 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model12 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model13 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model14 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model15 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model16 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model17 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model18 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model19 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model20 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model21 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model22 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model23 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model24 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model25 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model26 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model27 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model28 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model29 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model30 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1611639239019,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "7JTlq8aH8mzi",
    "outputId": "a7831e8c-a787-4b03-8006-c9aebd129183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "### Inspect form of model\n",
    "\n",
    "tf.keras.utils.plot_model(model1, to_file='model.png', show_shapes = True, show_dtype=True, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1611639239020,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "52Vb8b-qSIQ4",
    "outputId": "dd0ce729-358b-43ac-8d65-6c1f00ba812a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 3,794\n",
      "Trainable params: 3,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Check Trainable Parameters\n",
    "# Note: All the models are similarly structured\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Vyx2tehuTPfe"
   },
   "outputs": [],
   "source": [
    "#Set checkpoints, metrics, loss, and optimizer functions for the model\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model1.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model3.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model4.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model5.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model6.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model7.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model8.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model9.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model10.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model11.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model12.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model13.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model14.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model15.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model16.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model17.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model18.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model19.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model20.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model21.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model22.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model23.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model24.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model25.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model26.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model27.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model28.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model29.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model30.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 150145,
     "status": "error",
     "timestamp": 1611639383055,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "RhwA7DthFS72",
    "outputId": "64a70fe2-c811-4102-e25e-f708c8ee64e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Fitting\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:390: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 1 1 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 2s 3ms/step - loss: 0.6764 - acc: 0.5911 - precision: 0.1287 - recall: 0.8996 - f1_metric: 0.2199 - val_loss: 0.7104 - val_acc: 0.3458 - val_precision: 0.1304 - val_recall: 0.8555 - val_f1_metric: 0.2230\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6720 - acc: 0.4744 - precision: 0.1332 - recall: 0.7453 - f1_metric: 0.2216 - val_loss: 0.7011 - val_acc: 0.3409 - val_precision: 0.1331 - val_recall: 0.8876 - val_f1_metric: 0.2280\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6649 - acc: 0.4602 - precision: 0.1385 - recall: 0.7661 - f1_metric: 0.2268 - val_loss: 0.6967 - val_acc: 0.3605 - val_precision: 0.1382 - val_recall: 0.7355 - val_f1_metric: 0.2282\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6855 - acc: 0.3721 - precision: 0.1447 - recall: 0.7506 - f1_metric: 0.2367 - val_loss: 0.6726 - val_acc: 0.4333 - val_precision: 0.1303 - val_recall: 0.8086 - val_f1_metric: 0.2206\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6633 - acc: 0.4920 - precision: 0.1379 - recall: 0.7681 - f1_metric: 0.2280 - val_loss: 0.7245 - val_acc: 0.3133 - val_precision: 0.1460 - val_recall: 0.8334 - val_f1_metric: 0.2438\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.6801 - acc: 0.3848 - precision: 0.1536 - recall: 0.7740 - f1_metric: 0.2478 - val_loss: 0.6973 - val_acc: 0.3543 - val_precision: 0.1377 - val_recall: 0.8316 - val_f1_metric: 0.2322\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6820 - acc: 0.3682 - precision: 0.1386 - recall: 0.7519 - f1_metric: 0.2295 - val_loss: 0.6744 - val_acc: 0.4064 - val_precision: 0.1296 - val_recall: 0.7378 - val_f1_metric: 0.2163\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 987us/step - loss: 0.6916 - acc: 0.3873 - precision: 0.1569 - recall: 0.7953 - f1_metric: 0.2553 - val_loss: 0.6586 - val_acc: 0.4657 - val_precision: 0.1293 - val_recall: 0.7545 - val_f1_metric: 0.2163\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 970us/step - loss: 0.6742 - acc: 0.4754 - precision: 0.1433 - recall: 0.6734 - f1_metric: 0.2272 - val_loss: 0.6779 - val_acc: 0.4235 - val_precision: 0.1338 - val_recall: 0.6652 - val_f1_metric: 0.2182\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6585 - acc: 0.4944 - precision: 0.1301 - recall: 0.6202 - f1_metric: 0.2085 - val_loss: 0.7087 - val_acc: 0.3794 - val_precision: 0.1360 - val_recall: 0.6493 - val_f1_metric: 0.2201\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 999us/step - loss: 0.6843 - acc: 0.4311 - precision: 0.1495 - recall: 0.6201 - f1_metric: 0.2323 - val_loss: 0.6902 - val_acc: 0.3984 - val_precision: 0.1408 - val_recall: 0.6499 - val_f1_metric: 0.2265\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6776 - acc: 0.4601 - precision: 0.1487 - recall: 0.6231 - f1_metric: 0.2327 - val_loss: 0.6773 - val_acc: 0.4461 - val_precision: 0.1329 - val_recall: 0.6157 - val_f1_metric: 0.2132\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 926us/step - loss: 0.6800 - acc: 0.4554 - precision: 0.1405 - recall: 0.5650 - f1_metric: 0.2175 - val_loss: 0.6498 - val_acc: 0.6102 - val_precision: 0.1116 - val_recall: 0.2835 - val_f1_metric: 0.1524\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 996us/step - loss: 0.6800 - acc: 0.5046 - precision: 0.1368 - recall: 0.4587 - f1_metric: 0.2016 - val_loss: 0.6908 - val_acc: 0.3856 - val_precision: 0.1395 - val_recall: 0.5617 - val_f1_metric: 0.2175\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 915us/step - loss: 0.6843 - acc: 0.4384 - precision: 0.1432 - recall: 0.4988 - f1_metric: 0.2150 - val_loss: 0.6894 - val_acc: 0.3972 - val_precision: 0.1377 - val_recall: 0.6681 - val_f1_metric: 0.2233\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.6773 - acc: 0.4599 - precision: 0.1416 - recall: 0.5797 - f1_metric: 0.2210 - val_loss: 0.6757 - val_acc: 0.4718 - val_precision: 0.1426 - val_recall: 0.5224 - val_f1_metric: 0.2185\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 947us/step - loss: 0.6648 - acc: 0.5078 - precision: 0.1335 - recall: 0.5544 - f1_metric: 0.2073 - val_loss: 0.6826 - val_acc: 0.4572 - val_precision: 0.1388 - val_recall: 0.5746 - val_f1_metric: 0.2177\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 932us/step - loss: 0.6788 - acc: 0.4629 - precision: 0.1415 - recall: 0.6345 - f1_metric: 0.2245 - val_loss: 0.6907 - val_acc: 0.4229 - val_precision: 0.1315 - val_recall: 0.7481 - val_f1_metric: 0.2195\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 931us/step - loss: 0.6779 - acc: 0.4490 - precision: 0.1437 - recall: 0.6973 - f1_metric: 0.2320 - val_loss: 0.6849 - val_acc: 0.4406 - val_precision: 0.1321 - val_recall: 0.6536 - val_f1_metric: 0.2149\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.6777 - acc: 0.4682 - precision: 0.1376 - recall: 0.6107 - f1_metric: 0.2171 - val_loss: 0.6340 - val_acc: 0.6353 - val_precision: 0.1056 - val_recall: 0.3106 - val_f1_metric: 0.1505\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 913us/step - loss: 0.6719 - acc: 0.5303 - precision: 0.1178 - recall: 0.4450 - f1_metric: 0.1799 - val_loss: 0.6419 - val_acc: 0.6297 - val_precision: 0.0950 - val_recall: 0.2177 - val_f1_metric: 0.1244\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 939us/step - loss: 0.6715 - acc: 0.5284 - precision: 0.1241 - recall: 0.4478 - f1_metric: 0.1847 - val_loss: 0.6728 - val_acc: 0.4859 - val_precision: 0.1230 - val_recall: 0.3963 - val_f1_metric: 0.1807\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 923us/step - loss: 0.6660 - acc: 0.5272 - precision: 0.1246 - recall: 0.3901 - f1_metric: 0.1793 - val_loss: 0.6715 - val_acc: 0.4810 - val_precision: 0.1051 - val_recall: 0.2391 - val_f1_metric: 0.1377\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.6865 - acc: 0.4829 - precision: 0.1277 - recall: 0.3833 - f1_metric: 0.1827 - val_loss: 0.6272 - val_acc: 0.6830 - val_precision: 0.0950 - val_recall: 0.2712 - val_f1_metric: 0.1348\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 942us/step - loss: 0.6713 - acc: 0.5739 - precision: 0.1307 - recall: 0.4371 - f1_metric: 0.1936 - val_loss: 0.7508 - val_acc: 0.3415 - val_precision: 0.1446 - val_recall: 0.8212 - val_f1_metric: 0.2414\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.6809 - acc: 0.4485 - precision: 0.1466 - recall: 0.7069 - f1_metric: 0.2355 - val_loss: 0.6217 - val_acc: 0.7173 - val_precision: 0.0891 - val_recall: 0.2840 - val_f1_metric: 0.1303\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.6660 - acc: 0.5438 - precision: 0.1301 - recall: 0.5117 - f1_metric: 0.1997 - val_loss: 0.6866 - val_acc: 0.4449 - val_precision: 0.1455 - val_recall: 0.6404 - val_f1_metric: 0.2308\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.6685 - acc: 0.4618 - precision: 0.1423 - recall: 0.6204 - f1_metric: 0.2228 - val_loss: 0.6211 - val_acc: 0.6952 - val_precision: 0.0887 - val_recall: 0.3057 - val_f1_metric: 0.1331\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6682 - acc: 0.5501 - precision: 0.1337 - recall: 0.5883 - f1_metric: 0.2116 - val_loss: 0.7076 - val_acc: 0.3886 - val_precision: 0.1611 - val_recall: 0.8128 - val_f1_metric: 0.2629\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6728 - acc: 0.4860 - precision: 0.1413 - recall: 0.5849 - f1_metric: 0.2187 - val_loss: 0.6434 - val_acc: 0.5973 - val_precision: 0.1201 - val_recall: 0.2823 - val_f1_metric: 0.1600\n",
      "Model 2 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5258 - acc: 0.7716 - precision: 0.1308 - recall: 0.7636 - f1_metric: 0.2147 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 885us/step - loss: 0.4000 - acc: 0.8637 - precision: 0.1365 - recall: 0.9701 - f1_metric: 0.2354 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 850us/step - loss: 0.4023 - acc: 0.8620 - precision: 0.1387 - recall: 0.9958 - f1_metric: 0.2389 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3979 - acc: 0.8642 - precision: 0.1359 - recall: 0.9769 - f1_metric: 0.2340 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.3846 - acc: 0.8714 - precision: 0.1295 - recall: 0.9812 - f1_metric: 0.2244 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4006 - acc: 0.8625 - precision: 0.1381 - recall: 0.9776 - f1_metric: 0.2373 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3938 - acc: 0.8650 - precision: 0.1344 - recall: 0.9786 - f1_metric: 0.2320 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.3976 - acc: 0.8621 - precision: 0.1386 - recall: 0.9679 - f1_metric: 0.2370 - val_loss: 0.3945 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.4057 - acc: 0.8583 - precision: 0.1432 - recall: 0.9574 - f1_metric: 0.2433 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.3967 - acc: 0.8605 - precision: 0.1387 - recall: 0.9447 - f1_metric: 0.2373 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 935us/step - loss: 0.3993 - acc: 0.8603 - precision: 0.1392 - recall: 0.9421 - f1_metric: 0.2385 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.3918 - acc: 0.8641 - precision: 0.1352 - recall: 0.9600 - f1_metric: 0.2334 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.3999 - acc: 0.8618 - precision: 0.1380 - recall: 0.9756 - f1_metric: 0.2380 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 862us/step - loss: 0.3980 - acc: 0.8625 - precision: 0.1379 - recall: 0.9851 - f1_metric: 0.2371 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3996 - acc: 0.8604 - precision: 0.1400 - recall: 0.9731 - f1_metric: 0.2396 - val_loss: 0.3909 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.3826 - acc: 0.8702 - precision: 0.1300 - recall: 0.9895 - f1_metric: 0.2250 - val_loss: 0.3909 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4003 - acc: 0.8592 - precision: 0.1407 - recall: 0.9858 - f1_metric: 0.2421 - val_loss: 0.3891 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 856us/step - loss: 0.4037 - acc: 0.8576 - precision: 0.1423 - recall: 0.9946 - f1_metric: 0.2442 - val_loss: 0.3887 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 917us/step - loss: 0.3921 - acc: 0.8620 - precision: 0.1376 - recall: 0.9683 - f1_metric: 0.2361 - val_loss: 0.3876 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3749 - acc: 0.8725 - precision: 0.1275 - recall: 0.9914 - f1_metric: 0.2206 - val_loss: 0.3879 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3996 - acc: 0.8581 - precision: 0.1419 - recall: 0.9728 - f1_metric: 0.2442 - val_loss: 0.3855 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.4006 - acc: 0.8548 - precision: 0.1452 - recall: 1.0000 - f1_metric: 0.2476 - val_loss: 0.3851 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3873 - acc: 0.8641 - precision: 0.1359 - recall: 0.9784 - f1_metric: 0.2338 - val_loss: 0.3829 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3794 - acc: 0.8668 - precision: 0.1332 - recall: 1.0000 - f1_metric: 0.2311 - val_loss: 0.3811 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3976 - acc: 0.8570 - precision: 0.1430 - recall: 0.9862 - f1_metric: 0.2441 - val_loss: 0.3792 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3842 - acc: 0.8650 - precision: 0.1350 - recall: 0.9984 - f1_metric: 0.2333 - val_loss: 0.3773 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.3851 - acc: 0.8609 - precision: 0.1391 - recall: 0.9894 - f1_metric: 0.2392 - val_loss: 0.3743 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 968us/step - loss: 0.3898 - acc: 0.8566 - precision: 0.1434 - recall: 0.9878 - f1_metric: 0.2451 - val_loss: 0.3708 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3741 - acc: 0.8639 - precision: 0.1361 - recall: 1.0000 - f1_metric: 0.2343 - val_loss: 0.3700 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.3687 - acc: 0.8640 - precision: 0.1360 - recall: 1.0000 - f1_metric: 0.2353 - val_loss: 0.3643 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 3 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4602 - acc: 0.8545 - precision: 0.1385 - recall: 0.8613 - f1_metric: 0.2286 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4017 - acc: 0.8639 - precision: 0.1356 - recall: 0.9868 - f1_metric: 0.2343 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 893us/step - loss: 0.4038 - acc: 0.8608 - precision: 0.1393 - recall: 0.9683 - f1_metric: 0.2392 - val_loss: 0.3951 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4056 - acc: 0.8610 - precision: 0.1389 - recall: 0.9991 - f1_metric: 0.2389 - val_loss: 0.3943 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.3976 - acc: 0.8604 - precision: 0.1396 - recall: 0.9928 - f1_metric: 0.2403 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4048 - acc: 0.8593 - precision: 0.1407 - recall: 1.0000 - f1_metric: 0.2417 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3957 - acc: 0.8663 - precision: 0.1337 - recall: 0.9939 - f1_metric: 0.2313 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3933 - acc: 0.8650 - precision: 0.1350 - recall: 0.9832 - f1_metric: 0.2334 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3873 - acc: 0.8681 - precision: 0.1319 - recall: 0.9917 - f1_metric: 0.2278 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 976us/step - loss: 0.3886 - acc: 0.8679 - precision: 0.1321 - recall: 0.9915 - f1_metric: 0.2293 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3952 - acc: 0.8634 - precision: 0.1364 - recall: 0.9919 - f1_metric: 0.2341 - val_loss: 0.3912 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 932us/step - loss: 0.3933 - acc: 0.8655 - precision: 0.1345 - recall: 0.9835 - f1_metric: 0.2324 - val_loss: 0.3913 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3886 - acc: 0.8667 - precision: 0.1333 - recall: 1.0000 - f1_metric: 0.2298 - val_loss: 0.3941 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3976 - acc: 0.8607 - precision: 0.1393 - recall: 0.9824 - f1_metric: 0.2392 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4044 - acc: 0.8567 - precision: 0.1433 - recall: 0.9986 - f1_metric: 0.2459 - val_loss: 0.3912 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3931 - acc: 0.8629 - precision: 0.1371 - recall: 0.9923 - f1_metric: 0.2349 - val_loss: 0.3886 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3919 - acc: 0.8616 - precision: 0.1384 - recall: 0.9967 - f1_metric: 0.2385 - val_loss: 0.3893 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3921 - acc: 0.8619 - precision: 0.1381 - recall: 0.9934 - f1_metric: 0.2378 - val_loss: 0.3887 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3816 - acc: 0.8686 - precision: 0.1314 - recall: 0.9964 - f1_metric: 0.2270 - val_loss: 0.3859 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.3828 - acc: 0.8672 - precision: 0.1328 - recall: 0.9876 - f1_metric: 0.2293 - val_loss: 0.3866 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.3888 - acc: 0.8623 - precision: 0.1377 - recall: 0.9913 - f1_metric: 0.2379 - val_loss: 0.3826 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3883 - acc: 0.8627 - precision: 0.1373 - recall: 1.0000 - f1_metric: 0.2377 - val_loss: 0.3805 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3761 - acc: 0.8672 - precision: 0.1328 - recall: 0.9925 - f1_metric: 0.2289 - val_loss: 0.3779 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 866us/step - loss: 0.3752 - acc: 0.8666 - precision: 0.1334 - recall: 0.9998 - f1_metric: 0.2312 - val_loss: 0.3818 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.3731 - acc: 0.8678 - precision: 0.1322 - recall: 0.9946 - f1_metric: 0.2283 - val_loss: 0.3855 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3751 - acc: 0.8628 - precision: 0.1372 - recall: 0.9989 - f1_metric: 0.2366 - val_loss: 0.3708 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3791 - acc: 0.8617 - precision: 0.1383 - recall: 0.9919 - f1_metric: 0.2386 - val_loss: 0.3712 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3750 - acc: 0.8610 - precision: 0.1390 - recall: 0.9883 - f1_metric: 0.2390 - val_loss: 0.3751 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3577 - acc: 0.8703 - precision: 0.1297 - recall: 1.0000 - f1_metric: 0.2258 - val_loss: 0.3658 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3766 - acc: 0.8578 - precision: 0.1422 - recall: 0.9871 - f1_metric: 0.2437 - val_loss: 0.3654 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 4 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4897 - acc: 0.8351 - precision: 0.1388 - recall: 0.8826 - f1_metric: 0.2329 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3924 - acc: 0.8673 - precision: 0.1327 - recall: 0.9929 - f1_metric: 0.2300 - val_loss: 0.3949 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.4123 - acc: 0.8567 - precision: 0.1430 - recall: 0.9896 - f1_metric: 0.2445 - val_loss: 0.3957 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 946us/step - loss: 0.4013 - acc: 0.8628 - precision: 0.1375 - recall: 0.9962 - f1_metric: 0.2351 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4149 - acc: 0.8566 - precision: 0.1435 - recall: 0.9959 - f1_metric: 0.2456 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4074 - acc: 0.8593 - precision: 0.1409 - recall: 0.9775 - f1_metric: 0.2415 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3947 - acc: 0.8644 - precision: 0.1358 - recall: 0.9969 - f1_metric: 0.2337 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3896 - acc: 0.8647 - precision: 0.1352 - recall: 0.9853 - f1_metric: 0.2340 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 949us/step - loss: 0.4092 - acc: 0.8570 - precision: 0.1431 - recall: 0.9831 - f1_metric: 0.2461 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 945us/step - loss: 0.4004 - acc: 0.8597 - precision: 0.1404 - recall: 0.9857 - f1_metric: 0.2415 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 966us/step - loss: 0.4018 - acc: 0.8593 - precision: 0.1407 - recall: 0.9997 - f1_metric: 0.2415 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 918us/step - loss: 0.4052 - acc: 0.8600 - precision: 0.1400 - recall: 0.9835 - f1_metric: 0.2406 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.3928 - acc: 0.8648 - precision: 0.1352 - recall: 1.0000 - f1_metric: 0.2332 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3921 - acc: 0.8657 - precision: 0.1343 - recall: 0.9886 - f1_metric: 0.2322 - val_loss: 0.3910 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 952us/step - loss: 0.3964 - acc: 0.8619 - precision: 0.1381 - recall: 0.9933 - f1_metric: 0.2384 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3984 - acc: 0.8617 - precision: 0.1383 - recall: 0.9961 - f1_metric: 0.2380 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 951us/step - loss: 0.4007 - acc: 0.8587 - precision: 0.1413 - recall: 0.9927 - f1_metric: 0.2428 - val_loss: 0.3899 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 979us/step - loss: 0.3939 - acc: 0.8628 - precision: 0.1372 - recall: 0.9704 - f1_metric: 0.2361 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 953us/step - loss: 0.3798 - acc: 0.8694 - precision: 0.1306 - recall: 0.9946 - f1_metric: 0.2256 - val_loss: 0.3890 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 930us/step - loss: 0.3884 - acc: 0.8661 - precision: 0.1339 - recall: 0.9988 - f1_metric: 0.2316 - val_loss: 0.3889 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 979us/step - loss: 0.3854 - acc: 0.8656 - precision: 0.1344 - recall: 0.9907 - f1_metric: 0.2315 - val_loss: 0.3881 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 988us/step - loss: 0.3765 - acc: 0.8695 - precision: 0.1305 - recall: 0.9934 - f1_metric: 0.2258 - val_loss: 0.3864 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 942us/step - loss: 0.4020 - acc: 0.8570 - precision: 0.1430 - recall: 0.9898 - f1_metric: 0.2461 - val_loss: 0.3866 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 979us/step - loss: 0.3928 - acc: 0.8611 - precision: 0.1389 - recall: 0.9823 - f1_metric: 0.2372 - val_loss: 0.3901 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 943us/step - loss: 0.3870 - acc: 0.8634 - precision: 0.1366 - recall: 1.0000 - f1_metric: 0.2355 - val_loss: 0.3844 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3951 - acc: 0.8579 - precision: 0.1421 - recall: 0.9826 - f1_metric: 0.2434 - val_loss: 0.3805 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 945us/step - loss: 0.3799 - acc: 0.8649 - precision: 0.1351 - recall: 0.9788 - f1_metric: 0.2340 - val_loss: 0.3815 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 981us/step - loss: 0.3824 - acc: 0.8647 - precision: 0.1353 - recall: 0.9894 - f1_metric: 0.2327 - val_loss: 0.3764 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3906 - acc: 0.8548 - precision: 0.1452 - recall: 0.9980 - f1_metric: 0.2487 - val_loss: 0.3731 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3715 - acc: 0.8655 - precision: 0.1345 - recall: 0.9858 - f1_metric: 0.2325 - val_loss: 0.3700 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 5 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.5048 - acc: 0.8322 - precision: 0.1315 - recall: 0.9376 - f1_metric: 0.2257 - val_loss: 0.3951 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4097 - acc: 0.8612 - precision: 0.1391 - recall: 0.9951 - f1_metric: 0.2383 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4050 - acc: 0.8607 - precision: 0.1395 - recall: 0.9950 - f1_metric: 0.2401 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4089 - acc: 0.8608 - precision: 0.1394 - recall: 0.9878 - f1_metric: 0.2392 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 933us/step - loss: 0.4018 - acc: 0.8636 - precision: 0.1366 - recall: 0.9716 - f1_metric: 0.2339 - val_loss: 0.3987 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4033 - acc: 0.8630 - precision: 0.1371 - recall: 0.9893 - f1_metric: 0.2376 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3980 - acc: 0.8650 - precision: 0.1351 - recall: 0.9938 - f1_metric: 0.2335 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4005 - acc: 0.8613 - precision: 0.1387 - recall: 0.9898 - f1_metric: 0.2385 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3924 - acc: 0.8664 - precision: 0.1336 - recall: 0.9972 - f1_metric: 0.2317 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3786 - acc: 0.8712 - precision: 0.1288 - recall: 0.9964 - f1_metric: 0.2238 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4098 - acc: 0.8573 - precision: 0.1427 - recall: 0.9919 - f1_metric: 0.2442 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3954 - acc: 0.8622 - precision: 0.1378 - recall: 0.9886 - f1_metric: 0.2374 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 973us/step - loss: 0.3882 - acc: 0.8673 - precision: 0.1327 - recall: 0.9925 - f1_metric: 0.2288 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4029 - acc: 0.8584 - precision: 0.1416 - recall: 0.9973 - f1_metric: 0.2426 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4004 - acc: 0.8611 - precision: 0.1389 - recall: 0.9897 - f1_metric: 0.2393 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 924us/step - loss: 0.3818 - acc: 0.8702 - precision: 0.1298 - recall: 0.9900 - f1_metric: 0.2253 - val_loss: 0.3917 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.3958 - acc: 0.8606 - precision: 0.1394 - recall: 0.9920 - f1_metric: 0.2395 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4037 - acc: 0.8575 - precision: 0.1425 - recall: 0.9826 - f1_metric: 0.2450 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 940us/step - loss: 0.3912 - acc: 0.8657 - precision: 0.1343 - recall: 0.9739 - f1_metric: 0.2314 - val_loss: 0.3908 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 911us/step - loss: 0.4060 - acc: 0.8554 - precision: 0.1446 - recall: 0.9968 - f1_metric: 0.2474 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 924us/step - loss: 0.3844 - acc: 0.8664 - precision: 0.1336 - recall: 0.9884 - f1_metric: 0.2319 - val_loss: 0.3894 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 911us/step - loss: 0.3918 - acc: 0.8635 - precision: 0.1365 - recall: 0.9869 - f1_metric: 0.2354 - val_loss: 0.3879 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 933us/step - loss: 0.3967 - acc: 0.8591 - precision: 0.1409 - recall: 0.9995 - f1_metric: 0.2427 - val_loss: 0.3861 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3955 - acc: 0.8602 - precision: 0.1398 - recall: 0.9948 - f1_metric: 0.2402 - val_loss: 0.3969 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3868 - acc: 0.8637 - precision: 0.1363 - recall: 0.9921 - f1_metric: 0.2340 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4061 - acc: 0.8554 - precision: 0.1446 - recall: 0.9918 - f1_metric: 0.2474 - val_loss: 0.3824 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 933us/step - loss: 0.3893 - acc: 0.8627 - precision: 0.1373 - recall: 1.0000 - f1_metric: 0.2371 - val_loss: 0.3808 - val_acc: 0.8654 - val_precision: 0.1323 - val_recall: 0.9808 - val_f1_metric: 0.2297\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3895 - acc: 0.8583 - precision: 0.1416 - recall: 1.0000 - f1_metric: 0.2438 - val_loss: 0.3778 - val_acc: 0.8654 - val_precision: 0.1323 - val_recall: 0.9808 - val_f1_metric: 0.2297\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3886 - acc: 0.8587 - precision: 0.1413 - recall: 0.9937 - f1_metric: 0.2427 - val_loss: 0.3743 - val_acc: 0.8654 - val_precision: 0.1323 - val_recall: 0.9808 - val_f1_metric: 0.2297\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.3845 - acc: 0.8607 - precision: 0.1392 - recall: 0.9863 - f1_metric: 0.2404 - val_loss: 0.3721 - val_acc: 0.8647 - val_precision: 0.1323 - val_recall: 0.9808 - val_f1_metric: 0.2297\n",
      "Model 6 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5022 - acc: 0.8030 - precision: 0.1327 - recall: 0.9175 - f1_metric: 0.2264 - val_loss: 0.3941 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 975us/step - loss: 0.4083 - acc: 0.8614 - precision: 0.1385 - recall: 0.9883 - f1_metric: 0.2384 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 996us/step - loss: 0.3940 - acc: 0.8669 - precision: 0.1330 - recall: 0.9988 - f1_metric: 0.2300 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 933us/step - loss: 0.4197 - acc: 0.8510 - precision: 0.1489 - recall: 0.9757 - f1_metric: 0.2528 - val_loss: 0.3951 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3980 - acc: 0.8635 - precision: 0.1366 - recall: 1.0036 - f1_metric: 0.2361 - val_loss: 0.3948 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 890us/step - loss: 0.3976 - acc: 0.8635 - precision: 0.1367 - recall: 0.9943 - f1_metric: 0.2356 - val_loss: 0.3941 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 913us/step - loss: 0.4047 - acc: 0.8576 - precision: 0.1423 - recall: 0.9958 - f1_metric: 0.2450 - val_loss: 0.3948 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.3921 - acc: 0.8662 - precision: 0.1340 - recall: 0.9889 - f1_metric: 0.2313 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3898 - acc: 0.8664 - precision: 0.1337 - recall: 0.9932 - f1_metric: 0.2315 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3898 - acc: 0.8653 - precision: 0.1346 - recall: 0.9716 - f1_metric: 0.2328 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4006 - acc: 0.8622 - precision: 0.1376 - recall: 0.9964 - f1_metric: 0.2368 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3863 - acc: 0.8697 - precision: 0.1303 - recall: 0.9989 - f1_metric: 0.2258 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3895 - acc: 0.8678 - precision: 0.1321 - recall: 1.0000 - f1_metric: 0.2299 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 924us/step - loss: 0.3993 - acc: 0.8600 - precision: 0.1404 - recall: 1.0005 - f1_metric: 0.2417 - val_loss: 0.3912 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 944us/step - loss: 0.3932 - acc: 0.8629 - precision: 0.1369 - recall: 0.9981 - f1_metric: 0.2371 - val_loss: 0.3911 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 918us/step - loss: 0.3975 - acc: 0.8613 - precision: 0.1387 - recall: 1.0000 - f1_metric: 0.2395 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3902 - acc: 0.8649 - precision: 0.1351 - recall: 0.9584 - f1_metric: 0.2331 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.3953 - acc: 0.8620 - precision: 0.1380 - recall: 0.9974 - f1_metric: 0.2374 - val_loss: 0.3896 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 920us/step - loss: 0.3832 - acc: 0.8685 - precision: 0.1315 - recall: 0.9972 - f1_metric: 0.2284 - val_loss: 0.3894 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3944 - acc: 0.8619 - precision: 0.1381 - recall: 0.9851 - f1_metric: 0.2379 - val_loss: 0.3895 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.3818 - acc: 0.8684 - precision: 0.1316 - recall: 0.9859 - f1_metric: 0.2285 - val_loss: 0.3878 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3919 - acc: 0.8617 - precision: 0.1383 - recall: 1.0000 - f1_metric: 0.2375 - val_loss: 0.3871 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.3988 - acc: 0.8582 - precision: 0.1418 - recall: 1.0000 - f1_metric: 0.2435 - val_loss: 0.3862 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3920 - acc: 0.8611 - precision: 0.1389 - recall: 1.0000 - f1_metric: 0.2395 - val_loss: 0.3864 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 855us/step - loss: 0.3860 - acc: 0.8650 - precision: 0.1350 - recall: 0.9970 - f1_metric: 0.2335 - val_loss: 0.3843 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.3819 - acc: 0.8659 - precision: 0.1341 - recall: 0.9967 - f1_metric: 0.2318 - val_loss: 0.3829 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3971 - acc: 0.8553 - precision: 0.1447 - recall: 0.9995 - f1_metric: 0.2472 - val_loss: 0.3784 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.3773 - acc: 0.8653 - precision: 0.1347 - recall: 1.0000 - f1_metric: 0.2328 - val_loss: 0.3759 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3763 - acc: 0.8641 - precision: 0.1360 - recall: 0.9873 - f1_metric: 0.2347 - val_loss: 0.3840 - val_acc: 0.8654 - val_precision: 0.1323 - val_recall: 0.9808 - val_f1_metric: 0.2297\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3824 - acc: 0.8615 - precision: 0.1386 - recall: 0.9992 - f1_metric: 0.2392 - val_loss: 0.3748 - val_acc: 0.8654 - val_precision: 0.1323 - val_recall: 0.9808 - val_f1_metric: 0.2297\n",
      "Model 7 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4406 - acc: 0.8643 - precision: 0.1353 - recall: 0.9868 - f1_metric: 0.2325 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4108 - acc: 0.8585 - precision: 0.1415 - recall: 1.0000 - f1_metric: 0.2423 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.3914 - acc: 0.8680 - precision: 0.1320 - recall: 0.9855 - f1_metric: 0.2287 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4081 - acc: 0.8605 - precision: 0.1395 - recall: 0.9859 - f1_metric: 0.2391 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4114 - acc: 0.8572 - precision: 0.1428 - recall: 0.9997 - f1_metric: 0.2452 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 996us/step - loss: 0.3881 - acc: 0.8703 - precision: 0.1297 - recall: 0.9883 - f1_metric: 0.2249 - val_loss: 0.3946 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 893us/step - loss: 0.3959 - acc: 0.8658 - precision: 0.1342 - recall: 0.9995 - f1_metric: 0.2319 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3977 - acc: 0.8621 - precision: 0.1379 - recall: 0.9830 - f1_metric: 0.2370 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4046 - acc: 0.8587 - precision: 0.1413 - recall: 0.9984 - f1_metric: 0.2419 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4015 - acc: 0.8607 - precision: 0.1393 - recall: 0.9964 - f1_metric: 0.2400 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4004 - acc: 0.8611 - precision: 0.1389 - recall: 0.9954 - f1_metric: 0.2395 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.3861 - acc: 0.8688 - precision: 0.1312 - recall: 0.9943 - f1_metric: 0.2268 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3858 - acc: 0.8676 - precision: 0.1324 - recall: 0.9956 - f1_metric: 0.2290 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 850us/step - loss: 0.3821 - acc: 0.8697 - precision: 0.1303 - recall: 0.9905 - f1_metric: 0.2254 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.3881 - acc: 0.8680 - precision: 0.1320 - recall: 0.9937 - f1_metric: 0.2279 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3944 - acc: 0.8638 - precision: 0.1362 - recall: 1.0000 - f1_metric: 0.2358 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.3890 - acc: 0.8657 - precision: 0.1343 - recall: 0.9819 - f1_metric: 0.2318 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4023 - acc: 0.8581 - precision: 0.1419 - recall: 0.9711 - f1_metric: 0.2431 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4103 - acc: 0.8523 - precision: 0.1477 - recall: 0.9917 - f1_metric: 0.2524 - val_loss: 0.3883 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.3794 - acc: 0.8702 - precision: 0.1298 - recall: 1.0000 - f1_metric: 0.2258 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 935us/step - loss: 0.3942 - acc: 0.8607 - precision: 0.1393 - recall: 0.9924 - f1_metric: 0.2401 - val_loss: 0.3859 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3901 - acc: 0.8624 - precision: 0.1376 - recall: 0.9915 - f1_metric: 0.2361 - val_loss: 0.3858 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3884 - acc: 0.8638 - precision: 0.1362 - recall: 0.9660 - f1_metric: 0.2347 - val_loss: 0.3880 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3827 - acc: 0.8646 - precision: 0.1353 - recall: 0.9973 - f1_metric: 0.2335 - val_loss: 0.3813 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3813 - acc: 0.8636 - precision: 0.1365 - recall: 0.9996 - f1_metric: 0.2362 - val_loss: 0.3780 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.3843 - acc: 0.8635 - precision: 0.1365 - recall: 0.9874 - f1_metric: 0.2350 - val_loss: 0.3767 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3751 - acc: 0.8667 - precision: 0.1333 - recall: 1.0000 - f1_metric: 0.2310 - val_loss: 0.3755 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3802 - acc: 0.8612 - precision: 0.1388 - recall: 0.9898 - f1_metric: 0.2393 - val_loss: 0.3695 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.3738 - acc: 0.8643 - precision: 0.1358 - recall: 0.9821 - f1_metric: 0.2341 - val_loss: 0.3727 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.3730 - acc: 0.8624 - precision: 0.1376 - recall: 0.9943 - f1_metric: 0.2379 - val_loss: 0.3621 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 8 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5121 - acc: 0.7940 - precision: 0.1388 - recall: 1.1154 - f1_metric: 0.2418 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3938 - acc: 0.8674 - precision: 0.1326 - recall: 0.9995 - f1_metric: 0.2300 - val_loss: 0.3951 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 931us/step - loss: 0.3906 - acc: 0.8682 - precision: 0.1318 - recall: 0.9984 - f1_metric: 0.2287 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3950 - acc: 0.8666 - precision: 0.1334 - recall: 0.9969 - f1_metric: 0.2301 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 913us/step - loss: 0.3996 - acc: 0.8631 - precision: 0.1369 - recall: 0.9948 - f1_metric: 0.2359 - val_loss: 0.3943 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4039 - acc: 0.8594 - precision: 0.1406 - recall: 0.9942 - f1_metric: 0.2413 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3935 - acc: 0.8663 - precision: 0.1337 - recall: 0.9952 - f1_metric: 0.2316 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 880us/step - loss: 0.3865 - acc: 0.8687 - precision: 0.1313 - recall: 0.9895 - f1_metric: 0.2275 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.3993 - acc: 0.8623 - precision: 0.1377 - recall: 0.9989 - f1_metric: 0.2377 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.3984 - acc: 0.8623 - precision: 0.1377 - recall: 0.9966 - f1_metric: 0.2372 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4000 - acc: 0.8614 - precision: 0.1386 - recall: 0.9939 - f1_metric: 0.2384 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3888 - acc: 0.8668 - precision: 0.1332 - recall: 0.9979 - f1_metric: 0.2307 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3837 - acc: 0.8697 - precision: 0.1303 - recall: 0.9773 - f1_metric: 0.2258 - val_loss: 0.3958 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3996 - acc: 0.8610 - precision: 0.1390 - recall: 1.0000 - f1_metric: 0.2375 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3973 - acc: 0.8628 - precision: 0.1372 - recall: 0.9915 - f1_metric: 0.2360 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.3861 - acc: 0.8685 - precision: 0.1315 - recall: 0.9698 - f1_metric: 0.2275 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.4028 - acc: 0.8598 - precision: 0.1402 - recall: 0.9967 - f1_metric: 0.2409 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.3815 - acc: 0.8706 - precision: 0.1294 - recall: 0.9684 - f1_metric: 0.2236 - val_loss: 0.3912 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4025 - acc: 0.8593 - precision: 0.1407 - recall: 0.9978 - f1_metric: 0.2417 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3921 - acc: 0.8654 - precision: 0.1346 - recall: 0.9852 - f1_metric: 0.2325 - val_loss: 0.3911 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.3936 - acc: 0.8625 - precision: 0.1375 - recall: 0.9881 - f1_metric: 0.2368 - val_loss: 0.3896 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4034 - acc: 0.8579 - precision: 0.1421 - recall: 1.0000 - f1_metric: 0.2436 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3950 - acc: 0.8613 - precision: 0.1387 - recall: 1.0000 - f1_metric: 0.2390 - val_loss: 0.3890 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 924us/step - loss: 0.3939 - acc: 0.8612 - precision: 0.1388 - recall: 0.9930 - f1_metric: 0.2383 - val_loss: 0.3875 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3866 - acc: 0.8651 - precision: 0.1349 - recall: 0.9782 - f1_metric: 0.2330 - val_loss: 0.3870 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3882 - acc: 0.8651 - precision: 0.1349 - recall: 0.9946 - f1_metric: 0.2324 - val_loss: 0.3909 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.3939 - acc: 0.8579 - precision: 0.1420 - recall: 0.9949 - f1_metric: 0.2448 - val_loss: 0.3841 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3871 - acc: 0.8635 - precision: 0.1365 - recall: 0.9905 - f1_metric: 0.2352 - val_loss: 0.3850 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3859 - acc: 0.8631 - precision: 0.1369 - recall: 0.9991 - f1_metric: 0.2361 - val_loss: 0.3812 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.3758 - acc: 0.8682 - precision: 0.1318 - recall: 0.9981 - f1_metric: 0.2286 - val_loss: 0.3827 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 9 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5211 - acc: 0.7924 - precision: 0.1433 - recall: 1.2700 - f1_metric: 0.2512 - val_loss: 0.3967 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.3960 - acc: 0.8678 - precision: 0.1322 - recall: 0.9834 - f1_metric: 0.2284 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 951us/step - loss: 0.4168 - acc: 0.8566 - precision: 0.1434 - recall: 1.0000 - f1_metric: 0.2465 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 947us/step - loss: 0.3979 - acc: 0.8643 - precision: 0.1356 - recall: 1.0000 - f1_metric: 0.2347 - val_loss: 0.3945 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3986 - acc: 0.8637 - precision: 0.1363 - recall: 0.9854 - f1_metric: 0.2349 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3911 - acc: 0.8673 - precision: 0.1327 - recall: 0.9758 - f1_metric: 0.2286 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.3874 - acc: 0.8685 - precision: 0.1316 - recall: 1.0002 - f1_metric: 0.2284 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4102 - acc: 0.8566 - precision: 0.1436 - recall: 0.9952 - f1_metric: 0.2459 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 896us/step - loss: 0.4136 - acc: 0.8538 - precision: 0.1462 - recall: 0.9932 - f1_metric: 0.2504 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3977 - acc: 0.8616 - precision: 0.1384 - recall: 1.0000 - f1_metric: 0.2384 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.3918 - acc: 0.8651 - precision: 0.1349 - recall: 0.9912 - f1_metric: 0.2340 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4243 - acc: 0.8474 - precision: 0.1526 - recall: 0.9935 - f1_metric: 0.2594 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3891 - acc: 0.8642 - precision: 0.1358 - recall: 0.9769 - f1_metric: 0.2328 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 862us/step - loss: 0.3879 - acc: 0.8683 - precision: 0.1317 - recall: 0.9974 - f1_metric: 0.2274 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.4006 - acc: 0.8593 - precision: 0.1407 - recall: 0.9948 - f1_metric: 0.2408 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4026 - acc: 0.8575 - precision: 0.1425 - recall: 0.9917 - f1_metric: 0.2450 - val_loss: 0.3899 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3899 - acc: 0.8657 - precision: 0.1343 - recall: 0.9964 - f1_metric: 0.2326 - val_loss: 0.3896 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3937 - acc: 0.8634 - precision: 0.1366 - recall: 0.9944 - f1_metric: 0.2358 - val_loss: 0.3888 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3836 - acc: 0.8669 - precision: 0.1331 - recall: 0.9995 - f1_metric: 0.2303 - val_loss: 0.3876 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.3947 - acc: 0.8598 - precision: 0.1402 - recall: 0.9936 - f1_metric: 0.2411 - val_loss: 0.3876 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.4055 - acc: 0.8556 - precision: 0.1444 - recall: 0.9895 - f1_metric: 0.2464 - val_loss: 0.3869 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3876 - acc: 0.8634 - precision: 0.1366 - recall: 0.9724 - f1_metric: 0.2352 - val_loss: 0.3852 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.3948 - acc: 0.8591 - precision: 0.1409 - recall: 0.9956 - f1_metric: 0.2416 - val_loss: 0.3837 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3841 - acc: 0.8649 - precision: 0.1351 - recall: 0.9936 - f1_metric: 0.2339 - val_loss: 0.3812 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3912 - acc: 0.8583 - precision: 0.1417 - recall: 0.9909 - f1_metric: 0.2438 - val_loss: 0.3804 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.3821 - acc: 0.8638 - precision: 0.1363 - recall: 1.0000 - f1_metric: 0.2351 - val_loss: 0.3774 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3842 - acc: 0.8630 - precision: 0.1370 - recall: 0.9824 - f1_metric: 0.2369 - val_loss: 0.3742 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 918us/step - loss: 0.3684 - acc: 0.8678 - precision: 0.1322 - recall: 0.9898 - f1_metric: 0.2299 - val_loss: 0.3717 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.3797 - acc: 0.8621 - precision: 0.1379 - recall: 1.0000 - f1_metric: 0.2383 - val_loss: 0.3687 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 916us/step - loss: 0.3790 - acc: 0.8609 - precision: 0.1391 - recall: 0.9940 - f1_metric: 0.2379 - val_loss: 0.3645 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 10 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5170 - acc: 0.8311 - precision: 0.1359 - recall: 0.8009 - f1_metric: 0.2206 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3899 - acc: 0.8679 - precision: 0.1321 - recall: 0.9907 - f1_metric: 0.2282 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.4041 - acc: 0.8619 - precision: 0.1382 - recall: 1.0000 - f1_metric: 0.2383 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.3952 - acc: 0.8663 - precision: 0.1336 - recall: 0.9882 - f1_metric: 0.2315 - val_loss: 0.3945 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3910 - acc: 0.8671 - precision: 0.1329 - recall: 0.9853 - f1_metric: 0.2302 - val_loss: 0.3962 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3860 - acc: 0.8690 - precision: 0.1310 - recall: 0.9917 - f1_metric: 0.2274 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4046 - acc: 0.8597 - precision: 0.1403 - recall: 1.0000 - f1_metric: 0.2405 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3919 - acc: 0.8668 - precision: 0.1332 - recall: 0.9989 - f1_metric: 0.2305 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3929 - acc: 0.8643 - precision: 0.1357 - recall: 0.9832 - f1_metric: 0.2345 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 889us/step - loss: 0.3941 - acc: 0.8651 - precision: 0.1349 - recall: 0.9952 - f1_metric: 0.2321 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4098 - acc: 0.8578 - precision: 0.1422 - recall: 0.9971 - f1_metric: 0.2434 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3976 - acc: 0.8625 - precision: 0.1375 - recall: 0.9971 - f1_metric: 0.2379 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3902 - acc: 0.8664 - precision: 0.1336 - recall: 0.9842 - f1_metric: 0.2314 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 913us/step - loss: 0.3857 - acc: 0.8676 - precision: 0.1324 - recall: 1.0000 - f1_metric: 0.2292 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 917us/step - loss: 0.3811 - acc: 0.8710 - precision: 0.1290 - recall: 0.9962 - f1_metric: 0.2240 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3930 - acc: 0.8630 - precision: 0.1370 - recall: 0.9923 - f1_metric: 0.2361 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3865 - acc: 0.8670 - precision: 0.1330 - recall: 0.9745 - f1_metric: 0.2304 - val_loss: 0.3908 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3955 - acc: 0.8611 - precision: 0.1389 - recall: 0.9959 - f1_metric: 0.2380 - val_loss: 0.3908 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 857us/step - loss: 0.3861 - acc: 0.8679 - precision: 0.1321 - recall: 0.9842 - f1_metric: 0.2279 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 859us/step - loss: 0.3873 - acc: 0.8673 - precision: 0.1327 - recall: 0.9799 - f1_metric: 0.2286 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.3854 - acc: 0.8666 - precision: 0.1334 - recall: 1.0000 - f1_metric: 0.2304 - val_loss: 0.3891 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3856 - acc: 0.8681 - precision: 0.1319 - recall: 0.9397 - f1_metric: 0.2273 - val_loss: 0.3885 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3873 - acc: 0.8646 - precision: 0.1354 - recall: 0.9973 - f1_metric: 0.2340 - val_loss: 0.3889 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3966 - acc: 0.8621 - precision: 0.1379 - recall: 0.9806 - f1_metric: 0.2367 - val_loss: 0.3870 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.4052 - acc: 0.8562 - precision: 0.1438 - recall: 0.9965 - f1_metric: 0.2458 - val_loss: 0.3865 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3685 - acc: 0.8737 - precision: 0.1263 - recall: 0.9943 - f1_metric: 0.2193 - val_loss: 0.3846 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.3991 - acc: 0.8575 - precision: 0.1425 - recall: 0.9932 - f1_metric: 0.2445 - val_loss: 0.3829 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.3916 - acc: 0.8591 - precision: 0.1409 - recall: 0.9957 - f1_metric: 0.2427 - val_loss: 0.3801 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.3671 - acc: 0.8719 - precision: 0.1281 - recall: 0.9720 - f1_metric: 0.2213 - val_loss: 0.3790 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 929us/step - loss: 0.3879 - acc: 0.8600 - precision: 0.1399 - recall: 0.9987 - f1_metric: 0.2396 - val_loss: 0.3762 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 11 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4942 - acc: 0.8180 - precision: 0.1389 - recall: 1.4521 - f1_metric: 0.2482 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.3977 - acc: 0.8656 - precision: 0.1340 - recall: 0.9994 - f1_metric: 0.2318 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4109 - acc: 0.8564 - precision: 0.1450 - recall: 1.0128 - f1_metric: 0.2484 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4038 - acc: 0.8610 - precision: 0.1391 - recall: 0.9996 - f1_metric: 0.2401 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.3983 - acc: 0.8641 - precision: 0.1360 - recall: 1.0010 - f1_metric: 0.2340 - val_loss: 0.3960 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4032 - acc: 0.8588 - precision: 0.1423 - recall: 1.0101 - f1_metric: 0.2451 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3930 - acc: 0.8652 - precision: 0.1347 - recall: 0.9924 - f1_metric: 0.2321 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.4120 - acc: 0.8557 - precision: 0.1445 - recall: 1.0007 - f1_metric: 0.2472 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 927us/step - loss: 0.3961 - acc: 0.8629 - precision: 0.1368 - recall: 0.9880 - f1_metric: 0.2364 - val_loss: 0.3948 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.3858 - acc: 0.8686 - precision: 0.1313 - recall: 1.0007 - f1_metric: 0.2283 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 877us/step - loss: 0.4004 - acc: 0.8614 - precision: 0.1386 - recall: 1.0019 - f1_metric: 0.2375 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.3934 - acc: 0.8639 - precision: 0.1360 - recall: 1.0004 - f1_metric: 0.2348 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3792 - acc: 0.8720 - precision: 0.1284 - recall: 0.9898 - f1_metric: 0.2231 - val_loss: 0.3954 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.4002 - acc: 0.8603 - precision: 0.1396 - recall: 0.9939 - f1_metric: 0.2405 - val_loss: 0.3912 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.3885 - acc: 0.8659 - precision: 0.1341 - recall: 0.9658 - f1_metric: 0.2304 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3954 - acc: 0.8620 - precision: 0.1380 - recall: 0.9999 - f1_metric: 0.2377 - val_loss: 0.3908 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4030 - acc: 0.8584 - precision: 0.1416 - recall: 0.9948 - f1_metric: 0.2434 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3995 - acc: 0.8594 - precision: 0.1406 - recall: 0.9862 - f1_metric: 0.2427 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 912us/step - loss: 0.4004 - acc: 0.8590 - precision: 0.1410 - recall: 1.0000 - f1_metric: 0.2424 - val_loss: 0.3897 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.3861 - acc: 0.8658 - precision: 0.1342 - recall: 0.9945 - f1_metric: 0.2319 - val_loss: 0.3893 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3933 - acc: 0.8639 - precision: 0.1361 - recall: 0.9980 - f1_metric: 0.2350 - val_loss: 0.3889 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.3826 - acc: 0.8679 - precision: 0.1321 - recall: 0.9773 - f1_metric: 0.2275 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.3897 - acc: 0.8631 - precision: 0.1369 - recall: 0.9881 - f1_metric: 0.2356 - val_loss: 0.3880 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.4047 - acc: 0.8550 - precision: 0.1450 - recall: 0.9973 - f1_metric: 0.2483 - val_loss: 0.3864 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 923us/step - loss: 0.3975 - acc: 0.8602 - precision: 0.1398 - recall: 0.9820 - f1_metric: 0.2392 - val_loss: 0.3872 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3876 - acc: 0.8632 - precision: 0.1368 - recall: 0.9979 - f1_metric: 0.2353 - val_loss: 0.3851 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3821 - acc: 0.8648 - precision: 0.1352 - recall: 0.9845 - f1_metric: 0.2345 - val_loss: 0.3836 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.3774 - acc: 0.8680 - precision: 0.1320 - recall: 0.9940 - f1_metric: 0.2282 - val_loss: 0.3810 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3885 - acc: 0.8620 - precision: 0.1380 - recall: 0.9805 - f1_metric: 0.2375 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 918us/step - loss: 0.3933 - acc: 0.8590 - precision: 0.1410 - recall: 0.9965 - f1_metric: 0.2429 - val_loss: 0.3782 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 12 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4747 - acc: 0.8528 - precision: 0.1327 - recall: 0.8003 - f1_metric: 0.2163 - val_loss: 0.3979 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3911 - acc: 0.8682 - precision: 0.1319 - recall: 0.9882 - f1_metric: 0.2279 - val_loss: 0.3941 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4004 - acc: 0.8625 - precision: 0.1376 - recall: 0.9992 - f1_metric: 0.2374 - val_loss: 0.3978 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.3967 - acc: 0.8652 - precision: 0.1349 - recall: 0.9879 - f1_metric: 0.2327 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3852 - acc: 0.8706 - precision: 0.1293 - recall: 0.9980 - f1_metric: 0.2240 - val_loss: 0.3952 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3979 - acc: 0.8645 - precision: 0.1357 - recall: 0.9569 - f1_metric: 0.2332 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3942 - acc: 0.8623 - precision: 0.1376 - recall: 0.9891 - f1_metric: 0.2372 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4038 - acc: 0.8607 - precision: 0.1390 - recall: 0.9770 - f1_metric: 0.2389 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4023 - acc: 0.8616 - precision: 0.1386 - recall: 0.9939 - f1_metric: 0.2394 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4016 - acc: 0.8616 - precision: 0.1385 - recall: 0.9917 - f1_metric: 0.2391 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4017 - acc: 0.8587 - precision: 0.1419 - recall: 0.9859 - f1_metric: 0.2433 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 879us/step - loss: 0.4000 - acc: 0.8590 - precision: 0.1404 - recall: 0.9900 - f1_metric: 0.2416 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3963 - acc: 0.8631 - precision: 0.1367 - recall: 0.9960 - f1_metric: 0.2351 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3900 - acc: 0.8671 - precision: 0.1334 - recall: 0.9937 - f1_metric: 0.2302 - val_loss: 0.3917 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 866us/step - loss: 0.3861 - acc: 0.8663 - precision: 0.1326 - recall: 0.9704 - f1_metric: 0.2283 - val_loss: 0.3910 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3989 - acc: 0.8593 - precision: 0.1403 - recall: 0.9725 - f1_metric: 0.2406 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3901 - acc: 0.8644 - precision: 0.1356 - recall: 0.9779 - f1_metric: 0.2336 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3988 - acc: 0.8606 - precision: 0.1394 - recall: 0.9755 - f1_metric: 0.2405 - val_loss: 0.3902 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.3858 - acc: 0.8673 - precision: 0.1325 - recall: 0.9705 - f1_metric: 0.2280 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.3892 - acc: 0.8655 - precision: 0.1345 - recall: 0.9991 - f1_metric: 0.2335 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3943 - acc: 0.8627 - precision: 0.1373 - recall: 0.9820 - f1_metric: 0.2356 - val_loss: 0.3872 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3969 - acc: 0.8603 - precision: 0.1397 - recall: 0.9732 - f1_metric: 0.2399 - val_loss: 0.3874 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.3880 - acc: 0.8645 - precision: 0.1355 - recall: 0.9832 - f1_metric: 0.2341 - val_loss: 0.3865 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.3991 - acc: 0.8573 - precision: 0.1427 - recall: 0.9939 - f1_metric: 0.2446 - val_loss: 0.3850 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.3938 - acc: 0.8609 - precision: 0.1391 - recall: 0.9799 - f1_metric: 0.2386 - val_loss: 0.3870 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3873 - acc: 0.8650 - precision: 0.1350 - recall: 0.9904 - f1_metric: 0.2325 - val_loss: 0.3825 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.3858 - acc: 0.8621 - precision: 0.1379 - recall: 0.9966 - f1_metric: 0.2370 - val_loss: 0.3818 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3824 - acc: 0.8650 - precision: 0.1350 - recall: 1.0000 - f1_metric: 0.2330 - val_loss: 0.3852 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.3784 - acc: 0.8652 - precision: 0.1348 - recall: 0.9979 - f1_metric: 0.2320 - val_loss: 0.3777 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.3850 - acc: 0.8628 - precision: 0.1372 - recall: 0.9990 - f1_metric: 0.2358 - val_loss: 0.3772 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 13 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4955 - acc: 0.8311 - precision: 0.1338 - recall: 0.8533 - f1_metric: 0.2255 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4039 - acc: 0.8643 - precision: 0.1357 - recall: 0.9786 - f1_metric: 0.2338 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4034 - acc: 0.8618 - precision: 0.1381 - recall: 0.9996 - f1_metric: 0.2389 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3968 - acc: 0.8633 - precision: 0.1370 - recall: 0.9812 - f1_metric: 0.2350 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.3954 - acc: 0.8653 - precision: 0.1344 - recall: 0.9936 - f1_metric: 0.2323 - val_loss: 0.3950 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3939 - acc: 0.8659 - precision: 0.1337 - recall: 0.9758 - f1_metric: 0.2307 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3990 - acc: 0.8625 - precision: 0.1374 - recall: 0.9978 - f1_metric: 0.2374 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3964 - acc: 0.8639 - precision: 0.1364 - recall: 0.9975 - f1_metric: 0.2350 - val_loss: 0.3945 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3971 - acc: 0.8633 - precision: 0.1367 - recall: 0.9968 - f1_metric: 0.2363 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 912us/step - loss: 0.3866 - acc: 0.8679 - precision: 0.1320 - recall: 1.0000 - f1_metric: 0.2292 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.3941 - acc: 0.8633 - precision: 0.1369 - recall: 0.9885 - f1_metric: 0.2363 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.3960 - acc: 0.8622 - precision: 0.1377 - recall: 1.0000 - f1_metric: 0.2371 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 902us/step - loss: 0.3939 - acc: 0.8631 - precision: 0.1368 - recall: 0.9846 - f1_metric: 0.2363 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.3977 - acc: 0.8606 - precision: 0.1394 - recall: 0.9987 - f1_metric: 0.2400 - val_loss: 0.3917 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 863us/step - loss: 0.3994 - acc: 0.8602 - precision: 0.1398 - recall: 0.9846 - f1_metric: 0.2401 - val_loss: 0.3912 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3993 - acc: 0.8623 - precision: 0.1377 - recall: 0.9875 - f1_metric: 0.2378 - val_loss: 0.3912 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.3996 - acc: 0.8582 - precision: 0.1418 - recall: 0.9947 - f1_metric: 0.2442 - val_loss: 0.3911 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.3926 - acc: 0.8643 - precision: 0.1357 - recall: 0.9973 - f1_metric: 0.2346 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.3931 - acc: 0.8640 - precision: 0.1360 - recall: 0.9818 - f1_metric: 0.2345 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3933 - acc: 0.8642 - precision: 0.1358 - recall: 0.9977 - f1_metric: 0.2337 - val_loss: 0.3899 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.3958 - acc: 0.8613 - precision: 0.1387 - recall: 0.9833 - f1_metric: 0.2394 - val_loss: 0.3892 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.4028 - acc: 0.8577 - precision: 0.1423 - recall: 0.9995 - f1_metric: 0.2448 - val_loss: 0.3901 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 863us/step - loss: 0.3890 - acc: 0.8643 - precision: 0.1357 - recall: 0.9985 - f1_metric: 0.2340 - val_loss: 0.3880 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 866us/step - loss: 0.3944 - acc: 0.8624 - precision: 0.1376 - recall: 0.9875 - f1_metric: 0.2357 - val_loss: 0.3870 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3816 - acc: 0.8682 - precision: 0.1318 - recall: 0.9798 - f1_metric: 0.2275 - val_loss: 0.3875 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3937 - acc: 0.8614 - precision: 0.1386 - recall: 0.9927 - f1_metric: 0.2376 - val_loss: 0.3852 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3826 - acc: 0.8662 - precision: 0.1338 - recall: 0.9924 - f1_metric: 0.2314 - val_loss: 0.3828 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.3908 - acc: 0.8601 - precision: 0.1399 - recall: 0.9955 - f1_metric: 0.2410 - val_loss: 0.3814 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3901 - acc: 0.8599 - precision: 0.1401 - recall: 1.0000 - f1_metric: 0.2414 - val_loss: 0.3790 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3774 - acc: 0.8646 - precision: 0.1354 - recall: 0.9935 - f1_metric: 0.2334 - val_loss: 0.3767 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 14 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4976 - acc: 0.8310 - precision: 0.1501 - recall: 0.9415 - f1_metric: 0.2519 - val_loss: 0.3958 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 958us/step - loss: 0.3906 - acc: 0.8696 - precision: 0.1304 - recall: 0.9942 - f1_metric: 0.2246 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.3918 - acc: 0.8676 - precision: 0.1324 - recall: 0.9954 - f1_metric: 0.2283 - val_loss: 0.3941 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.4012 - acc: 0.8625 - precision: 0.1375 - recall: 0.9694 - f1_metric: 0.2369 - val_loss: 0.3945 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4091 - acc: 0.8589 - precision: 0.1411 - recall: 0.9983 - f1_metric: 0.2419 - val_loss: 0.3945 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.4002 - acc: 0.8635 - precision: 0.1365 - recall: 0.9884 - f1_metric: 0.2343 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3995 - acc: 0.8599 - precision: 0.1401 - recall: 0.9751 - f1_metric: 0.2408 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.3881 - acc: 0.8676 - precision: 0.1324 - recall: 0.9962 - f1_metric: 0.2294 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.3959 - acc: 0.8640 - precision: 0.1359 - recall: 0.9886 - f1_metric: 0.2339 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.4105 - acc: 0.8559 - precision: 0.1441 - recall: 0.9877 - f1_metric: 0.2457 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 970us/step - loss: 0.3970 - acc: 0.8619 - precision: 0.1381 - recall: 0.9799 - f1_metric: 0.2381 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3963 - acc: 0.8620 - precision: 0.1380 - recall: 0.9827 - f1_metric: 0.2382 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.3901 - acc: 0.8653 - precision: 0.1347 - recall: 0.9976 - f1_metric: 0.2323 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 892us/step - loss: 0.3930 - acc: 0.8674 - precision: 0.1326 - recall: 0.9981 - f1_metric: 0.2300 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3846 - acc: 0.8686 - precision: 0.1314 - recall: 0.9912 - f1_metric: 0.2279 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3863 - acc: 0.8670 - precision: 0.1330 - recall: 0.9855 - f1_metric: 0.2295 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3768 - acc: 0.8722 - precision: 0.1278 - recall: 0.9841 - f1_metric: 0.2221 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3846 - acc: 0.8680 - precision: 0.1320 - recall: 0.9980 - f1_metric: 0.2289 - val_loss: 0.3910 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.3896 - acc: 0.8643 - precision: 0.1357 - recall: 0.9876 - f1_metric: 0.2340 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3965 - acc: 0.8604 - precision: 0.1396 - recall: 0.9919 - f1_metric: 0.2407 - val_loss: 0.3910 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3984 - acc: 0.8599 - precision: 0.1401 - recall: 1.0000 - f1_metric: 0.2415 - val_loss: 0.3877 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.3923 - acc: 0.8621 - precision: 0.1380 - recall: 0.9819 - f1_metric: 0.2363 - val_loss: 0.3898 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3757 - acc: 0.8719 - precision: 0.1281 - recall: 0.9796 - f1_metric: 0.2223 - val_loss: 0.3900 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.3910 - acc: 0.8614 - precision: 0.1386 - recall: 1.0000 - f1_metric: 0.2389 - val_loss: 0.3858 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.3831 - acc: 0.8656 - precision: 0.1344 - recall: 0.9578 - f1_metric: 0.2318 - val_loss: 0.3816 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3795 - acc: 0.8652 - precision: 0.1348 - recall: 1.0000 - f1_metric: 0.2334 - val_loss: 0.3806 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3874 - acc: 0.8593 - precision: 0.1407 - recall: 0.9787 - f1_metric: 0.2406 - val_loss: 0.3770 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 856us/step - loss: 0.3821 - acc: 0.8631 - precision: 0.1369 - recall: 0.9901 - f1_metric: 0.2369 - val_loss: 0.3765 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 937us/step - loss: 0.3802 - acc: 0.8641 - precision: 0.1359 - recall: 0.9995 - f1_metric: 0.2335 - val_loss: 0.3711 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 942us/step - loss: 0.3859 - acc: 0.8571 - precision: 0.1429 - recall: 0.9891 - f1_metric: 0.2463 - val_loss: 0.3718 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 15 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5117 - acc: 0.8073 - precision: 0.1474 - recall: 1.1339 - f1_metric: 0.2563 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 930us/step - loss: 0.3930 - acc: 0.8695 - precision: 0.1305 - recall: 0.9987 - f1_metric: 0.2256 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 972us/step - loss: 0.4008 - acc: 0.8645 - precision: 0.1355 - recall: 0.9957 - f1_metric: 0.2351 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3987 - acc: 0.8636 - precision: 0.1364 - recall: 0.9920 - f1_metric: 0.2354 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3952 - acc: 0.8666 - precision: 0.1334 - recall: 0.9700 - f1_metric: 0.2300 - val_loss: 0.3955 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4050 - acc: 0.8589 - precision: 0.1410 - recall: 0.9998 - f1_metric: 0.2419 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3947 - acc: 0.8646 - precision: 0.1354 - recall: 0.9946 - f1_metric: 0.2343 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4133 - acc: 0.8564 - precision: 0.1436 - recall: 1.0000 - f1_metric: 0.2456 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.3960 - acc: 0.8639 - precision: 0.1361 - recall: 0.9920 - f1_metric: 0.2352 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4066 - acc: 0.8567 - precision: 0.1433 - recall: 0.9981 - f1_metric: 0.2466 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3973 - acc: 0.8618 - precision: 0.1382 - recall: 0.9993 - f1_metric: 0.2384 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4020 - acc: 0.8607 - precision: 0.1393 - recall: 1.0000 - f1_metric: 0.2408 - val_loss: 0.3917 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3998 - acc: 0.8601 - precision: 0.1399 - recall: 0.9904 - f1_metric: 0.2407 - val_loss: 0.3914 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3988 - acc: 0.8602 - precision: 0.1398 - recall: 0.9973 - f1_metric: 0.2407 - val_loss: 0.3914 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4043 - acc: 0.8574 - precision: 0.1426 - recall: 0.9880 - f1_metric: 0.2441 - val_loss: 0.3911 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.3844 - acc: 0.8701 - precision: 0.1299 - recall: 0.9814 - f1_metric: 0.2253 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 998us/step - loss: 0.3934 - acc: 0.8621 - precision: 0.1379 - recall: 0.9883 - f1_metric: 0.2379 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4038 - acc: 0.8571 - precision: 0.1429 - recall: 0.9900 - f1_metric: 0.2441 - val_loss: 0.3902 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 956us/step - loss: 0.4053 - acc: 0.8548 - precision: 0.1452 - recall: 0.9941 - f1_metric: 0.2496 - val_loss: 0.3893 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3919 - acc: 0.8627 - precision: 0.1373 - recall: 0.9997 - f1_metric: 0.2361 - val_loss: 0.3887 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3917 - acc: 0.8637 - precision: 0.1363 - recall: 1.0000 - f1_metric: 0.2360 - val_loss: 0.3879 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 917us/step - loss: 0.3789 - acc: 0.8685 - precision: 0.1316 - recall: 0.9622 - f1_metric: 0.2270 - val_loss: 0.3874 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 941us/step - loss: 0.3969 - acc: 0.8601 - precision: 0.1399 - recall: 1.0000 - f1_metric: 0.2406 - val_loss: 0.3866 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 941us/step - loss: 0.3943 - acc: 0.8589 - precision: 0.1411 - recall: 1.0000 - f1_metric: 0.2422 - val_loss: 0.3845 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 950us/step - loss: 0.3977 - acc: 0.8579 - precision: 0.1421 - recall: 0.9998 - f1_metric: 0.2442 - val_loss: 0.3856 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3906 - acc: 0.8607 - precision: 0.1393 - recall: 0.9994 - f1_metric: 0.2405 - val_loss: 0.3808 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3867 - acc: 0.8621 - precision: 0.1379 - recall: 0.9889 - f1_metric: 0.2385 - val_loss: 0.3808 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.3743 - acc: 0.8687 - precision: 0.1313 - recall: 1.0000 - f1_metric: 0.2265 - val_loss: 0.3782 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3735 - acc: 0.8681 - precision: 0.1319 - recall: 1.0000 - f1_metric: 0.2292 - val_loss: 0.3782 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.3876 - acc: 0.8578 - precision: 0.1423 - recall: 0.9813 - f1_metric: 0.2440 - val_loss: 0.3729 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 16 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5185 - acc: 0.8370 - precision: 0.1194 - recall: 0.8498 - f1_metric: 0.2045 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3982 - acc: 0.8640 - precision: 0.1347 - recall: 0.9857 - f1_metric: 0.2320 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3898 - acc: 0.8687 - precision: 0.1314 - recall: 0.9991 - f1_metric: 0.2277 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 935us/step - loss: 0.3907 - acc: 0.8661 - precision: 0.1337 - recall: 0.9989 - f1_metric: 0.2314 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3979 - acc: 0.8637 - precision: 0.1364 - recall: 0.9945 - f1_metric: 0.2351 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3899 - acc: 0.8668 - precision: 0.1326 - recall: 0.9816 - f1_metric: 0.2295 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3877 - acc: 0.8681 - precision: 0.1316 - recall: 0.9961 - f1_metric: 0.2274 - val_loss: 0.3956 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 956us/step - loss: 0.4011 - acc: 0.8608 - precision: 0.1387 - recall: 0.9956 - f1_metric: 0.2392 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3928 - acc: 0.8645 - precision: 0.1355 - recall: 0.9638 - f1_metric: 0.2323 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4017 - acc: 0.8606 - precision: 0.1393 - recall: 0.9953 - f1_metric: 0.2399 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3931 - acc: 0.8643 - precision: 0.1357 - recall: 0.9886 - f1_metric: 0.2328 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3947 - acc: 0.8649 - precision: 0.1351 - recall: 0.9935 - f1_metric: 0.2334 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3983 - acc: 0.8609 - precision: 0.1394 - recall: 0.9930 - f1_metric: 0.2404 - val_loss: 0.3960 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3989 - acc: 0.8625 - precision: 0.1375 - recall: 0.9908 - f1_metric: 0.2373 - val_loss: 0.3913 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3989 - acc: 0.8603 - precision: 0.1397 - recall: 0.9907 - f1_metric: 0.2399 - val_loss: 0.3913 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3907 - acc: 0.8657 - precision: 0.1343 - recall: 0.9950 - f1_metric: 0.2311 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4029 - acc: 0.8581 - precision: 0.1418 - recall: 0.9991 - f1_metric: 0.2441 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3981 - acc: 0.8601 - precision: 0.1399 - recall: 0.9906 - f1_metric: 0.2411 - val_loss: 0.3895 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.4061 - acc: 0.8556 - precision: 0.1444 - recall: 0.9800 - f1_metric: 0.2465 - val_loss: 0.3896 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3976 - acc: 0.8604 - precision: 0.1396 - recall: 0.9993 - f1_metric: 0.2398 - val_loss: 0.3887 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3859 - acc: 0.8652 - precision: 0.1348 - recall: 1.0000 - f1_metric: 0.2333 - val_loss: 0.3871 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3838 - acc: 0.8652 - precision: 0.1348 - recall: 0.9891 - f1_metric: 0.2331 - val_loss: 0.3862 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.3890 - acc: 0.8646 - precision: 0.1354 - recall: 0.9759 - f1_metric: 0.2326 - val_loss: 0.3849 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3891 - acc: 0.8627 - precision: 0.1373 - recall: 0.9937 - f1_metric: 0.2361 - val_loss: 0.3831 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 985us/step - loss: 0.3954 - acc: 0.8576 - precision: 0.1424 - recall: 0.9991 - f1_metric: 0.2454 - val_loss: 0.3806 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3771 - acc: 0.8676 - precision: 0.1324 - recall: 0.9873 - f1_metric: 0.2283 - val_loss: 0.3789 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 947us/step - loss: 0.3804 - acc: 0.8644 - precision: 0.1356 - recall: 0.9929 - f1_metric: 0.2339 - val_loss: 0.3751 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 976us/step - loss: 0.3878 - acc: 0.8570 - precision: 0.1430 - recall: 0.9900 - f1_metric: 0.2458 - val_loss: 0.3715 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.3811 - acc: 0.8591 - precision: 0.1409 - recall: 0.9914 - f1_metric: 0.2425 - val_loss: 0.3721 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 939us/step - loss: 0.3738 - acc: 0.8655 - precision: 0.1345 - recall: 0.9917 - f1_metric: 0.2315 - val_loss: 0.3735 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 17 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4955 - acc: 0.8303 - precision: 0.1395 - recall: 0.8076 - f1_metric: 0.2222 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 912us/step - loss: 0.3927 - acc: 0.8680 - precision: 0.1320 - recall: 0.9873 - f1_metric: 0.2281 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 944us/step - loss: 0.3892 - acc: 0.8693 - precision: 0.1307 - recall: 0.9967 - f1_metric: 0.2265 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 936us/step - loss: 0.4093 - acc: 0.8606 - precision: 0.1394 - recall: 0.9985 - f1_metric: 0.2408 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4078 - acc: 0.8595 - precision: 0.1405 - recall: 1.0000 - f1_metric: 0.2426 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 919us/step - loss: 0.4034 - acc: 0.8638 - precision: 0.1362 - recall: 0.9746 - f1_metric: 0.2343 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3940 - acc: 0.8674 - precision: 0.1327 - recall: 0.9939 - f1_metric: 0.2300 - val_loss: 0.3950 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 935us/step - loss: 0.4162 - acc: 0.8534 - precision: 0.1466 - recall: 1.0000 - f1_metric: 0.2506 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4050 - acc: 0.8600 - precision: 0.1400 - recall: 0.9790 - f1_metric: 0.2412 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3953 - acc: 0.8634 - precision: 0.1366 - recall: 1.0000 - f1_metric: 0.2364 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3837 - acc: 0.8692 - precision: 0.1308 - recall: 0.9992 - f1_metric: 0.2270 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4099 - acc: 0.8562 - precision: 0.1438 - recall: 0.9862 - f1_metric: 0.2455 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.3841 - acc: 0.8710 - precision: 0.1290 - recall: 1.0000 - f1_metric: 0.2233 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.3991 - acc: 0.8613 - precision: 0.1387 - recall: 1.0000 - f1_metric: 0.2387 - val_loss: 0.3911 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.3907 - acc: 0.8655 - precision: 0.1345 - recall: 0.9977 - f1_metric: 0.2336 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.4044 - acc: 0.8574 - precision: 0.1426 - recall: 0.9784 - f1_metric: 0.2442 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 890us/step - loss: 0.3893 - acc: 0.8647 - precision: 0.1353 - recall: 0.9908 - f1_metric: 0.2330 - val_loss: 0.3917 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 911us/step - loss: 0.3981 - acc: 0.8612 - precision: 0.1388 - recall: 0.9857 - f1_metric: 0.2386 - val_loss: 0.3892 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4048 - acc: 0.8573 - precision: 0.1427 - recall: 0.9981 - f1_metric: 0.2453 - val_loss: 0.3890 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3775 - acc: 0.8703 - precision: 0.1297 - recall: 0.9966 - f1_metric: 0.2252 - val_loss: 0.3883 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 969us/step - loss: 0.4045 - acc: 0.8565 - precision: 0.1435 - recall: 0.9912 - f1_metric: 0.2454 - val_loss: 0.3879 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3954 - acc: 0.8598 - precision: 0.1402 - recall: 0.9965 - f1_metric: 0.2416 - val_loss: 0.3860 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3815 - acc: 0.8670 - precision: 0.1330 - recall: 0.9954 - f1_metric: 0.2298 - val_loss: 0.3846 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3823 - acc: 0.8679 - precision: 0.1320 - recall: 0.9818 - f1_metric: 0.2286 - val_loss: 0.3838 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.3959 - acc: 0.8578 - precision: 0.1422 - recall: 0.9958 - f1_metric: 0.2450 - val_loss: 0.3835 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3718 - acc: 0.8708 - precision: 0.1292 - recall: 1.0000 - f1_metric: 0.2243 - val_loss: 0.3800 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 953us/step - loss: 0.3879 - acc: 0.8619 - precision: 0.1380 - recall: 0.9797 - f1_metric: 0.2373 - val_loss: 0.3773 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.3884 - acc: 0.8604 - precision: 0.1396 - recall: 0.9726 - f1_metric: 0.2392 - val_loss: 0.3775 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3628 - acc: 0.8738 - precision: 0.1262 - recall: 0.9593 - f1_metric: 0.2194 - val_loss: 0.3737 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3654 - acc: 0.8695 - precision: 0.1305 - recall: 1.0000 - f1_metric: 0.2260 - val_loss: 0.3691 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 18 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5073 - acc: 0.8269 - precision: 0.1274 - recall: 1.0204 - f1_metric: 0.2232 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.4036 - acc: 0.8637 - precision: 0.1352 - recall: 1.0490 - f1_metric: 0.2350 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4133 - acc: 0.8588 - precision: 0.1389 - recall: 1.0549 - f1_metric: 0.2409 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.3983 - acc: 0.8637 - precision: 0.1339 - recall: 1.1088 - f1_metric: 0.2339 - val_loss: 0.3945 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4008 - acc: 0.8628 - precision: 0.1323 - recall: 1.0908 - f1_metric: 0.2320 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 860us/step - loss: 0.3912 - acc: 0.8678 - precision: 0.1300 - recall: 1.1416 - f1_metric: 0.2293 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1320 - val_recall: 0.9808 - val_f1_metric: 0.2293\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.4048 - acc: 0.8584 - precision: 0.1410 - recall: 1.1558 - f1_metric: 0.2473 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 860us/step - loss: 0.3924 - acc: 0.8662 - precision: 0.1314 - recall: 1.0784 - f1_metric: 0.2309 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4133 - acc: 0.8542 - precision: 0.1443 - recall: 1.0820 - f1_metric: 0.2505 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4037 - acc: 0.8600 - precision: 0.1402 - recall: 1.0885 - f1_metric: 0.2437 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 935us/step - loss: 0.3804 - acc: 0.8721 - precision: 0.1290 - recall: 1.0522 - f1_metric: 0.2256 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 944us/step - loss: 0.4005 - acc: 0.8594 - precision: 0.1385 - recall: 1.0245 - f1_metric: 0.2399 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4090 - acc: 0.8556 - precision: 0.1425 - recall: 1.0147 - f1_metric: 0.2460 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3854 - acc: 0.8687 - precision: 0.1312 - recall: 1.0269 - f1_metric: 0.2281 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 944us/step - loss: 0.3993 - acc: 0.8609 - precision: 0.1386 - recall: 1.0038 - f1_metric: 0.2384 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 996us/step - loss: 0.3970 - acc: 0.8621 - precision: 0.1368 - recall: 1.0000 - f1_metric: 0.2357 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 866us/step - loss: 0.4178 - acc: 0.8502 - precision: 0.1488 - recall: 0.9791 - f1_metric: 0.2526 - val_loss: 0.3914 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 921us/step - loss: 0.3967 - acc: 0.8605 - precision: 0.1390 - recall: 1.0066 - f1_metric: 0.2400 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 922us/step - loss: 0.3901 - acc: 0.8652 - precision: 0.1342 - recall: 0.9810 - f1_metric: 0.2319 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 957us/step - loss: 0.4047 - acc: 0.8571 - precision: 0.1421 - recall: 0.9910 - f1_metric: 0.2438 - val_loss: 0.3902 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 944us/step - loss: 0.3868 - acc: 0.8658 - precision: 0.1343 - recall: 1.0020 - f1_metric: 0.2324 - val_loss: 0.3895 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 995us/step - loss: 0.3915 - acc: 0.8636 - precision: 0.1363 - recall: 0.9882 - f1_metric: 0.2350 - val_loss: 0.3899 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.3991 - acc: 0.8604 - precision: 0.1395 - recall: 0.9958 - f1_metric: 0.2398 - val_loss: 0.3892 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.3936 - acc: 0.8595 - precision: 0.1405 - recall: 0.9801 - f1_metric: 0.2410 - val_loss: 0.3879 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3892 - acc: 0.8632 - precision: 0.1368 - recall: 1.0000 - f1_metric: 0.2358 - val_loss: 0.3868 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.3945 - acc: 0.8609 - precision: 0.1391 - recall: 0.9965 - f1_metric: 0.2394 - val_loss: 0.3844 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 967us/step - loss: 0.3803 - acc: 0.8679 - precision: 0.1321 - recall: 0.9832 - f1_metric: 0.2281 - val_loss: 0.3827 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3920 - acc: 0.8603 - precision: 0.1397 - recall: 1.0000 - f1_metric: 0.2412 - val_loss: 0.3797 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 944us/step - loss: 0.3755 - acc: 0.8684 - precision: 0.1316 - recall: 0.9895 - f1_metric: 0.2280 - val_loss: 0.3772 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3817 - acc: 0.8634 - precision: 0.1366 - recall: 0.9992 - f1_metric: 0.2357 - val_loss: 0.3747 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 19 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5340 - acc: 0.7974 - precision: 0.1391 - recall: 0.9839 - f1_metric: 0.2373 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 911us/step - loss: 0.4007 - acc: 0.8628 - precision: 0.1371 - recall: 0.9760 - f1_metric: 0.2364 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3973 - acc: 0.8660 - precision: 0.1340 - recall: 1.0000 - f1_metric: 0.2315 - val_loss: 0.3943 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3945 - acc: 0.8640 - precision: 0.1366 - recall: 0.9861 - f1_metric: 0.2360 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4036 - acc: 0.8616 - precision: 0.1383 - recall: 0.9937 - f1_metric: 0.2382 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 966us/step - loss: 0.3964 - acc: 0.8632 - precision: 0.1367 - recall: 1.0000 - f1_metric: 0.2359 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.3964 - acc: 0.8636 - precision: 0.1363 - recall: 1.0000 - f1_metric: 0.2361 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4068 - acc: 0.8579 - precision: 0.1420 - recall: 0.9895 - f1_metric: 0.2429 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4096 - acc: 0.8567 - precision: 0.1435 - recall: 0.9787 - f1_metric: 0.2456 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4206 - acc: 0.8526 - precision: 0.1474 - recall: 0.9939 - f1_metric: 0.2506 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 2ms/step - loss: 0.4012 - acc: 0.8582 - precision: 0.1418 - recall: 0.9654 - f1_metric: 0.2409 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3892 - acc: 0.8676 - precision: 0.1324 - recall: 0.9909 - f1_metric: 0.2293 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3830 - acc: 0.8702 - precision: 0.1298 - recall: 0.9928 - f1_metric: 0.2242 - val_loss: 0.3941 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3861 - acc: 0.8679 - precision: 0.1321 - recall: 0.9768 - f1_metric: 0.2287 - val_loss: 0.3913 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3942 - acc: 0.8614 - precision: 0.1386 - recall: 0.9938 - f1_metric: 0.2384 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 929us/step - loss: 0.3877 - acc: 0.8657 - precision: 0.1343 - recall: 0.9990 - f1_metric: 0.2326 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3948 - acc: 0.8632 - precision: 0.1368 - recall: 0.9975 - f1_metric: 0.2367 - val_loss: 0.3901 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 945us/step - loss: 0.4006 - acc: 0.8594 - precision: 0.1407 - recall: 1.0000 - f1_metric: 0.2417 - val_loss: 0.3902 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 937us/step - loss: 0.3868 - acc: 0.8654 - precision: 0.1346 - recall: 0.9846 - f1_metric: 0.2321 - val_loss: 0.3888 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3832 - acc: 0.8678 - precision: 0.1322 - recall: 0.9867 - f1_metric: 0.2277 - val_loss: 0.3883 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3908 - acc: 0.8645 - precision: 0.1355 - recall: 1.0000 - f1_metric: 0.2339 - val_loss: 0.3882 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 917us/step - loss: 0.3923 - acc: 0.8619 - precision: 0.1381 - recall: 0.9968 - f1_metric: 0.2367 - val_loss: 0.3864 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 919us/step - loss: 0.3959 - acc: 0.8591 - precision: 0.1409 - recall: 0.9943 - f1_metric: 0.2414 - val_loss: 0.3855 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3877 - acc: 0.8637 - precision: 0.1363 - recall: 0.9747 - f1_metric: 0.2350 - val_loss: 0.3853 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 933us/step - loss: 0.3945 - acc: 0.8594 - precision: 0.1406 - recall: 0.9899 - f1_metric: 0.2419 - val_loss: 0.3835 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3872 - acc: 0.8635 - precision: 0.1365 - recall: 0.9910 - f1_metric: 0.2362 - val_loss: 0.3823 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3961 - acc: 0.8561 - precision: 0.1439 - recall: 0.9998 - f1_metric: 0.2457 - val_loss: 0.3804 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.3876 - acc: 0.8596 - precision: 0.1404 - recall: 0.9940 - f1_metric: 0.2409 - val_loss: 0.3792 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3749 - acc: 0.8658 - precision: 0.1342 - recall: 0.9987 - f1_metric: 0.2310 - val_loss: 0.3779 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.3835 - acc: 0.8624 - precision: 0.1376 - recall: 1.0000 - f1_metric: 0.2369 - val_loss: 0.3718 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 20 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4919 - acc: 0.8427 - precision: 0.1397 - recall: 1.1324 - f1_metric: 0.2441 - val_loss: 0.3959 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3932 - acc: 0.8687 - precision: 0.1309 - recall: 0.9825 - f1_metric: 0.2265 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4062 - acc: 0.8609 - precision: 0.1393 - recall: 0.9846 - f1_metric: 0.2390 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3981 - acc: 0.8650 - precision: 0.1349 - recall: 1.0000 - f1_metric: 0.2334 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3830 - acc: 0.8720 - precision: 0.1282 - recall: 0.9983 - f1_metric: 0.2226 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.3992 - acc: 0.8625 - precision: 0.1373 - recall: 0.9920 - f1_metric: 0.2371 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.3987 - acc: 0.8622 - precision: 0.1382 - recall: 0.9992 - f1_metric: 0.2379 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 913us/step - loss: 0.4030 - acc: 0.8601 - precision: 0.1398 - recall: 0.9928 - f1_metric: 0.2405 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3982 - acc: 0.8630 - precision: 0.1368 - recall: 0.9757 - f1_metric: 0.2352 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3963 - acc: 0.8628 - precision: 0.1371 - recall: 0.9870 - f1_metric: 0.2362 - val_loss: 0.3941 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.3948 - acc: 0.8625 - precision: 0.1375 - recall: 0.9997 - f1_metric: 0.2365 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.3975 - acc: 0.8636 - precision: 0.1364 - recall: 0.9978 - f1_metric: 0.2360 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4015 - acc: 0.8586 - precision: 0.1414 - recall: 0.9998 - f1_metric: 0.2436 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3969 - acc: 0.8626 - precision: 0.1374 - recall: 1.0000 - f1_metric: 0.2364 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 946us/step - loss: 0.3946 - acc: 0.8636 - precision: 0.1365 - recall: 0.9789 - f1_metric: 0.2350 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.3982 - acc: 0.8615 - precision: 0.1385 - recall: 0.9991 - f1_metric: 0.2378 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 859us/step - loss: 0.3944 - acc: 0.8630 - precision: 0.1370 - recall: 0.9981 - f1_metric: 0.2370 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.4019 - acc: 0.8599 - precision: 0.1402 - recall: 1.0000 - f1_metric: 0.2411 - val_loss: 0.3913 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 929us/step - loss: 0.3985 - acc: 0.8601 - precision: 0.1399 - recall: 0.9963 - f1_metric: 0.2412 - val_loss: 0.3896 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 862us/step - loss: 0.3899 - acc: 0.8641 - precision: 0.1359 - recall: 0.9964 - f1_metric: 0.2343 - val_loss: 0.3886 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3855 - acc: 0.8670 - precision: 0.1330 - recall: 0.9562 - f1_metric: 0.2290 - val_loss: 0.3889 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3894 - acc: 0.8641 - precision: 0.1359 - recall: 1.0000 - f1_metric: 0.2346 - val_loss: 0.3876 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.3827 - acc: 0.8665 - precision: 0.1335 - recall: 0.9978 - f1_metric: 0.2303 - val_loss: 0.3864 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3857 - acc: 0.8655 - precision: 0.1345 - recall: 0.9811 - f1_metric: 0.2322 - val_loss: 0.3868 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.3943 - acc: 0.8586 - precision: 0.1414 - recall: 0.9992 - f1_metric: 0.2431 - val_loss: 0.3846 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3746 - acc: 0.8722 - precision: 0.1278 - recall: 1.0000 - f1_metric: 0.2226 - val_loss: 0.3819 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.3818 - acc: 0.8646 - precision: 0.1354 - recall: 0.9919 - f1_metric: 0.2334 - val_loss: 0.3792 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.3914 - acc: 0.8583 - precision: 0.1416 - recall: 0.9859 - f1_metric: 0.2427 - val_loss: 0.3773 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3783 - acc: 0.8660 - precision: 0.1341 - recall: 0.9969 - f1_metric: 0.2317 - val_loss: 0.3769 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 852us/step - loss: 0.3808 - acc: 0.8617 - precision: 0.1383 - recall: 1.0000 - f1_metric: 0.2381 - val_loss: 0.3730 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 21 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5097 - acc: 0.7775 - precision: 0.1364 - recall: 1.0147 - f1_metric: 0.2363 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3998 - acc: 0.8647 - precision: 0.1355 - recall: 0.9749 - f1_metric: 0.2344 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.4064 - acc: 0.8629 - precision: 0.1363 - recall: 0.9909 - f1_metric: 0.2350 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.4188 - acc: 0.8555 - precision: 0.1442 - recall: 0.9800 - f1_metric: 0.2461 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3982 - acc: 0.8647 - precision: 0.1362 - recall: 0.9909 - f1_metric: 0.2349 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3877 - acc: 0.8711 - precision: 0.1297 - recall: 0.9908 - f1_metric: 0.2243 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.4070 - acc: 0.8584 - precision: 0.1415 - recall: 0.9887 - f1_metric: 0.2420 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4037 - acc: 0.8593 - precision: 0.1407 - recall: 0.9866 - f1_metric: 0.2419 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3908 - acc: 0.8689 - precision: 0.1313 - recall: 0.9838 - f1_metric: 0.2262 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4010 - acc: 0.8610 - precision: 0.1393 - recall: 0.9928 - f1_metric: 0.2399 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4117 - acc: 0.8565 - precision: 0.1433 - recall: 0.9953 - f1_metric: 0.2455 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3900 - acc: 0.8682 - precision: 0.1318 - recall: 0.9964 - f1_metric: 0.2262 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4029 - acc: 0.8604 - precision: 0.1396 - recall: 0.9959 - f1_metric: 0.2401 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3872 - acc: 0.8671 - precision: 0.1329 - recall: 0.9965 - f1_metric: 0.2310 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3955 - acc: 0.8620 - precision: 0.1381 - recall: 0.9984 - f1_metric: 0.2380 - val_loss: 0.3910 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 915us/step - loss: 0.3919 - acc: 0.8649 - precision: 0.1351 - recall: 0.9874 - f1_metric: 0.2330 - val_loss: 0.3905 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 923us/step - loss: 0.3860 - acc: 0.8671 - precision: 0.1329 - recall: 0.9872 - f1_metric: 0.2304 - val_loss: 0.3958 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3841 - acc: 0.8689 - precision: 0.1312 - recall: 0.9703 - f1_metric: 0.2253 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 911us/step - loss: 0.3787 - acc: 0.8709 - precision: 0.1291 - recall: 0.9858 - f1_metric: 0.2231 - val_loss: 0.3902 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 859us/step - loss: 0.3959 - acc: 0.8601 - precision: 0.1399 - recall: 0.9857 - f1_metric: 0.2412 - val_loss: 0.3897 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 903us/step - loss: 0.3906 - acc: 0.8625 - precision: 0.1375 - recall: 1.0000 - f1_metric: 0.2380 - val_loss: 0.3880 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3881 - acc: 0.8644 - precision: 0.1356 - recall: 1.0000 - f1_metric: 0.2341 - val_loss: 0.3873 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.3871 - acc: 0.8633 - precision: 0.1367 - recall: 0.9743 - f1_metric: 0.2345 - val_loss: 0.3865 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3983 - acc: 0.8570 - precision: 0.1430 - recall: 0.9881 - f1_metric: 0.2463 - val_loss: 0.3862 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 937us/step - loss: 0.3827 - acc: 0.8657 - precision: 0.1343 - recall: 0.9980 - f1_metric: 0.2318 - val_loss: 0.3840 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.3764 - acc: 0.8676 - precision: 0.1324 - recall: 0.9505 - f1_metric: 0.2289 - val_loss: 0.3815 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.3838 - acc: 0.8652 - precision: 0.1348 - recall: 0.9906 - f1_metric: 0.2329 - val_loss: 0.3806 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3913 - acc: 0.8575 - precision: 0.1425 - recall: 0.9929 - f1_metric: 0.2451 - val_loss: 0.3788 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.3760 - acc: 0.8652 - precision: 0.1348 - recall: 1.0000 - f1_metric: 0.2334 - val_loss: 0.3778 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3832 - acc: 0.8630 - precision: 0.1370 - recall: 0.9963 - f1_metric: 0.2370 - val_loss: 0.3759 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 22 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4731 - acc: 0.8281 - precision: 0.1409 - recall: 0.9388 - f1_metric: 0.2387 - val_loss: 0.3948 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.3976 - acc: 0.8684 - precision: 0.1316 - recall: 1.0000 - f1_metric: 0.2273 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4014 - acc: 0.8633 - precision: 0.1367 - recall: 0.9950 - f1_metric: 0.2358 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3941 - acc: 0.8664 - precision: 0.1336 - recall: 0.9632 - f1_metric: 0.2302 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 862us/step - loss: 0.3925 - acc: 0.8685 - precision: 0.1315 - recall: 0.9984 - f1_metric: 0.2279 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4029 - acc: 0.8599 - precision: 0.1401 - recall: 0.9795 - f1_metric: 0.2402 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4011 - acc: 0.8603 - precision: 0.1397 - recall: 0.9867 - f1_metric: 0.2404 - val_loss: 0.3946 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3907 - acc: 0.8665 - precision: 0.1335 - recall: 0.9843 - f1_metric: 0.2303 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3892 - acc: 0.8667 - precision: 0.1333 - recall: 0.9995 - f1_metric: 0.2312 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3894 - acc: 0.8670 - precision: 0.1330 - recall: 0.9749 - f1_metric: 0.2295 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4059 - acc: 0.8573 - precision: 0.1427 - recall: 0.9832 - f1_metric: 0.2450 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3973 - acc: 0.8612 - precision: 0.1388 - recall: 0.9977 - f1_metric: 0.2397 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 959us/step - loss: 0.3988 - acc: 0.8621 - precision: 0.1378 - recall: 1.0000 - f1_metric: 0.2379 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.3889 - acc: 0.8663 - precision: 0.1337 - recall: 0.9809 - f1_metric: 0.2301 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4062 - acc: 0.8557 - precision: 0.1443 - recall: 0.9858 - f1_metric: 0.2459 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.3951 - acc: 0.8620 - precision: 0.1380 - recall: 0.9958 - f1_metric: 0.2389 - val_loss: 0.3902 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.4025 - acc: 0.8584 - precision: 0.1416 - recall: 0.9958 - f1_metric: 0.2430 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 925us/step - loss: 0.3953 - acc: 0.8626 - precision: 0.1374 - recall: 0.9848 - f1_metric: 0.2360 - val_loss: 0.3893 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3824 - acc: 0.8689 - precision: 0.1311 - recall: 1.0000 - f1_metric: 0.2269 - val_loss: 0.3891 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 917us/step - loss: 0.3822 - acc: 0.8677 - precision: 0.1323 - recall: 0.9889 - f1_metric: 0.2287 - val_loss: 0.3877 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3949 - acc: 0.8610 - precision: 0.1390 - recall: 1.0000 - f1_metric: 0.2396 - val_loss: 0.3886 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 905us/step - loss: 0.3894 - acc: 0.8659 - precision: 0.1341 - recall: 1.0000 - f1_metric: 0.2323 - val_loss: 0.3866 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3848 - acc: 0.8653 - precision: 0.1347 - recall: 0.9881 - f1_metric: 0.2321 - val_loss: 0.3859 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 851us/step - loss: 0.3841 - acc: 0.8656 - precision: 0.1344 - recall: 0.9927 - f1_metric: 0.2327 - val_loss: 0.3827 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3773 - acc: 0.8671 - precision: 0.1329 - recall: 0.9901 - f1_metric: 0.2304 - val_loss: 0.3829 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.3913 - acc: 0.8611 - precision: 0.1389 - recall: 1.0000 - f1_metric: 0.2383 - val_loss: 0.3800 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3696 - acc: 0.8704 - precision: 0.1296 - recall: 0.9915 - f1_metric: 0.2251 - val_loss: 0.3768 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3750 - acc: 0.8662 - precision: 0.1338 - recall: 0.9760 - f1_metric: 0.2313 - val_loss: 0.3755 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3793 - acc: 0.8641 - precision: 0.1359 - recall: 0.9902 - f1_metric: 0.2346 - val_loss: 0.3729 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3726 - acc: 0.8639 - precision: 0.1361 - recall: 0.9978 - f1_metric: 0.2345 - val_loss: 0.3716 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 23 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.6092 - acc: 0.6596 - precision: 0.1342 - recall: 0.4412 - f1_metric: 0.1732 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4051 - acc: 0.8620 - precision: 0.1367 - recall: 0.9150 - f1_metric: 0.2324 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3911 - acc: 0.8699 - precision: 0.1296 - recall: 0.9734 - f1_metric: 0.2247 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3990 - acc: 0.8639 - precision: 0.1358 - recall: 0.9892 - f1_metric: 0.2344 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4114 - acc: 0.8584 - precision: 0.1413 - recall: 0.9832 - f1_metric: 0.2424 - val_loss: 0.3953 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.4082 - acc: 0.8577 - precision: 0.1428 - recall: 0.9979 - f1_metric: 0.2446 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.3828 - acc: 0.8708 - precision: 0.1292 - recall: 0.9925 - f1_metric: 0.2245 - val_loss: 0.3954 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4016 - acc: 0.8624 - precision: 0.1377 - recall: 0.9970 - f1_metric: 0.2379 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3959 - acc: 0.8645 - precision: 0.1355 - recall: 0.9752 - f1_metric: 0.2314 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3855 - acc: 0.8696 - precision: 0.1304 - recall: 0.9991 - f1_metric: 0.2262 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3922 - acc: 0.8650 - precision: 0.1350 - recall: 0.9972 - f1_metric: 0.2336 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4013 - acc: 0.8590 - precision: 0.1410 - recall: 0.9928 - f1_metric: 0.2421 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3959 - acc: 0.8640 - precision: 0.1361 - recall: 0.9845 - f1_metric: 0.2347 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3891 - acc: 0.8651 - precision: 0.1349 - recall: 0.9929 - f1_metric: 0.2332 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 924us/step - loss: 0.3902 - acc: 0.8654 - precision: 0.1346 - recall: 0.9979 - f1_metric: 0.2320 - val_loss: 0.3913 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4110 - acc: 0.8535 - precision: 0.1465 - recall: 0.9920 - f1_metric: 0.2499 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 912us/step - loss: 0.3944 - acc: 0.8632 - precision: 0.1368 - recall: 1.0000 - f1_metric: 0.2358 - val_loss: 0.3909 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 917us/step - loss: 0.3813 - acc: 0.8703 - precision: 0.1297 - recall: 0.9962 - f1_metric: 0.2253 - val_loss: 0.3908 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3798 - acc: 0.8709 - precision: 0.1291 - recall: 1.0000 - f1_metric: 0.2244 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.3954 - acc: 0.8621 - precision: 0.1379 - recall: 0.9977 - f1_metric: 0.2369 - val_loss: 0.3908 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3837 - acc: 0.8686 - precision: 0.1314 - recall: 1.0000 - f1_metric: 0.2279 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3943 - acc: 0.8613 - precision: 0.1387 - recall: 0.9990 - f1_metric: 0.2397 - val_loss: 0.3901 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 912us/step - loss: 0.3906 - acc: 0.8639 - precision: 0.1361 - recall: 0.9865 - f1_metric: 0.2354 - val_loss: 0.3883 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.3905 - acc: 0.8636 - precision: 0.1364 - recall: 0.9883 - f1_metric: 0.2350 - val_loss: 0.3873 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.3875 - acc: 0.8629 - precision: 0.1371 - recall: 0.9995 - f1_metric: 0.2363 - val_loss: 0.3858 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.3928 - acc: 0.8615 - precision: 0.1385 - recall: 0.9995 - f1_metric: 0.2372 - val_loss: 0.3854 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.3783 - acc: 0.8686 - precision: 0.1314 - recall: 0.9951 - f1_metric: 0.2281 - val_loss: 0.3867 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3840 - acc: 0.8640 - precision: 0.1360 - recall: 1.0000 - f1_metric: 0.2344 - val_loss: 0.3822 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.3836 - acc: 0.8618 - precision: 0.1382 - recall: 0.9849 - f1_metric: 0.2379 - val_loss: 0.3821 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.3726 - acc: 0.8695 - precision: 0.1305 - recall: 0.9912 - f1_metric: 0.2264 - val_loss: 0.3782 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 24 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5055 - acc: 0.8272 - precision: 0.1323 - recall: 0.8922 - f1_metric: 0.2233 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4112 - acc: 0.8595 - precision: 0.1403 - recall: 0.9605 - f1_metric: 0.2397 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.4105 - acc: 0.8572 - precision: 0.1432 - recall: 0.9986 - f1_metric: 0.2448 - val_loss: 0.3948 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.4004 - acc: 0.8628 - precision: 0.1366 - recall: 0.9846 - f1_metric: 0.2353 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.4109 - acc: 0.8561 - precision: 0.1442 - recall: 0.9816 - f1_metric: 0.2465 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.4111 - acc: 0.8562 - precision: 0.1433 - recall: 0.9751 - f1_metric: 0.2448 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4100 - acc: 0.8560 - precision: 0.1436 - recall: 0.9757 - f1_metric: 0.2461 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4053 - acc: 0.8598 - precision: 0.1398 - recall: 0.9627 - f1_metric: 0.2386 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.4084 - acc: 0.8572 - precision: 0.1414 - recall: 0.9546 - f1_metric: 0.2421 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 912us/step - loss: 0.3969 - acc: 0.8636 - precision: 0.1359 - recall: 0.9620 - f1_metric: 0.2333 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.3960 - acc: 0.8652 - precision: 0.1352 - recall: 0.9840 - f1_metric: 0.2324 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.4106 - acc: 0.8576 - precision: 0.1434 - recall: 0.9820 - f1_metric: 0.2457 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3996 - acc: 0.8621 - precision: 0.1388 - recall: 0.9852 - f1_metric: 0.2390 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 920us/step - loss: 0.4038 - acc: 0.8579 - precision: 0.1415 - recall: 0.9808 - f1_metric: 0.2420 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.3970 - acc: 0.8624 - precision: 0.1368 - recall: 0.9640 - f1_metric: 0.2348 - val_loss: 0.3914 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.4094 - acc: 0.8539 - precision: 0.1462 - recall: 0.9660 - f1_metric: 0.2495 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3916 - acc: 0.8658 - precision: 0.1345 - recall: 0.9909 - f1_metric: 0.2316 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.3979 - acc: 0.8628 - precision: 0.1373 - recall: 0.9714 - f1_metric: 0.2355 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.3956 - acc: 0.8613 - precision: 0.1389 - recall: 0.9603 - f1_metric: 0.2391 - val_loss: 0.3909 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3796 - acc: 0.8693 - precision: 0.1313 - recall: 0.9799 - f1_metric: 0.2263 - val_loss: 0.3900 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 916us/step - loss: 0.3882 - acc: 0.8672 - precision: 0.1326 - recall: 0.9822 - f1_metric: 0.2293 - val_loss: 0.3897 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.3992 - acc: 0.8599 - precision: 0.1405 - recall: 0.9856 - f1_metric: 0.2410 - val_loss: 0.3910 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 943us/step - loss: 0.3873 - acc: 0.8651 - precision: 0.1350 - recall: 0.9885 - f1_metric: 0.2339 - val_loss: 0.3908 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 922us/step - loss: 0.3923 - acc: 0.8638 - precision: 0.1364 - recall: 0.9796 - f1_metric: 0.2354 - val_loss: 0.3909 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3856 - acc: 0.8673 - precision: 0.1331 - recall: 0.9952 - f1_metric: 0.2295 - val_loss: 0.3898 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.3861 - acc: 0.8655 - precision: 0.1346 - recall: 0.9862 - f1_metric: 0.2327 - val_loss: 0.3850 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.3972 - acc: 0.8575 - precision: 0.1425 - recall: 0.9880 - f1_metric: 0.2440 - val_loss: 0.3838 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.3941 - acc: 0.8588 - precision: 0.1412 - recall: 0.9954 - f1_metric: 0.2419 - val_loss: 0.3818 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3809 - acc: 0.8644 - precision: 0.1356 - recall: 0.9967 - f1_metric: 0.2342 - val_loss: 0.3800 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3802 - acc: 0.8662 - precision: 0.1338 - recall: 0.9649 - f1_metric: 0.2297 - val_loss: 0.3772 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 25 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5552 - acc: 0.7178 - precision: 0.1121 - recall: 0.3927 - f1_metric: 0.1519 - val_loss: 0.3949 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3959 - acc: 0.8669 - precision: 0.1273 - recall: 0.7772 - f1_metric: 0.2140 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.4011 - acc: 0.8628 - precision: 0.1339 - recall: 0.8747 - f1_metric: 0.2270 - val_loss: 0.3957 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.4100 - acc: 0.8580 - precision: 0.1416 - recall: 0.8650 - f1_metric: 0.2380 - val_loss: 0.3940 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.4033 - acc: 0.8632 - precision: 0.1383 - recall: 0.9064 - f1_metric: 0.2354 - val_loss: 0.3946 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 863us/step - loss: 0.3934 - acc: 0.8654 - precision: 0.1351 - recall: 0.9013 - f1_metric: 0.2302 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3865 - acc: 0.8696 - precision: 0.1331 - recall: 0.9107 - f1_metric: 0.2283 - val_loss: 0.3993 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.3929 - acc: 0.8648 - precision: 0.1313 - recall: 0.8596 - f1_metric: 0.2225 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 913us/step - loss: 0.3907 - acc: 0.8676 - precision: 0.1292 - recall: 0.8718 - f1_metric: 0.2202 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.4002 - acc: 0.8608 - precision: 0.1396 - recall: 0.8792 - f1_metric: 0.2343 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.4017 - acc: 0.8595 - precision: 0.1366 - recall: 0.8775 - f1_metric: 0.2316 - val_loss: 0.3926 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3989 - acc: 0.8648 - precision: 0.1364 - recall: 0.9363 - f1_metric: 0.2327 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3888 - acc: 0.8673 - precision: 0.1321 - recall: 0.9150 - f1_metric: 0.2268 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.4064 - acc: 0.8574 - precision: 0.1419 - recall: 0.9142 - f1_metric: 0.2416 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.3975 - acc: 0.8613 - precision: 0.1409 - recall: 0.9481 - f1_metric: 0.2402 - val_loss: 0.3913 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.3973 - acc: 0.8618 - precision: 0.1359 - recall: 0.9168 - f1_metric: 0.2319 - val_loss: 0.3909 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3935 - acc: 0.8632 - precision: 0.1360 - recall: 0.9419 - f1_metric: 0.2327 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3954 - acc: 0.8631 - precision: 0.1391 - recall: 0.9770 - f1_metric: 0.2385 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.3930 - acc: 0.8659 - precision: 0.1337 - recall: 0.9564 - f1_metric: 0.2306 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.3985 - acc: 0.8615 - precision: 0.1393 - recall: 0.9842 - f1_metric: 0.2391 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 926us/step - loss: 0.3913 - acc: 0.8654 - precision: 0.1354 - recall: 0.9989 - f1_metric: 0.2338 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3848 - acc: 0.8668 - precision: 0.1335 - recall: 0.9696 - f1_metric: 0.2307 - val_loss: 0.3884 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3965 - acc: 0.8615 - precision: 0.1385 - recall: 0.9905 - f1_metric: 0.2392 - val_loss: 0.3884 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4016 - acc: 0.8573 - precision: 0.1427 - recall: 0.9977 - f1_metric: 0.2459 - val_loss: 0.3866 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 922us/step - loss: 0.3952 - acc: 0.8606 - precision: 0.1394 - recall: 0.9888 - f1_metric: 0.2397 - val_loss: 0.3862 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 859us/step - loss: 0.3835 - acc: 0.8678 - precision: 0.1322 - recall: 0.9994 - f1_metric: 0.2291 - val_loss: 0.3846 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3919 - acc: 0.8600 - precision: 0.1400 - recall: 0.9855 - f1_metric: 0.2413 - val_loss: 0.3832 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.3748 - acc: 0.8681 - precision: 0.1319 - recall: 0.9567 - f1_metric: 0.2269 - val_loss: 0.3812 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.3779 - acc: 0.8683 - precision: 0.1317 - recall: 0.9747 - f1_metric: 0.2276 - val_loss: 0.3796 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 933us/step - loss: 0.3861 - acc: 0.8614 - precision: 0.1386 - recall: 0.9721 - f1_metric: 0.2378 - val_loss: 0.3763 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 26 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4853 - acc: 0.8413 - precision: 0.1377 - recall: 0.9633 - f1_metric: 0.2364 - val_loss: 0.3953 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.3994 - acc: 0.8627 - precision: 0.1373 - recall: 1.0000 - f1_metric: 0.2367 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 860us/step - loss: 0.3969 - acc: 0.8661 - precision: 0.1339 - recall: 0.9770 - f1_metric: 0.2302 - val_loss: 0.3950 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4107 - acc: 0.8552 - precision: 0.1448 - recall: 0.9880 - f1_metric: 0.2472 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.3975 - acc: 0.8633 - precision: 0.1367 - recall: 0.9907 - f1_metric: 0.2355 - val_loss: 0.3933 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3993 - acc: 0.8595 - precision: 0.1405 - recall: 0.9901 - f1_metric: 0.2416 - val_loss: 0.3949 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4063 - acc: 0.8591 - precision: 0.1409 - recall: 0.9980 - f1_metric: 0.2416 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3911 - acc: 0.8650 - precision: 0.1350 - recall: 0.9655 - f1_metric: 0.2332 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.3982 - acc: 0.8631 - precision: 0.1369 - recall: 1.0000 - f1_metric: 0.2354 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3950 - acc: 0.8628 - precision: 0.1372 - recall: 0.9959 - f1_metric: 0.2360 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3908 - acc: 0.8642 - precision: 0.1358 - recall: 0.9945 - f1_metric: 0.2346 - val_loss: 0.3918 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.3965 - acc: 0.8613 - precision: 0.1387 - recall: 0.9728 - f1_metric: 0.2386 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3918 - acc: 0.8662 - precision: 0.1338 - recall: 0.9957 - f1_metric: 0.2318 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.3821 - acc: 0.8704 - precision: 0.1296 - recall: 0.9737 - f1_metric: 0.2241 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.3954 - acc: 0.8623 - precision: 0.1377 - recall: 0.9976 - f1_metric: 0.2371 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.3814 - acc: 0.8702 - precision: 0.1298 - recall: 0.9936 - f1_metric: 0.2255 - val_loss: 0.3980 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 863us/step - loss: 0.3858 - acc: 0.8692 - precision: 0.1308 - recall: 0.9925 - f1_metric: 0.2266 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 940us/step - loss: 0.3852 - acc: 0.8675 - precision: 0.1325 - recall: 0.9837 - f1_metric: 0.2290 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3995 - acc: 0.8598 - precision: 0.1402 - recall: 0.9961 - f1_metric: 0.2407 - val_loss: 0.3891 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 924us/step - loss: 0.4004 - acc: 0.8586 - precision: 0.1414 - recall: 0.9926 - f1_metric: 0.2429 - val_loss: 0.3889 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 921us/step - loss: 0.3953 - acc: 0.8612 - precision: 0.1388 - recall: 0.9853 - f1_metric: 0.2386 - val_loss: 0.3878 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 943us/step - loss: 0.3987 - acc: 0.8583 - precision: 0.1417 - recall: 0.9925 - f1_metric: 0.2439 - val_loss: 0.3867 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 936us/step - loss: 0.3918 - acc: 0.8617 - precision: 0.1383 - recall: 0.9987 - f1_metric: 0.2385 - val_loss: 0.3859 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3790 - acc: 0.8686 - precision: 0.1314 - recall: 0.9975 - f1_metric: 0.2277 - val_loss: 0.3848 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 933us/step - loss: 0.4018 - acc: 0.8557 - precision: 0.1443 - recall: 0.9889 - f1_metric: 0.2475 - val_loss: 0.3861 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 905us/step - loss: 0.3798 - acc: 0.8661 - precision: 0.1339 - recall: 0.9915 - f1_metric: 0.2301 - val_loss: 0.3809 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3839 - acc: 0.8616 - precision: 0.1384 - recall: 0.9815 - f1_metric: 0.2379 - val_loss: 0.3787 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.3897 - acc: 0.8600 - precision: 0.1400 - recall: 1.0000 - f1_metric: 0.2399 - val_loss: 0.3770 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.3781 - acc: 0.8636 - precision: 0.1364 - recall: 0.9999 - f1_metric: 0.2355 - val_loss: 0.3738 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.3765 - acc: 0.8646 - precision: 0.1354 - recall: 0.9890 - f1_metric: 0.2334 - val_loss: 0.3773 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 27 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5324 - acc: 0.7789 - precision: 0.1363 - recall: 1.0893 - f1_metric: 0.2379 - val_loss: 0.3946 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4099 - acc: 0.8581 - precision: 0.1420 - recall: 0.9993 - f1_metric: 0.2441 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4040 - acc: 0.8624 - precision: 0.1378 - recall: 0.9988 - f1_metric: 0.2385 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3855 - acc: 0.8715 - precision: 0.1285 - recall: 0.9982 - f1_metric: 0.2230 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.4005 - acc: 0.8621 - precision: 0.1377 - recall: 0.9786 - f1_metric: 0.2373 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.4019 - acc: 0.8610 - precision: 0.1390 - recall: 0.9953 - f1_metric: 0.2384 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.3997 - acc: 0.8622 - precision: 0.1377 - recall: 0.9856 - f1_metric: 0.2375 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3877 - acc: 0.8687 - precision: 0.1313 - recall: 0.9780 - f1_metric: 0.2274 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4002 - acc: 0.8611 - precision: 0.1384 - recall: 0.9966 - f1_metric: 0.2392 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.4001 - acc: 0.8620 - precision: 0.1380 - recall: 0.9919 - f1_metric: 0.2384 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.3861 - acc: 0.8677 - precision: 0.1325 - recall: 0.9873 - f1_metric: 0.2297 - val_loss: 0.3925 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4009 - acc: 0.8581 - precision: 0.1417 - recall: 0.9871 - f1_metric: 0.2426 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3956 - acc: 0.8633 - precision: 0.1369 - recall: 1.0014 - f1_metric: 0.2359 - val_loss: 0.3920 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.3903 - acc: 0.8661 - precision: 0.1342 - recall: 0.9831 - f1_metric: 0.2309 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.4059 - acc: 0.8556 - precision: 0.1456 - recall: 1.0066 - f1_metric: 0.2495 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.4091 - acc: 0.8533 - precision: 0.1476 - recall: 1.0029 - f1_metric: 0.2523 - val_loss: 0.3911 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 923us/step - loss: 0.3824 - acc: 0.8684 - precision: 0.1322 - recall: 1.0123 - f1_metric: 0.2299 - val_loss: 0.3976 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.4005 - acc: 0.8588 - precision: 0.1411 - recall: 0.9962 - f1_metric: 0.2423 - val_loss: 0.3896 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3936 - acc: 0.8625 - precision: 0.1375 - recall: 0.9848 - f1_metric: 0.2360 - val_loss: 0.3899 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.4037 - acc: 0.8561 - precision: 0.1439 - recall: 0.9924 - f1_metric: 0.2474 - val_loss: 0.3891 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 918us/step - loss: 0.4045 - acc: 0.8590 - precision: 0.1410 - recall: 0.9918 - f1_metric: 0.2416 - val_loss: 0.3898 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3956 - acc: 0.8598 - precision: 0.1402 - recall: 0.9988 - f1_metric: 0.2418 - val_loss: 0.3879 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.3905 - acc: 0.8618 - precision: 0.1382 - recall: 0.9848 - f1_metric: 0.2375 - val_loss: 0.3904 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.3852 - acc: 0.8657 - precision: 0.1343 - recall: 0.9780 - f1_metric: 0.2316 - val_loss: 0.3867 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4015 - acc: 0.8565 - precision: 0.1435 - recall: 0.9943 - f1_metric: 0.2467 - val_loss: 0.3855 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3861 - acc: 0.8646 - precision: 0.1354 - recall: 0.9935 - f1_metric: 0.2336 - val_loss: 0.3830 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 897us/step - loss: 0.4053 - acc: 0.8557 - precision: 0.1443 - recall: 1.0000 - f1_metric: 0.2476 - val_loss: 0.3811 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.3806 - acc: 0.8640 - precision: 0.1360 - recall: 0.9972 - f1_metric: 0.2352 - val_loss: 0.3813 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.3972 - acc: 0.8537 - precision: 0.1463 - recall: 1.0000 - f1_metric: 0.2505 - val_loss: 0.3775 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3674 - acc: 0.8697 - precision: 0.1303 - recall: 0.9915 - f1_metric: 0.2249 - val_loss: 0.3789 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 28 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.5271 - acc: 0.7608 - precision: 0.1452 - recall: 0.8523 - f1_metric: 0.2353 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.3966 - acc: 0.8661 - precision: 0.1339 - recall: 0.9951 - f1_metric: 0.2308 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3933 - acc: 0.8681 - precision: 0.1319 - recall: 0.9987 - f1_metric: 0.2279 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 931us/step - loss: 0.4129 - acc: 0.8576 - precision: 0.1425 - recall: 0.9810 - f1_metric: 0.2451 - val_loss: 0.3955 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3981 - acc: 0.8626 - precision: 0.1376 - recall: 0.9675 - f1_metric: 0.2369 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3977 - acc: 0.8642 - precision: 0.1358 - recall: 0.9871 - f1_metric: 0.2339 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 913us/step - loss: 0.3894 - acc: 0.8666 - precision: 0.1336 - recall: 0.9981 - f1_metric: 0.2306 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4032 - acc: 0.8573 - precision: 0.1432 - recall: 0.9998 - f1_metric: 0.2459 - val_loss: 0.3931 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 923us/step - loss: 0.3825 - acc: 0.8710 - precision: 0.1288 - recall: 0.9582 - f1_metric: 0.2236 - val_loss: 0.3939 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 921us/step - loss: 0.3870 - acc: 0.8709 - precision: 0.1290 - recall: 0.9821 - f1_metric: 0.2236 - val_loss: 0.3927 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.4002 - acc: 0.8612 - precision: 0.1388 - recall: 0.9989 - f1_metric: 0.2386 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3941 - acc: 0.8645 - precision: 0.1356 - recall: 1.0000 - f1_metric: 0.2342 - val_loss: 0.3932 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4014 - acc: 0.8584 - precision: 0.1420 - recall: 0.9993 - f1_metric: 0.2444 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.4106 - acc: 0.8550 - precision: 0.1449 - recall: 0.9983 - f1_metric: 0.2484 - val_loss: 0.3914 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4033 - acc: 0.8594 - precision: 0.1407 - recall: 0.9954 - f1_metric: 0.2423 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3980 - acc: 0.8619 - precision: 0.1381 - recall: 1.0000 - f1_metric: 0.2376 - val_loss: 0.3911 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4170 - acc: 0.8517 - precision: 0.1483 - recall: 1.0000 - f1_metric: 0.2543 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3996 - acc: 0.8609 - precision: 0.1391 - recall: 0.9888 - f1_metric: 0.2383 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.3898 - acc: 0.8642 - precision: 0.1358 - recall: 0.9957 - f1_metric: 0.2343 - val_loss: 0.3895 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 920us/step - loss: 0.4052 - acc: 0.8572 - precision: 0.1428 - recall: 0.9914 - f1_metric: 0.2454 - val_loss: 0.3893 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.3875 - acc: 0.8654 - precision: 0.1346 - recall: 0.9922 - f1_metric: 0.2324 - val_loss: 0.3900 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 915us/step - loss: 0.4003 - acc: 0.8587 - precision: 0.1413 - recall: 1.0000 - f1_metric: 0.2430 - val_loss: 0.3928 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 965us/step - loss: 0.3810 - acc: 0.8687 - precision: 0.1313 - recall: 0.9889 - f1_metric: 0.2267 - val_loss: 0.3888 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 922us/step - loss: 0.3856 - acc: 0.8653 - precision: 0.1347 - recall: 0.9845 - f1_metric: 0.2323 - val_loss: 0.3861 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 921us/step - loss: 0.3952 - acc: 0.8599 - precision: 0.1401 - recall: 0.9978 - f1_metric: 0.2411 - val_loss: 0.3847 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 920us/step - loss: 0.3932 - acc: 0.8602 - precision: 0.1398 - recall: 0.9931 - f1_metric: 0.2407 - val_loss: 0.3863 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 919us/step - loss: 0.3972 - acc: 0.8574 - precision: 0.1426 - recall: 1.0000 - f1_metric: 0.2451 - val_loss: 0.3826 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 915us/step - loss: 0.3724 - acc: 0.8707 - precision: 0.1293 - recall: 0.9762 - f1_metric: 0.2233 - val_loss: 0.3804 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.3783 - acc: 0.8667 - precision: 0.1333 - recall: 0.9936 - f1_metric: 0.2304 - val_loss: 0.3782 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.3853 - acc: 0.8603 - precision: 0.1397 - recall: 0.9971 - f1_metric: 0.2413 - val_loss: 0.3754 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 29 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 0.4965 - acc: 0.8346 - precision: 0.1387 - recall: 1.0028 - f1_metric: 0.2384 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 924us/step - loss: 0.4055 - acc: 0.8605 - precision: 0.1395 - recall: 0.9820 - f1_metric: 0.2387 - val_loss: 0.3936 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.4134 - acc: 0.8578 - precision: 0.1422 - recall: 1.0000 - f1_metric: 0.2447 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4149 - acc: 0.8550 - precision: 0.1450 - recall: 1.0000 - f1_metric: 0.2491 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.3878 - acc: 0.8701 - precision: 0.1299 - recall: 0.9885 - f1_metric: 0.2258 - val_loss: 0.3934 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3935 - acc: 0.8663 - precision: 0.1337 - recall: 0.9963 - f1_metric: 0.2306 - val_loss: 0.3935 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.4020 - acc: 0.8623 - precision: 0.1377 - recall: 0.9997 - f1_metric: 0.2381 - val_loss: 0.3946 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4009 - acc: 0.8637 - precision: 0.1363 - recall: 0.9832 - f1_metric: 0.2352 - val_loss: 0.3956 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3888 - acc: 0.8700 - precision: 0.1300 - recall: 0.9923 - f1_metric: 0.2250 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4062 - acc: 0.8583 - precision: 0.1417 - recall: 0.9869 - f1_metric: 0.2424 - val_loss: 0.3952 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4007 - acc: 0.8600 - precision: 0.1400 - recall: 1.0000 - f1_metric: 0.2406 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4044 - acc: 0.8593 - precision: 0.1407 - recall: 1.0000 - f1_metric: 0.2424 - val_loss: 0.3942 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.3942 - acc: 0.8632 - precision: 0.1368 - recall: 0.9922 - f1_metric: 0.2357 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3975 - acc: 0.8618 - precision: 0.1382 - recall: 1.0000 - f1_metric: 0.2389 - val_loss: 0.3912 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3934 - acc: 0.8634 - precision: 0.1366 - recall: 0.9846 - f1_metric: 0.2352 - val_loss: 0.3903 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3903 - acc: 0.8650 - precision: 0.1350 - recall: 0.9665 - f1_metric: 0.2318 - val_loss: 0.3899 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.3875 - acc: 0.8647 - precision: 0.1353 - recall: 1.0000 - f1_metric: 0.2329 - val_loss: 0.3896 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3878 - acc: 0.8665 - precision: 0.1335 - recall: 0.9954 - f1_metric: 0.2311 - val_loss: 0.3890 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3925 - acc: 0.8640 - precision: 0.1360 - recall: 0.9930 - f1_metric: 0.2336 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.3911 - acc: 0.8627 - precision: 0.1373 - recall: 0.9998 - f1_metric: 0.2372 - val_loss: 0.3872 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.3978 - acc: 0.8595 - precision: 0.1405 - recall: 0.9834 - f1_metric: 0.2413 - val_loss: 0.3868 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3809 - acc: 0.8681 - precision: 0.1319 - recall: 0.9957 - f1_metric: 0.2282 - val_loss: 0.3860 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 922us/step - loss: 0.3871 - acc: 0.8646 - precision: 0.1354 - recall: 0.9886 - f1_metric: 0.2334 - val_loss: 0.3842 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.3884 - acc: 0.8626 - precision: 0.1374 - recall: 1.0000 - f1_metric: 0.2370 - val_loss: 0.3807 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3760 - acc: 0.8677 - precision: 0.1323 - recall: 1.0000 - f1_metric: 0.2286 - val_loss: 0.3803 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 937us/step - loss: 0.3902 - acc: 0.8602 - precision: 0.1398 - recall: 1.0000 - f1_metric: 0.2393 - val_loss: 0.3766 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 936us/step - loss: 0.3927 - acc: 0.8547 - precision: 0.1453 - recall: 0.9882 - f1_metric: 0.2489 - val_loss: 0.3772 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.3688 - acc: 0.8671 - precision: 0.1329 - recall: 0.9890 - f1_metric: 0.2291 - val_loss: 0.3697 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 928us/step - loss: 0.3681 - acc: 0.8668 - precision: 0.1333 - recall: 0.9911 - f1_metric: 0.2307 - val_loss: 0.3677 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.3843 - acc: 0.8568 - precision: 0.1432 - recall: 0.9921 - f1_metric: 0.2463 - val_loss: 0.3634 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Model 30 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 0.4861 - acc: 0.8338 - precision: 0.1387 - recall: 1.0433 - f1_metric: 0.2399 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4072 - acc: 0.8608 - precision: 0.1392 - recall: 0.9946 - f1_metric: 0.2385 - val_loss: 0.3959 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4057 - acc: 0.8595 - precision: 0.1403 - recall: 0.9896 - f1_metric: 0.2408 - val_loss: 0.3943 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.4041 - acc: 0.8611 - precision: 0.1389 - recall: 0.9937 - f1_metric: 0.2392 - val_loss: 0.3938 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3904 - acc: 0.8681 - precision: 0.1319 - recall: 0.9824 - f1_metric: 0.2276 - val_loss: 0.3941 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4054 - acc: 0.8586 - precision: 0.1414 - recall: 0.9992 - f1_metric: 0.2423 - val_loss: 0.3944 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.4035 - acc: 0.8613 - precision: 0.1387 - recall: 0.9993 - f1_metric: 0.2387 - val_loss: 0.3930 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.3884 - acc: 0.8682 - precision: 0.1318 - recall: 0.9945 - f1_metric: 0.2288 - val_loss: 0.3929 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4070 - acc: 0.8562 - precision: 0.1438 - recall: 0.9940 - f1_metric: 0.2460 - val_loss: 0.3923 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.3917 - acc: 0.8647 - precision: 0.1353 - recall: 0.9970 - f1_metric: 0.2338 - val_loss: 0.3943 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3898 - acc: 0.8663 - precision: 0.1337 - recall: 1.0000 - f1_metric: 0.2299 - val_loss: 0.3937 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.3990 - acc: 0.8616 - precision: 0.1384 - recall: 1.0000 - f1_metric: 0.2390 - val_loss: 0.3922 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.3829 - acc: 0.8691 - precision: 0.1309 - recall: 0.9934 - f1_metric: 0.2269 - val_loss: 0.3919 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.3900 - acc: 0.8658 - precision: 0.1342 - recall: 0.9949 - f1_metric: 0.2320 - val_loss: 0.3916 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.4023 - acc: 0.8602 - precision: 0.1398 - recall: 0.9952 - f1_metric: 0.2392 - val_loss: 0.3924 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.3982 - acc: 0.8613 - precision: 0.1388 - recall: 0.9977 - f1_metric: 0.2388 - val_loss: 0.3917 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.3910 - acc: 0.8661 - precision: 0.1339 - recall: 1.0000 - f1_metric: 0.2321 - val_loss: 0.3913 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.3938 - acc: 0.8635 - precision: 0.1365 - recall: 0.9997 - f1_metric: 0.2356 - val_loss: 0.3915 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.3901 - acc: 0.8657 - precision: 0.1343 - recall: 0.9986 - f1_metric: 0.2323 - val_loss: 0.3921 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.3848 - acc: 0.8678 - precision: 0.1322 - recall: 1.0000 - f1_metric: 0.2292 - val_loss: 0.3906 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3855 - acc: 0.8659 - precision: 0.1341 - recall: 0.9999 - f1_metric: 0.2312 - val_loss: 0.3895 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.4002 - acc: 0.8593 - precision: 0.1407 - recall: 0.9984 - f1_metric: 0.2423 - val_loss: 0.3907 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.3967 - acc: 0.8600 - precision: 0.1400 - recall: 0.9993 - f1_metric: 0.2401 - val_loss: 0.3880 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.4023 - acc: 0.8568 - precision: 0.1432 - recall: 0.9914 - f1_metric: 0.2461 - val_loss: 0.3861 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.3945 - acc: 0.8609 - precision: 0.1391 - recall: 1.0000 - f1_metric: 0.2388 - val_loss: 0.3854 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.3758 - acc: 0.8695 - precision: 0.1305 - recall: 1.0000 - f1_metric: 0.2262 - val_loss: 0.3835 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.3921 - acc: 0.8621 - precision: 0.1379 - recall: 0.9953 - f1_metric: 0.2363 - val_loss: 0.3838 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.3854 - acc: 0.8637 - precision: 0.1363 - recall: 0.9960 - f1_metric: 0.2355 - val_loss: 0.3808 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 981us/step - loss: 0.3865 - acc: 0.8620 - precision: 0.1380 - recall: 0.9964 - f1_metric: 0.2379 - val_loss: 0.3785 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 0s 928us/step - loss: 0.3739 - acc: 0.8653 - precision: 0.1347 - recall: 1.0000 - f1_metric: 0.2330 - val_loss: 0.3761 - val_acc: 0.8654 - val_precision: 0.1322 - val_recall: 0.9808 - val_f1_metric: 0.2296\n"
     ]
    }
   ],
   "source": [
    "#Run the model\n",
    "\n",
    "print(\"Model 1 Fitting\")\n",
    "history1 = model1.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), class_weight = weights, callbacks=[tensorboard_callback])\n",
    "print(\"Model 2 Fitting\")\n",
    "history2 = model2.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 3 Fitting\")\n",
    "history3 = model3.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 4 Fitting\")\n",
    "history4 = model4.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 5 Fitting\")\n",
    "history5 = model5.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 6 Fitting\")\n",
    "history6 = model6.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 7 Fitting\")\n",
    "history7 = model7.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 8 Fitting\")\n",
    "history8 = model8.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 9 Fitting\")\n",
    "history9 = model9.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 10 Fitting\")\n",
    "history10 = model10.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 11 Fitting\")\n",
    "history11 = model11.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 12 Fitting\")\n",
    "history12 = model12.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 13 Fitting\")\n",
    "history13 = model13.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 14 Fitting\")\n",
    "history14 = model14.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 15 Fitting\")\n",
    "history15 = model15.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 16 Fitting\")\n",
    "history16 = model16.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 17 Fitting\")\n",
    "history17 = model17.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 18 Fitting\")\n",
    "history18 = model18.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 19 Fitting\")\n",
    "history19 = model19.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 20 Fitting\")\n",
    "history20 = model20.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 21 Fitting\")\n",
    "history21 = model21.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 22 Fitting\")\n",
    "history22 = model22.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 23 Fitting\")\n",
    "history23 = model23.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 24 Fitting\")\n",
    "history24 = model24.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 25 Fitting\")\n",
    "history25 = model25.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 26 Fitting\")\n",
    "history26 = model26.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 27 Fitting\")\n",
    "history27 = model27.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 28 Fitting\")\n",
    "history28 = model28.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 29 Fitting\")\n",
    "history29 = model29.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 30 Fitting\")\n",
    "history30 = model30.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GfHp9ww0FWvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 705us/step - loss: 0.6474 - acc: 0.5903 - precision: 0.1612 - recall: 0.3443 - f1_metric: 0.2079\n",
      "[0.6474183797836304, 0.5903083682060242, 0.1612202227115631, 0.34430813789367676, 0.20794248580932617]\n",
      "64/64 [==============================] - 0s 679us/step - loss: 0.3945 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.6474183797836304, 0.5903083682060242, 0.1612202227115631, 0.34430813789367676, 0.20794248580932617]\n",
      "64/64 [==============================] - 0s 610us/step - loss: 0.4031 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.6474183797836304, 0.5903083682060242, 0.1612202227115631, 0.34430813789367676, 0.20794248580932617]\n",
      "64/64 [==============================] - 0s 653us/step - loss: 0.4013 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4013214409351349, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 594us/step - loss: 0.4035 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4035281240940094, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 642us/step - loss: 0.4017 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.40169012546539307, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 602us/step - loss: 0.3946 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.39462390542030334, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 637us/step - loss: 0.4176 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4176245331764221, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 696us/step - loss: 0.3960 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.3960147798061371, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 592us/step - loss: 0.4065 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.40649229288101196, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 630us/step - loss: 0.4122 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4122418761253357, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 654us/step - loss: 0.4058 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4058326780796051, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 550us/step - loss: 0.4057 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4057213068008423, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 609us/step - loss: 0.4082 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4082394540309906, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 564us/step - loss: 0.4058 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4057694673538208, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 655us/step - loss: 0.3993 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.39928361773490906, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 601us/step - loss: 0.3988 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.39884281158447266, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 659us/step - loss: 0.4061 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4060707092285156, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 602us/step - loss: 0.4042 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4041980803012848, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 679us/step - loss: 0.4067 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4067228138446808, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 591us/step - loss: 0.4105 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4104562997817993, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 677us/step - loss: 0.3987 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.39866340160369873, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 686us/step - loss: 0.4096 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.409566730260849, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 641us/step - loss: 0.4093 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4092777669429779, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 595us/step - loss: 0.4079 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.40785735845565796, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 641us/step - loss: 0.4036 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.40361323952674866, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 602us/step - loss: 0.4152 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4152340590953827, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 669us/step - loss: 0.4062 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4061928689479828, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 754us/step - loss: 0.3961 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.39612290263175964, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n",
      "64/64 [==============================] - 0s 705us/step - loss: 0.4051 - acc: 0.8488 - precision: 0.1514 - recall: 1.0000 - f1_metric: 0.2580\n",
      "[0.4050947427749634, 0.8487518429756165, 0.1514214426279068, 1.0, 0.25801247358322144]\n"
     ]
    }
   ],
   "source": [
    "#Run model predictions\n",
    "\n",
    "# Model 1 \n",
    "prediction_features_1 = model1.predict(features_test)\n",
    "performance1 = model1.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 2 \n",
    "prediction_features_2 = model2.predict(features_test)\n",
    "performance2 = model2.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 3 \n",
    "prediction_features_3 = model3.predict(features_test)\n",
    "performance3 = model3.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 4 \n",
    "prediction_features_4 = model4.predict(features_test)\n",
    "performance4 = model4.evaluate(features_test, labels_test)\n",
    "print(performance4)\n",
    "# Model 5 \n",
    "prediction_features_5 = model5.predict(features_test)\n",
    "performance5 = model5.evaluate(features_test, labels_test)\n",
    "print(performance5)\n",
    "\n",
    "# Model 6 \n",
    "prediction_features_6 = model6.predict(features_test)\n",
    "performance6 = model6.evaluate(features_test, labels_test)\n",
    "print(performance6)\n",
    "# Model 7\n",
    "prediction_features_7 = model7.predict(features_test)\n",
    "performance7 = model7.evaluate(features_test, labels_test)\n",
    "print(performance7)\n",
    "# Model 8 \n",
    "prediction_features_8 = model8.predict(features_test)\n",
    "performance8 = model8.evaluate(features_test, labels_test)\n",
    "print(performance8)\n",
    "# Model 9\n",
    "prediction_features_9 = model9.predict(features_test)\n",
    "performance9 = model9.evaluate(features_test, labels_test)\n",
    "print(performance9)\n",
    "# Model 10\n",
    "prediction_features_10 = model10.predict(features_test)\n",
    "performance10 = model10.evaluate(features_test, labels_test)\n",
    "print(performance10)\n",
    "\n",
    "# Model 11 \n",
    "prediction_features_11 = model11.predict(features_test)\n",
    "performance11 = model11.evaluate(features_test, labels_test)\n",
    "print(performance11)\n",
    "# Model 12 \n",
    "prediction_features_12 = model12.predict(features_test)\n",
    "performance12 = model12.evaluate(features_test, labels_test)\n",
    "print(performance12)\n",
    "# Model 13 \n",
    "prediction_features_13 = model13.predict(features_test)\n",
    "performance13 = model13.evaluate(features_test, labels_test)\n",
    "print(performance13)\n",
    "# Model 14 \n",
    "prediction_features_14 = model14.predict(features_test)\n",
    "performance14 = model14.evaluate(features_test, labels_test)\n",
    "print(performance14)\n",
    "# Model 15 \n",
    "prediction_features_15 = model15.predict(features_test)\n",
    "performance15 = model15.evaluate(features_test, labels_test)\n",
    "print(performance15)\n",
    "\n",
    "# Model 16\n",
    "prediction_features_16 = model16.predict(features_test)\n",
    "performance16 = model16.evaluate(features_test, labels_test)\n",
    "print(performance16)\n",
    "# Model 17\n",
    "prediction_features_17 = model17.predict(features_test)\n",
    "performance17 = model17.evaluate(features_test, labels_test)\n",
    "print(performance17)\n",
    "# Model 18 \n",
    "prediction_features_18 = model18.predict(features_test)\n",
    "performance18 = model18.evaluate(features_test, labels_test)\n",
    "print(performance18)\n",
    "# Model 19\n",
    "prediction_features_19 = model19.predict(features_test)\n",
    "performance19 = model19.evaluate(features_test, labels_test)\n",
    "print(performance19)\n",
    "# Model 20 \n",
    "prediction_features_20 = model20.predict(features_test)\n",
    "performance20 = model20.evaluate(features_test, labels_test)\n",
    "print(performance20)\n",
    "\n",
    "# Model 21\n",
    "prediction_features_21 = model21.predict(features_test)\n",
    "performance21 = model21.evaluate(features_test, labels_test)\n",
    "print(performance21)\n",
    "# Model 22\n",
    "prediction_features_22 = model22.predict(features_test)\n",
    "performance22 = model22.evaluate(features_test, labels_test)\n",
    "print(performance22)\n",
    "# Model 23\n",
    "prediction_features_23 = model23.predict(features_test)\n",
    "performance23 = model23.evaluate(features_test, labels_test)\n",
    "print(performance23)\n",
    "# Model 24 \n",
    "prediction_features_24 = model24.predict(features_test)\n",
    "performance24 = model24.evaluate(features_test, labels_test)\n",
    "print(performance24)\n",
    "# Model 25\n",
    "prediction_features_25 = model25.predict(features_test)\n",
    "performance25 = model25.evaluate(features_test, labels_test)\n",
    "print(performance25)\n",
    "\n",
    "# Model 26 \n",
    "prediction_features_26 = model26.predict(features_test)\n",
    "performance26 = model26.evaluate(features_test, labels_test)\n",
    "print(performance26)\n",
    "# Model 27 \n",
    "prediction_features_27 = model27.predict(features_test)\n",
    "performance27 = model27.evaluate(features_test, labels_test)\n",
    "print(performance27)\n",
    "# Model 28 \n",
    "prediction_features_28 = model28.predict(features_test)\n",
    "performance28 = model28.evaluate(features_test, labels_test)\n",
    "print(performance28)\n",
    "# Model 29\n",
    "prediction_features_29 = model29.predict(features_test)\n",
    "performance29 = model29.evaluate(features_test, labels_test)\n",
    "print(performance29)\n",
    "# Model 30 \n",
    "prediction_features_30 = model30.predict(features_test)\n",
    "performance30 = model30.evaluate(features_test, labels_test)\n",
    "print(performance30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "-kn2xq4Ts7s-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Average:  0.41271214187145233\n",
      "Accuraccy Average:  0.8401370604832967\n",
      "Precision Average:  0.15174806863069534\n",
      "Recall Average:  0.9781436045964559\n",
      "F1 Average:  0.25634347399075824\n"
     ]
    }
   ],
   "source": [
    "# Averages\n",
    "\n",
    "# Loss\n",
    "loss_avg = (performance1[0] + performance2[0] + performance3[0] + performance4[0] + performance5[0] + performance6[0] + performance7[0] + performance8[0] + performance9[0] + performance10[0]\n",
    "            + performance11[0] + performance12[0] + performance13[0] + performance14[0] + performance15[0] + performance16[0] + performance17[0] + performance18[0] + performance19[0] + performance20[0]\n",
    "            + performance21[0] + performance22[0] + performance23[0] + performance24[0] + performance25[0] + performance26[0] + performance27[0] + performance28[0] + performance29[0] + performance30[0])/30\n",
    "print(\"Loss Average: \", loss_avg)\n",
    "\n",
    "# Accuracy\n",
    "acc_avg =(performance1[1] + performance2[1] + performance3[1] + performance4[1] + performance5[1] + performance6[1] + performance7[1] + performance8[1] + performance9[1] + performance10[1]\n",
    "            + performance11[1] + performance12[1] + performance13[1] + performance14[1] + performance15[1] + performance16[1] + performance17[1] + performance18[1] + performance19[1] + performance20[1]\n",
    "            + performance21[1] + performance22[1] + performance23[1] + performance24[1] + performance25[1] + performance26[1] + performance27[1] + performance28[1] + performance29[1] + performance30[1])/30\n",
    "print(\"Accuraccy Average: \", acc_avg)\n",
    "\n",
    "# Precision\n",
    "precision_avg = (performance1[2] + performance2[2] + performance3[2] + performance4[2] + performance5[2] + performance6[2] + performance7[2] + performance8[2] + performance9[2] + performance10[2]\n",
    "            + performance11[2] + performance12[2] + performance13[2] + performance14[2] + performance15[2] + performance16[2] + performance17[2] + performance18[2] + performance19[2] + performance20[2]\n",
    "            + performance21[2] + performance22[2] + performance23[2] + performance24[2] + performance25[2] + performance26[2] + performance27[2] + performance28[2] + performance29[2] + performance30[2])/30\n",
    "print(\"Precision Average: \", precision_avg)\n",
    "\n",
    "# Recall\n",
    "recall_avg = (performance1[3] + performance2[3] + performance3[3] + performance4[3] + performance5[3] + performance6[3] + performance7[3] + performance8[3] + performance9[3] + performance10[3]\n",
    "            + performance11[3] + performance12[3] + performance13[3] + performance14[3] + performance15[3] + performance16[3] + performance17[3] + performance18[3] + performance19[3] + performance20[3]\n",
    "            + performance21[3] + performance22[3] + performance23[3] + performance24[3] + performance25[3] + performance26[3] + performance27[3] + performance28[3] + performance29[3] + performance30[3])/30\n",
    "print(\"Recall Average: \", recall_avg)\n",
    "\n",
    "# f1_metric\n",
    "f1_avg = (performance1[4] + performance2[4] + performance3[4] + performance4[4] + performance5[4] + performance6[4] + performance7[4] + performance8[4] + performance9[4] + performance10[4]\n",
    "            + performance11[4] + performance12[4] + performance13[4] + performance14[4] + performance15[4] + performance16[4] + performance17[4] + performance18[4] + performance19[4] + performance20[4]\n",
    "            + performance21[4] + performance22[4] + performance23[4] + performance24[4] + performance25[4] + performance26[4] + performance27[4] + performance28[4] + performance29[4] + performance30[4])/30\n",
    "print(\"F1 Average: \", f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XuwAUg5_KOz4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss SE: 0.04468908360725235\n",
      "Accuraccy SE:  0.047185106990440984\n",
      "Precision SE:  0.0017890042959503067\n",
      "Recall SE:  0.11971240788273337\n",
      "F1_Score_SE:  0.009141487252590104\n"
     ]
    }
   ],
   "source": [
    "#Take the standard deviation of the model samples\n",
    "\n",
    "#Loss SE\n",
    "Loss_SE = statistics.stdev([performance1[0], performance2[0], performance3[0], performance4[0], performance5[0],\n",
    "                  performance6[0], performance7[0], performance8[0], performance9[0], performance10[0],\n",
    "                  performance11[0], performance12[0], performance13[0], performance14[0], performance15[0],\n",
    "                  performance16[0], performance17[0], performance18[0], performance19[0], performance20[0], \n",
    "                  performance21[0], performance22[0], performance23[0], performance24[0], performance25[0],\n",
    "                  performance26[0], performance27[0], performance28[0], performance29[0], performance30[0]])\n",
    "print(\"Loss SE:\", Loss_SE)\n",
    "\n",
    "#Accuracy SE\n",
    "Acc_SE = statistics.stdev([performance1[1], performance2[1], performance3[1], performance4[1], performance5[1],\n",
    "                  performance6[1], performance7[1], performance8[1], performance9[1], performance10[1],\n",
    "                  performance11[1], performance12[1], performance13[1], performance14[1], performance15[1],\n",
    "                  performance16[1], performance17[1], performance18[1], performance19[1], performance20[1], \n",
    "                  performance21[1], performance22[1], performance23[1], performance24[1], performance25[1],\n",
    "                  performance26[1], performance27[1], performance28[1], performance29[1], performance30[1]])\n",
    "print(\"Accuraccy SE: \", Acc_SE)\n",
    "\n",
    "#Precision SE\n",
    "precision_SE = statistics.stdev([performance1[2], performance2[2], performance3[2], performance4[2], performance5[2],\n",
    "                  performance6[2], performance7[2], performance8[2], performance9[2], performance10[2],\n",
    "                  performance11[2], performance12[2], performance13[2], performance14[2], performance15[2],\n",
    "                  performance16[2], performance17[2], performance18[2], performance19[2], performance20[2], \n",
    "                  performance21[2], performance22[2], performance23[2], performance24[2], performance25[2],\n",
    "                  performance26[2], performance27[2], performance28[2], performance29[2], performance30[2]])\n",
    "print(\"Precision SE: \", precision_SE)\n",
    "\n",
    "#Recall \n",
    "Recall_SE = statistics.stdev([performance1[3], performance2[3], performance3[3], performance4[3], performance5[3],\n",
    "                  performance6[3], performance7[3], performance8[3], performance9[3], performance10[3],\n",
    "                  performance11[3], performance12[3], performance13[3], performance14[3], performance15[3],\n",
    "                  performance16[3], performance17[3], performance18[3], performance19[3], performance20[3], \n",
    "                  performance21[3], performance22[3], performance23[3], performance24[3], performance25[3],\n",
    "                  performance26[3], performance27[3], performance28[3], performance29[3], performance30[3]])\n",
    "print(\"Recall SE: \", Recall_SE)\n",
    "\n",
    "#F1 Score\n",
    "F1_Score_SE = statistics.stdev([performance1[4], performance2[4], performance3[4], performance4[4], performance5[4],\n",
    "                  performance6[4], performance7[4], performance8[4], performance9[4], performance10[4],\n",
    "                  performance11[4], performance12[4], performance13[4], performance14[4], performance15[4],\n",
    "                  performance16[4], performance17[4], performance18[4], performance19[4], performance20[4], \n",
    "                  performance21[4], performance22[4], performance23[4], performance24[4], performance25[4],\n",
    "                  performance26[4], performance27[4], performance28[4], performance29[4], performance30[4]])\n",
    "print(\"F1_Score_SE: \", F1_Score_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "aMpLKJ4OKt-X"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-75740ae46e3ca3f5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-75740ae46e3ca3f5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tensorflow Graphics\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GA_ParliHeg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

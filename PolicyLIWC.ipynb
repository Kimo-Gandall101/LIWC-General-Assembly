{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g9kxq01msMg9"
   },
   "outputs": [],
   "source": [
    "### BEWARE:Tensorflow is stochastic - this means the model will not be replicated exactly. \n",
    "### Use GA_Load_Model for reproduction\n",
    "\n",
    "#!pip install mlxtend\n",
    "\n",
    "#!pip install h5py pyyaml\n",
    "\n",
    "#!pip install tensorboard\n",
    "\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tLftnb5sBe7B"
   },
   "outputs": [],
   "source": [
    "#Load packages\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iBsKz40OAGDU"
   },
   "outputs": [],
   "source": [
    "### Packages necessary for model construction \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks\n",
    "import datetime \n",
    "import statistics\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FrEcBaVWVVkb"
   },
   "outputs": [],
   "source": [
    "#Read the Data\n",
    "\n",
    "UN_Data = pd.read_csv('GA_Query_CleanLIWC') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 4369,
     "status": "ok",
     "timestamp": 1611639237251,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "FNUqnWTFS4y2",
    "outputId": "cf54ea17-a224-4830-8f6b-554ee9bafaf0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Policy Passed</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20075.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.34</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>822.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.04</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.18</td>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17021.0</td>\n",
       "      <td>98.45</td>\n",
       "      <td>...</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9289.0</td>\n",
       "      <td>98.94</td>\n",
       "      <td>...</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4059.0</td>\n",
       "      <td>98.95</td>\n",
       "      <td>...</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8210.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10209</th>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>99.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10210</th>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>98.88</td>\n",
       "      <td>...</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.66</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10211</th>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22700.0</td>\n",
       "      <td>98.26</td>\n",
       "      <td>...</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10212 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  Class M  Class S  Class I  Class P  Class B  Policy Passed  \\\n",
       "0      2012        1        0        0        0        0              0   \n",
       "1      2012        0        0        3        0        0              0   \n",
       "2      2003        0        0        0        0        0              0   \n",
       "3      1995        0        0        0        0        0              1   \n",
       "4      2007        0        0        0        0        0              0   \n",
       "...     ...      ...      ...      ...      ...      ...            ...   \n",
       "10207  2004        0        0        0        0        0              0   \n",
       "10208  1994        0        0        0        0        0              0   \n",
       "10209  2013        0        0        0        0        0              0   \n",
       "10210  2009        0        0        0        0        0              0   \n",
       "10211  2016        0        0        0        0        0              0   \n",
       "\n",
       "       Conflict Indicator       WC  Analytic  ...  Comma  Colon  SemiC  QMark  \\\n",
       "0                       1  20075.0     99.00  ...   4.34   0.03   0.04   0.00   \n",
       "1                       0    822.0     99.00  ...   3.04   1.70   0.00   0.00   \n",
       "2                       0    314.0     99.00  ...   3.50   0.96   0.00   0.00   \n",
       "3                       1  17021.0     98.45  ...   4.91   0.25   0.17   0.02   \n",
       "4                       0   9289.0     98.94  ...   3.80   0.16   0.15   0.00   \n",
       "...                   ...      ...       ...  ...    ...    ...    ...    ...   \n",
       "10207                   0   4059.0     98.95  ...   3.72   0.15   0.12   0.00   \n",
       "10208                   0   8210.0     99.00  ...   3.58   0.12   0.22   0.00   \n",
       "10209                   0    583.0     99.00  ...   3.09   0.86   0.00   0.69   \n",
       "10210                   0   1562.0     98.88  ...   2.82   0.19   0.45   0.00   \n",
       "10211                   0  22700.0     98.26  ...   3.69   0.04   0.03   0.04   \n",
       "\n",
       "       Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "0         0.0  1.23   0.07     0.64     0.82    0.60  \n",
       "1         0.0  0.85   0.49     0.12     4.38    1.46  \n",
       "2         0.0  2.23   0.00     0.64     3.18    2.87  \n",
       "3         0.0  1.33   0.22     0.16     0.64    2.18  \n",
       "4         0.0  0.93   0.28     0.75     1.42    1.52  \n",
       "...       ...   ...    ...      ...      ...     ...  \n",
       "10207     0.0  1.18   0.00     0.76     1.72    1.23  \n",
       "10208     0.0  1.06   0.02     0.29     1.05    1.75  \n",
       "10209     0.0  1.89   0.34     0.00     4.80    4.63  \n",
       "10210     0.0  1.34   0.00     0.77     1.66    2.18  \n",
       "10211     0.0  1.44   0.10     0.48     1.01    1.58  \n",
       "\n",
       "[10212 rows x 101 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect and Clean the Data\n",
    "\n",
    "UN_Data.head(5)\n",
    "\n",
    "UN_Data.drop(['Unnamed: 0'], axis = 1, inplace= True)\n",
    "\n",
    "UN_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 4368,
     "status": "ok",
     "timestamp": 1611639237252,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "tkn1c4RqeEdO",
    "outputId": "d56ecbce-990a-427d-e279-772f8546aeac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Policy Passed</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10212.00000</td>\n",
       "      <td>10212.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2005.852135</td>\n",
       "      <td>0.032805</td>\n",
       "      <td>0.009205</td>\n",
       "      <td>0.168625</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.021641</td>\n",
       "      <td>0.13964</td>\n",
       "      <td>0.457697</td>\n",
       "      <td>9442.204220</td>\n",
       "      <td>98.190282</td>\n",
       "      <td>...</td>\n",
       "      <td>5.193233</td>\n",
       "      <td>0.303105</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.031364</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>1.154469</td>\n",
       "      <td>0.260464</td>\n",
       "      <td>0.397916</td>\n",
       "      <td>1.744508</td>\n",
       "      <td>1.768528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.542111</td>\n",
       "      <td>0.229542</td>\n",
       "      <td>0.099521</td>\n",
       "      <td>0.861341</td>\n",
       "      <td>0.307019</td>\n",
       "      <td>0.156540</td>\n",
       "      <td>0.34663</td>\n",
       "      <td>0.498232</td>\n",
       "      <td>7786.325195</td>\n",
       "      <td>1.085791</td>\n",
       "      <td>...</td>\n",
       "      <td>3.290078</td>\n",
       "      <td>0.466117</td>\n",
       "      <td>0.190811</td>\n",
       "      <td>0.105541</td>\n",
       "      <td>0.713727</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>0.439789</td>\n",
       "      <td>1.273977</td>\n",
       "      <td>1.466890</td>\n",
       "      <td>4.337472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1993.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3838.500000</td>\n",
       "      <td>97.880000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2006.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7439.500000</td>\n",
       "      <td>98.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2012.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12438.750000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.990000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>2.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>74776.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>47.760000</td>\n",
       "      <td>22.710000</td>\n",
       "      <td>11.150000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>49.920000</td>\n",
       "      <td>22.880000</td>\n",
       "      <td>14.030000</td>\n",
       "      <td>89.620000</td>\n",
       "      <td>17.900000</td>\n",
       "      <td>213.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       Class M       Class S       Class I       Class P  \\\n",
       "count  10212.000000  10212.000000  10212.000000  10212.000000  10212.000000   \n",
       "mean    2005.852135      0.032805      0.009205      0.168625      0.051900   \n",
       "std        7.542111      0.229542      0.099521      0.861341      0.307019   \n",
       "min     1993.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     1999.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     2006.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%     2012.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max     2020.000000      9.000000      2.000000     28.000000      9.000000   \n",
       "\n",
       "            Class B  Policy Passed  Conflict Indicator            WC  \\\n",
       "count  10212.000000    10212.00000        10212.000000  10190.000000   \n",
       "mean       0.021641        0.13964            0.457697   9442.204220   \n",
       "std        0.156540        0.34663            0.498232   7786.325195   \n",
       "min        0.000000        0.00000            0.000000     44.000000   \n",
       "25%        0.000000        0.00000            0.000000   3838.500000   \n",
       "50%        0.000000        0.00000            0.000000   7439.500000   \n",
       "75%        0.000000        0.00000            1.000000  12438.750000   \n",
       "max        3.000000        1.00000            1.000000  74776.000000   \n",
       "\n",
       "           Analytic  ...         Comma         Colon         SemiC  \\\n",
       "count  10190.000000  ...  10190.000000  10190.000000  10190.000000   \n",
       "mean      98.190282  ...      5.193233      0.303105      0.144628   \n",
       "std        1.085791  ...      3.290078      0.466117      0.190811   \n",
       "min       80.460000  ...      0.210000      0.000000      0.000000   \n",
       "25%       97.880000  ...      3.980000      0.090000      0.060000   \n",
       "50%       98.440000  ...      4.440000      0.160000      0.110000   \n",
       "75%       99.000000  ...      4.990000      0.320000      0.190000   \n",
       "max       99.000000  ...     47.760000     22.710000     11.150000   \n",
       "\n",
       "              QMark        Exclam          Dash         Quote       Apostro  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.031364      0.014043      1.154469      0.260464      0.397916   \n",
       "std        0.105541      0.713727      0.643019      0.439789      1.273977   \n",
       "min        0.000000      0.000000      0.020000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.800000      0.050000      0.230000   \n",
       "50%        0.000000      0.000000      1.040000      0.130000      0.340000   \n",
       "75%        0.020000      0.000000      1.380000      0.300000      0.490000   \n",
       "max        3.400000     49.920000     22.880000     14.030000     89.620000   \n",
       "\n",
       "            Parenth        OtherP  \n",
       "count  10190.000000  10190.000000  \n",
       "mean       1.744508      1.768528  \n",
       "std        1.466890      4.337472  \n",
       "min        0.130000      0.010000  \n",
       "25%        0.860000      0.740000  \n",
       "50%        1.330000      1.170000  \n",
       "75%        2.090000      2.070000  \n",
       "max       17.900000    213.040000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect the data by key descriptive statistics\n",
    "\n",
    "UN_Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 4367,
     "status": "ok",
     "timestamp": 1611639237253,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "1c56fLBjJKA3",
    "outputId": "8b5f5028-9f16-4543-a585-e014b90e8ef7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Policy Passed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8786</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>...</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "      <td>8764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>...</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "      <td>1426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  Class M  Class S  Class I  Class P  Class B  \\\n",
       "Policy Passed                                                      \n",
       "0              8786     8786     8786     8786     8786     8786   \n",
       "1              1426     1426     1426     1426     1426     1426   \n",
       "\n",
       "               Conflict Indicator    WC  Analytic  Clout  ...  Comma  Colon  \\\n",
       "Policy Passed                                             ...                 \n",
       "0                            8786  8764      8764   8764  ...   8764   8764   \n",
       "1                            1426  1426      1426   1426  ...   1426   1426   \n",
       "\n",
       "               SemiC  QMark  Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "Policy Passed                                                               \n",
       "0               8764   8764    8764  8764   8764     8764     8764    8764  \n",
       "1               1426   1426    1426  1426   1426     1426     1426    1426  \n",
       "\n",
       "[2 rows x 100 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Group the data by our label (dependent variable) of policy passage\n",
    "\n",
    "UN_Data.groupby(['Policy Passed']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Class M</th>\n",
       "      <th>Class S</th>\n",
       "      <th>Class I</th>\n",
       "      <th>Class P</th>\n",
       "      <th>Class B</th>\n",
       "      <th>Conflict Indicator</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>...</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Policy Passed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>10190.000000</td>\n",
       "      <td>1.019000e+04</td>\n",
       "      <td>10212.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.345714</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.881989</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>4.345803e-04</td>\n",
       "      <td>0.13964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.252698</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.193529</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>1.748792e-03</td>\n",
       "      <td>0.34663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.026924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021901</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.473887e-07</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.159519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.885374</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>6.634241e-05</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.260213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.965297</td>\n",
       "      <td>0.012765</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>1.553934e-04</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.463364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.987100</td>\n",
       "      <td>0.022704</td>\n",
       "      <td>0.015068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>3.987904e-04</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997075</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.999635</td>\n",
       "      <td>0.049344</td>\n",
       "      <td>0.040877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005398</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.010615</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>0.019591</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>9.570280e-02</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date       Class M       Class S       Class I       Class P  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.345714      0.000004      0.000001      0.000021      0.000007   \n",
       "std        0.252698      0.000030      0.000017      0.000110      0.000050   \n",
       "min        0.026924      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.159519      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.260213      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.463364      0.000000      0.000000      0.000000      0.000000   \n",
       "max        0.997075      0.001033      0.000412      0.002388      0.001449   \n",
       "\n",
       "            Class B  Conflict Indicator            WC      Analytic  \\\n",
       "count  10190.000000        10190.000000  10190.000000  10190.000000   \n",
       "mean       0.000002            0.000056      0.881989      0.016961   \n",
       "std        0.000021            0.000086      0.193529      0.012448   \n",
       "min        0.000000            0.000000      0.021901      0.001303   \n",
       "25%        0.000000            0.000000      0.885374      0.007792   \n",
       "50%        0.000000            0.000000      0.965297      0.012765   \n",
       "75%        0.000000            0.000094      0.987100      0.022704   \n",
       "max        0.000514            0.000492      0.999635      0.049344   \n",
       "\n",
       "              Clout  ...         Colon         SemiC         QMark  \\\n",
       "count  10190.000000  ...  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.011428  ...      0.000081      0.000027      0.000006   \n",
       "std        0.008500  ...      0.000179      0.000070      0.000029   \n",
       "min        0.001053  ...      0.000000      0.000000      0.000000   \n",
       "25%        0.005150  ...      0.000008      0.000004      0.000000   \n",
       "50%        0.008537  ...      0.000020      0.000012      0.000000   \n",
       "75%        0.015068  ...      0.000066      0.000029      0.000001   \n",
       "max        0.040877  ...      0.005398      0.004388      0.001033   \n",
       "\n",
       "             Exclam          Dash         Quote       Apostro       Parenth  \\\n",
       "count  10190.000000  10190.000000  10190.000000  10190.000000  10190.000000   \n",
       "mean       0.000003      0.000222      0.000051      0.000066      0.000417   \n",
       "std        0.000154      0.000308      0.000150      0.000281      0.000665   \n",
       "min        0.000000      0.000003      0.000000      0.000000      0.000003   \n",
       "25%        0.000000      0.000069      0.000004      0.000020      0.000073   \n",
       "50%        0.000000      0.000129      0.000013      0.000040      0.000180   \n",
       "75%        0.000000      0.000252      0.000041      0.000077      0.000431   \n",
       "max        0.010615      0.010232      0.006303      0.019591      0.008041   \n",
       "\n",
       "             OtherP  Policy Passed  \n",
       "count  1.019000e+04    10212.00000  \n",
       "mean   4.345803e-04        0.13964  \n",
       "std    1.748792e-03        0.34663  \n",
       "min    4.473887e-07        0.00000  \n",
       "25%    6.634241e-05        0.00000  \n",
       "50%    1.553934e-04        0.00000  \n",
       "75%    3.987904e-04        0.00000  \n",
       "max    9.570280e-02        1.00000  \n",
       "\n",
       "[8 rows x 101 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalize the data \n",
    "\n",
    "UN_Data1 = tf.keras.utils.normalize(UN_Data.drop(columns = ['Policy Passed']))\n",
    "\n",
    "UN_Data1[\"Policy Passed\"] = UN_Data['Policy Passed']\n",
    "\n",
    "UN_Data1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "r6BF8hmXGEnF"
   },
   "outputs": [],
   "source": [
    "#Divide our variables between the independent variables (features) and dependent variables (policy passage)\n",
    "\n",
    "labels = UN_Data ['Policy Passed']\n",
    "features = UN_Data.drop(columns= ['Policy Passed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Null Values\n",
    "\n",
    "features = features.fillna(0)\n",
    "labels = labels.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1611639237254,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "4FJRWpUNH8Dk",
    "outputId": "fe514d19-f576-4030-c91f-fc2b562d2213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10212, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspect shape of features\n",
    "\n",
    "features = pd.get_dummies(features)\n",
    "features.shape[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-oGgPi4CG0xx"
   },
   "outputs": [],
   "source": [
    "#Define type of feature and label values\n",
    "\n",
    "features = features.values.astype('float32')\n",
    "labels = labels.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XOea_aP_HPSE"
   },
   "outputs": [],
   "source": [
    "#Data Sets for Training\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)\n",
    "features_train, features_validation, labels_train, labels_validation = train_test_split(features_train, labels_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "vHSYOIsh5vq4"
   },
   "outputs": [],
   "source": [
    "#Define Precision, Recall, and F1 score metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4VQ_OqkCHkLM"
   },
   "outputs": [],
   "source": [
    "#Create your model\n",
    "\n",
    "model1 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model2 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model3 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model4 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model5 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model6 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model7 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model8 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model9 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model10 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model11 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model12 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model13 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model14 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model15 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model16 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model17 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model18 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model19 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model20 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model21 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model22 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model23 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model24 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model25 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "\n",
    "model26 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model27 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model28 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model29 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])\n",
    "model30 = tf.keras.Sequential([keras.layers.Dense(32, input_shape=(100,)),keras.layers.Dropout(.20),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(2, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1611639239019,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "7JTlq8aH8mzi",
    "outputId": "a7831e8c-a787-4b03-8006-c9aebd129183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "### Inspect form of model\n",
    "\n",
    "tf.keras.utils.plot_model(model1, to_file='model.png', show_shapes = True, show_dtype=True, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6114,
     "status": "ok",
     "timestamp": 1611639239020,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "52Vb8b-qSIQ4",
    "outputId": "dd0ce729-358b-43ac-8d65-6c1f00ba812a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                3232      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 3,794\n",
      "Trainable params: 3,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Check Trainable Parameters\n",
    "# Note: All the models are similarly structured\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Vyx2tehuTPfe"
   },
   "outputs": [],
   "source": [
    "#Set checkpoints, metrics, loss, and optimizer functions for the model\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model1.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model3.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model4.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model5.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model6.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model7.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model8.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model9.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model10.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model11.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model12.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model13.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model14.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model15.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model16.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model17.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model18.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model19.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model20.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model21.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model22.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model23.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model24.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model25.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "\n",
    "model26.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model27.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model28.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model29.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])\n",
    "model30.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['acc', precision, recall, f1_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 150145,
     "status": "error",
     "timestamp": 1611639383055,
     "user": {
      "displayName": "Kimo Gandall",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GimtqABjWQhcZq3EN733En5BNQLUL5UuOeyZFu4=s64",
      "userId": "00887894863642395375"
     },
     "user_tz": 480
    },
    "id": "RhwA7DthFS72",
    "outputId": "64a70fe2-c811-4102-e25e-f708c8ee64e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 3ms/step - loss: 259.1378 - acc: 0.8517 - precision: 0.0583 - recall: 0.0323 - f1_metric: 0.0348 - val_loss: 7.7538 - val_acc: 0.8703 - val_precision: 0.0676 - val_recall: 0.1574 - val_f1_metric: 0.0905\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 925us/step - loss: 11.1206 - acc: 0.8505 - precision: 0.1206 - recall: 0.6282 - f1_metric: 0.1969 - val_loss: 0.5163 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 2.8555 - acc: 0.8551 - precision: 0.1353 - recall: 0.8932 - f1_metric: 0.2304 - val_loss: 0.4478 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 918us/step - loss: 2.3559 - acc: 0.8525 - precision: 0.1422 - recall: 0.9459 - f1_metric: 0.2423 - val_loss: 0.4165 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.7504 - acc: 0.8602 - precision: 0.1397 - recall: 0.9817 - f1_metric: 0.2399 - val_loss: 0.4020 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6576 - acc: 0.8588 - precision: 0.1423 - recall: 0.9865 - f1_metric: 0.2440 - val_loss: 0.3945 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.7286 - acc: 0.8612 - precision: 0.1361 - recall: 0.9565 - f1_metric: 0.2341 - val_loss: 0.3906 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6158 - acc: 0.8586 - precision: 0.1410 - recall: 0.9842 - f1_metric: 0.2421 - val_loss: 0.3886 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4922 - acc: 0.8566 - precision: 0.1419 - recall: 1.0020 - f1_metric: 0.2443 - val_loss: 0.3875 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.7564 - acc: 0.8567 - precision: 0.1410 - recall: 0.9790 - f1_metric: 0.2420 - val_loss: 0.3870 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4689 - acc: 0.8558 - precision: 0.1430 - recall: 1.0097 - f1_metric: 0.2453 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4583 - acc: 0.8542 - precision: 0.1448 - recall: 0.9969 - f1_metric: 0.2477 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4745 - acc: 0.8541 - precision: 0.1439 - recall: 1.0124 - f1_metric: 0.2466 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4171 - acc: 0.8626 - precision: 0.1368 - recall: 1.0188 - f1_metric: 0.2367 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4697 - acc: 0.8671 - precision: 0.1316 - recall: 0.9910 - f1_metric: 0.2272 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4096 - acc: 0.8619 - precision: 0.1381 - recall: 1.0069 - f1_metric: 0.2379 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 941us/step - loss: 0.4523 - acc: 0.8594 - precision: 0.1376 - recall: 0.9966 - f1_metric: 0.2376 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4087 - acc: 0.8601 - precision: 0.1381 - recall: 1.0021 - f1_metric: 0.2391 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4128 - acc: 0.8569 - precision: 0.1425 - recall: 1.0139 - f1_metric: 0.2455 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4188 - acc: 0.8545 - precision: 0.1449 - recall: 1.0045 - f1_metric: 0.2486 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4947 - acc: 0.8545 - precision: 0.1457 - recall: 1.0073 - f1_metric: 0.2487 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.4049 - acc: 0.8621 - precision: 0.1375 - recall: 0.9931 - f1_metric: 0.2374 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 958us/step - loss: 0.4075 - acc: 0.8636 - precision: 0.1350 - recall: 0.9876 - f1_metric: 0.2331 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.4045 - acc: 0.8645 - precision: 0.1351 - recall: 1.0027 - f1_metric: 0.2335 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4201 - acc: 0.8526 - precision: 0.1486 - recall: 1.0170 - f1_metric: 0.2551 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.4148 - acc: 0.8569 - precision: 0.1433 - recall: 1.0055 - f1_metric: 0.2465 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 970us/step - loss: 0.4005 - acc: 0.8639 - precision: 0.1350 - recall: 0.9873 - f1_metric: 0.2334 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.3931 - acc: 0.8671 - precision: 0.1319 - recall: 0.9931 - f1_metric: 0.2277 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.4658 - acc: 0.8507 - precision: 0.1487 - recall: 1.0041 - f1_metric: 0.2538 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.4157 - acc: 0.8537 - precision: 0.1457 - recall: 0.9841 - f1_metric: 0.2493 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Model 2 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 313.5075 - acc: 0.8202 - precision: 0.1502 - recall: 1.6045 - f1_metric: 0.2710 - val_loss: 10.0094 - val_acc: 0.8703 - val_precision: 0.1477 - val_recall: 1.9808 - val_f1_metric: 0.2645\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 37.6648 - acc: 0.8441 - precision: 0.1437 - recall: 1.3842 - f1_metric: 0.2553 - val_loss: 0.5278 - val_acc: 0.8703 - val_precision: 0.1543 - val_recall: 1.8054 - val_f1_metric: 0.2740\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 854us/step - loss: 3.3354 - acc: 0.8496 - precision: 0.1412 - recall: 1.0151 - f1_metric: 0.2423 - val_loss: 0.4893 - val_acc: 0.8703 - val_precision: 0.1545 - val_recall: 1.8231 - val_f1_metric: 0.2746\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 2.0869 - acc: 0.8604 - precision: 0.1458 - recall: 1.1119 - f1_metric: 0.2528 - val_loss: 0.4910 - val_acc: 0.8703 - val_precision: 0.1545 - val_recall: 1.7702 - val_f1_metric: 0.2740\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 948us/step - loss: 1.1601 - acc: 0.8612 - precision: 0.1369 - recall: 1.0086 - f1_metric: 0.2344 - val_loss: 0.4563 - val_acc: 0.8703 - val_precision: 0.1544 - val_recall: 1.7766 - val_f1_metric: 0.2739\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.8791 - acc: 0.8522 - precision: 0.1523 - recall: 1.0950 - f1_metric: 0.2611 - val_loss: 0.4456 - val_acc: 0.8703 - val_precision: 0.1543 - val_recall: 1.8054 - val_f1_metric: 0.2740\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 1.3466 - acc: 0.8563 - precision: 0.1436 - recall: 1.0202 - f1_metric: 0.2467 - val_loss: 0.4295 - val_acc: 0.8703 - val_precision: 0.1542 - val_recall: 1.8279 - val_f1_metric: 0.2742\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.7230 - acc: 0.8559 - precision: 0.1498 - recall: 1.1354 - f1_metric: 0.2591 - val_loss: 0.4354 - val_acc: 0.8703 - val_precision: 0.1546 - val_recall: 1.9042 - val_f1_metric: 0.2756\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 862us/step - loss: 0.7417 - acc: 0.8603 - precision: 0.1365 - recall: 1.1301 - f1_metric: 0.2400 - val_loss: 0.3987 - val_acc: 0.8703 - val_precision: 0.1534 - val_recall: 1.9567 - val_f1_metric: 0.2741\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 860us/step - loss: 0.8231 - acc: 0.8532 - precision: 0.1534 - recall: 1.3340 - f1_metric: 0.2697 - val_loss: 0.4009 - val_acc: 0.8703 - val_precision: 0.1518 - val_recall: 1.9567 - val_f1_metric: 0.2713\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.5087 - acc: 0.8550 - precision: 0.1447 - recall: 1.3164 - f1_metric: 0.2558 - val_loss: 0.3977 - val_acc: 0.8703 - val_precision: 0.1515 - val_recall: 1.9567 - val_f1_metric: 0.2709\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4937 - acc: 0.8597 - precision: 0.1413 - recall: 1.3042 - f1_metric: 0.2514 - val_loss: 0.3926 - val_acc: 0.8703 - val_precision: 0.1521 - val_recall: 1.9567 - val_f1_metric: 0.2719\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.5691 - acc: 0.8588 - precision: 0.1405 - recall: 1.3322 - f1_metric: 0.2500 - val_loss: 0.3912 - val_acc: 0.8703 - val_precision: 0.1520 - val_recall: 1.9567 - val_f1_metric: 0.2717\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4585 - acc: 0.8583 - precision: 0.1421 - recall: 1.2668 - f1_metric: 0.2511 - val_loss: 0.3889 - val_acc: 0.8703 - val_precision: 0.1524 - val_recall: 1.9567 - val_f1_metric: 0.2723\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.6350 - acc: 0.8621 - precision: 0.1469 - recall: 1.3588 - f1_metric: 0.2603 - val_loss: 0.3869 - val_acc: 0.8703 - val_precision: 0.1534 - val_recall: 1.9567 - val_f1_metric: 0.2742\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.4236 - acc: 0.8639 - precision: 0.1375 - recall: 1.2329 - f1_metric: 0.2422 - val_loss: 0.3869 - val_acc: 0.8703 - val_precision: 0.1563 - val_recall: 1.0607 - val_f1_metric: 0.2573\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.5141 - acc: 0.8572 - precision: 0.1487 - recall: 1.1480 - f1_metric: 0.2582 - val_loss: 0.3870 - val_acc: 0.8703 - val_precision: 0.1549 - val_recall: 0.9521 - val_f1_metric: 0.2520\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.5482 - acc: 0.8546 - precision: 0.1494 - recall: 1.1151 - f1_metric: 0.2581 - val_loss: 0.3887 - val_acc: 0.8703 - val_precision: 0.1549 - val_recall: 0.9569 - val_f1_metric: 0.2523\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4348 - acc: 0.8585 - precision: 0.1429 - recall: 1.1164 - f1_metric: 0.2483 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1529 - val_recall: 0.9784 - val_f1_metric: 0.2501\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.5350 - acc: 0.8632 - precision: 0.1361 - recall: 1.1057 - f1_metric: 0.2376 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1519 - val_recall: 0.9808 - val_f1_metric: 0.2487\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4065 - acc: 0.8658 - precision: 0.1370 - recall: 1.0420 - f1_metric: 0.2375 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1520 - val_recall: 0.9784 - val_f1_metric: 0.2487\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 863us/step - loss: 0.4302 - acc: 0.8586 - precision: 0.1435 - recall: 1.0136 - f1_metric: 0.2454 - val_loss: 0.3872 - val_acc: 0.8703 - val_precision: 0.1520 - val_recall: 0.9784 - val_f1_metric: 0.2487\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.4006 - acc: 0.8660 - precision: 0.1333 - recall: 0.9858 - f1_metric: 0.2313 - val_loss: 0.3852 - val_acc: 0.8703 - val_precision: 0.1508 - val_recall: 0.9808 - val_f1_metric: 0.2470\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4137 - acc: 0.8612 - precision: 0.1480 - recall: 1.0592 - f1_metric: 0.2547 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1503 - val_recall: 0.9808 - val_f1_metric: 0.2462\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4141 - acc: 0.8574 - precision: 0.1427 - recall: 0.9549 - f1_metric: 0.2425 - val_loss: 0.3853 - val_acc: 0.8703 - val_precision: 0.1504 - val_recall: 0.9872 - val_f1_metric: 0.2466\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4857 - acc: 0.8571 - precision: 0.1422 - recall: 0.9902 - f1_metric: 0.2436 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1504 - val_recall: 0.9872 - val_f1_metric: 0.2466\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4188 - acc: 0.8628 - precision: 0.1397 - recall: 0.9881 - f1_metric: 0.2402 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1501 - val_recall: 0.9872 - val_f1_metric: 0.2462\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4279 - acc: 0.8520 - precision: 0.1474 - recall: 0.9750 - f1_metric: 0.2517 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1490 - val_recall: 0.9904 - val_f1_metric: 0.2447\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4211 - acc: 0.8598 - precision: 0.1369 - recall: 0.9955 - f1_metric: 0.2355 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 0.9904 - val_f1_metric: 0.2435\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 852us/step - loss: 0.4774 - acc: 0.8622 - precision: 0.1406 - recall: 1.0207 - f1_metric: 0.2432 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1484 - val_recall: 0.9904 - val_f1_metric: 0.2436\n",
      "Model 3 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 565.7784 - acc: 0.6508 - precision: 0.1418 - recall: 0.8503 - f1_metric: 0.2360 - val_loss: 2.6977 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0367 - val_f1_metric: 0.2382\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 30.9349 - acc: 0.8290 - precision: 0.1414 - recall: 1.1579 - f1_metric: 0.2478 - val_loss: 0.5869 - val_acc: 0.8703 - val_precision: 0.1394 - val_recall: 1.0216 - val_f1_metric: 0.2304\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 6.6585 - acc: 0.8474 - precision: 0.1375 - recall: 1.0108 - f1_metric: 0.2373 - val_loss: 0.4900 - val_acc: 0.8703 - val_precision: 0.1405 - val_recall: 1.0192 - val_f1_metric: 0.2320\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 3.3893 - acc: 0.8548 - precision: 0.1358 - recall: 1.0018 - f1_metric: 0.2346 - val_loss: 0.4337 - val_acc: 0.8703 - val_precision: 0.1413 - val_recall: 1.0128 - val_f1_metric: 0.2332\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 1.5270 - acc: 0.8545 - precision: 0.1386 - recall: 1.0127 - f1_metric: 0.2389 - val_loss: 0.4088 - val_acc: 0.8703 - val_precision: 0.1423 - val_recall: 1.0096 - val_f1_metric: 0.2346\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 1.0217 - acc: 0.8588 - precision: 0.1379 - recall: 1.0083 - f1_metric: 0.2380 - val_loss: 0.3963 - val_acc: 0.8703 - val_precision: 0.1425 - val_recall: 1.0096 - val_f1_metric: 0.2350\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 1.0157 - acc: 0.8599 - precision: 0.1348 - recall: 0.9964 - f1_metric: 0.2328 - val_loss: 0.3899 - val_acc: 0.8703 - val_precision: 0.1427 - val_recall: 1.0048 - val_f1_metric: 0.2351\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.7753 - acc: 0.8553 - precision: 0.1402 - recall: 1.0035 - f1_metric: 0.2411 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1426 - val_recall: 1.0048 - val_f1_metric: 0.2350\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 863us/step - loss: 0.8873 - acc: 0.8519 - precision: 0.1430 - recall: 1.0123 - f1_metric: 0.2464 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1427 - val_recall: 1.0048 - val_f1_metric: 0.2351\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.5332 - acc: 0.8545 - precision: 0.1418 - recall: 1.0076 - f1_metric: 0.2439 - val_loss: 0.3857 - val_acc: 0.8703 - val_precision: 0.1427 - val_recall: 1.0048 - val_f1_metric: 0.2351\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.5362 - acc: 0.8587 - precision: 0.1397 - recall: 1.0088 - f1_metric: 0.2417 - val_loss: 0.3846 - val_acc: 0.8703 - val_precision: 0.1426 - val_recall: 1.0048 - val_f1_metric: 0.2350\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.6515 - acc: 0.8622 - precision: 0.1356 - recall: 1.0131 - f1_metric: 0.2341 - val_loss: 0.3847 - val_acc: 0.8703 - val_precision: 0.1427 - val_recall: 1.0048 - val_f1_metric: 0.2351\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.7300 - acc: 0.8586 - precision: 0.1391 - recall: 1.0130 - f1_metric: 0.2404 - val_loss: 0.3843 - val_acc: 0.8703 - val_precision: 0.1425 - val_recall: 1.0048 - val_f1_metric: 0.2347\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4448 - acc: 0.8529 - precision: 0.1436 - recall: 1.0093 - f1_metric: 0.2473 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1427 - val_recall: 1.0048 - val_f1_metric: 0.2351\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4368 - acc: 0.8603 - precision: 0.1361 - recall: 0.9998 - f1_metric: 0.2348 - val_loss: 0.3841 - val_acc: 0.8703 - val_precision: 0.1428 - val_recall: 1.0096 - val_f1_metric: 0.2354\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.6588 - acc: 0.8481 - precision: 0.1483 - recall: 1.0036 - f1_metric: 0.2539 - val_loss: 0.3853 - val_acc: 0.8703 - val_precision: 0.1429 - val_recall: 1.0096 - val_f1_metric: 0.2356\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4496 - acc: 0.8634 - precision: 0.1334 - recall: 0.9952 - f1_metric: 0.2310 - val_loss: 0.3842 - val_acc: 0.8703 - val_precision: 0.1428 - val_recall: 1.0096 - val_f1_metric: 0.2354\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4565 - acc: 0.8557 - precision: 0.1426 - recall: 1.0182 - f1_metric: 0.2451 - val_loss: 0.3844 - val_acc: 0.8703 - val_precision: 0.1431 - val_recall: 1.0000 - val_f1_metric: 0.2356\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.5676 - acc: 0.8584 - precision: 0.1397 - recall: 1.0052 - f1_metric: 0.2408 - val_loss: 0.3856 - val_acc: 0.8703 - val_precision: 0.1439 - val_recall: 1.0000 - val_f1_metric: 0.2369\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4286 - acc: 0.8592 - precision: 0.1392 - recall: 0.9965 - f1_metric: 0.2408 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1444 - val_recall: 1.0000 - val_f1_metric: 0.2376\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4469 - acc: 0.8564 - precision: 0.1418 - recall: 0.9833 - f1_metric: 0.2426 - val_loss: 0.3857 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4041 - acc: 0.8615 - precision: 0.1378 - recall: 1.0134 - f1_metric: 0.2372 - val_loss: 0.3858 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.4130 - acc: 0.8575 - precision: 0.1405 - recall: 0.9964 - f1_metric: 0.2415 - val_loss: 0.3854 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.4058 - acc: 0.8598 - precision: 0.1387 - recall: 1.0077 - f1_metric: 0.2392 - val_loss: 0.3857 - val_acc: 0.8703 - val_precision: 0.1438 - val_recall: 1.0000 - val_f1_metric: 0.2367\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 853us/step - loss: 0.4307 - acc: 0.8566 - precision: 0.1427 - recall: 0.9818 - f1_metric: 0.2438 - val_loss: 0.3851 - val_acc: 0.8703 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_metric: 0.2362\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 997us/step - loss: 0.4146 - acc: 0.8592 - precision: 0.1392 - recall: 1.0085 - f1_metric: 0.2403 - val_loss: 0.3857 - val_acc: 0.8703 - val_precision: 0.1438 - val_recall: 1.0000 - val_f1_metric: 0.2367\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 858us/step - loss: 0.4049 - acc: 0.8615 - precision: 0.1365 - recall: 1.0066 - f1_metric: 0.2360 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1441 - val_recall: 1.0000 - val_f1_metric: 0.2373\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.5722 - acc: 0.8605 - precision: 0.1366 - recall: 1.0066 - f1_metric: 0.2369 - val_loss: 0.3854 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0000 - val_f1_metric: 0.2364\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4121 - acc: 0.8559 - precision: 0.1420 - recall: 1.0111 - f1_metric: 0.2453 - val_loss: 0.3848 - val_acc: 0.8703 - val_precision: 0.1431 - val_recall: 1.0000 - val_f1_metric: 0.2356\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4129 - acc: 0.8626 - precision: 0.1353 - recall: 1.0017 - f1_metric: 0.2330 - val_loss: 0.3841 - val_acc: 0.8703 - val_precision: 0.1427 - val_recall: 1.0048 - val_f1_metric: 0.2351\n",
      "Model 4 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 308.6640 - acc: 0.8418 - precision: 0.1362 - recall: 1.7148 - f1_metric: 0.2496 - val_loss: 20.0601 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 35.4475 - acc: 0.8490 - precision: 0.1420 - recall: 1.7817 - f1_metric: 0.2595 - val_loss: 0.5171 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 922us/step - loss: 3.2712 - acc: 0.8557 - precision: 0.1383 - recall: 1.6931 - f1_metric: 0.2522 - val_loss: 0.4679 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 1.7161 - acc: 0.8634 - precision: 0.1365 - recall: 1.6699 - f1_metric: 0.2486 - val_loss: 0.4676 - val_acc: 0.8703 - val_precision: 0.1455 - val_recall: 2.0000 - val_f1_metric: 0.2608\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 1.1922 - acc: 0.8591 - precision: 0.1433 - recall: 1.7032 - f1_metric: 0.2613 - val_loss: 0.4346 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 2.0000 - val_f1_metric: 0.2658\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 862us/step - loss: 1.5171 - acc: 0.8534 - precision: 0.1539 - recall: 1.6746 - f1_metric: 0.2774 - val_loss: 0.4423 - val_acc: 0.8703 - val_precision: 0.1482 - val_recall: 2.0000 - val_f1_metric: 0.2654\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.7896 - acc: 0.8623 - precision: 0.1383 - recall: 1.5763 - f1_metric: 0.2502 - val_loss: 0.4335 - val_acc: 0.8703 - val_precision: 0.1492 - val_recall: 1.9808 - val_f1_metric: 0.2671\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.6277 - acc: 0.8552 - precision: 0.1434 - recall: 1.5559 - f1_metric: 0.2590 - val_loss: 0.4497 - val_acc: 0.8703 - val_precision: 0.1505 - val_recall: 1.9615 - val_f1_metric: 0.2691\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4969 - acc: 0.8606 - precision: 0.1346 - recall: 1.4182 - f1_metric: 0.2421 - val_loss: 0.4414 - val_acc: 0.8703 - val_precision: 0.1511 - val_recall: 1.9439 - val_f1_metric: 0.2700\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.5782 - acc: 0.8619 - precision: 0.1360 - recall: 1.4235 - f1_metric: 0.2438 - val_loss: 0.3978 - val_acc: 0.8703 - val_precision: 0.1535 - val_recall: 1.9314 - val_f1_metric: 0.2740\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 854us/step - loss: 0.5102 - acc: 0.8567 - precision: 0.1501 - recall: 1.4582 - f1_metric: 0.2672 - val_loss: 0.3989 - val_acc: 0.8703 - val_precision: 0.1523 - val_recall: 1.8221 - val_f1_metric: 0.2707\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.4724 - acc: 0.8587 - precision: 0.1469 - recall: 1.3757 - f1_metric: 0.2608 - val_loss: 0.3895 - val_acc: 0.8703 - val_precision: 0.1532 - val_recall: 1.7958 - val_f1_metric: 0.2720\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.5003 - acc: 0.8610 - precision: 0.1389 - recall: 1.3000 - f1_metric: 0.2467 - val_loss: 0.3942 - val_acc: 0.8703 - val_precision: 0.1544 - val_recall: 1.8054 - val_f1_metric: 0.2742\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.4669 - acc: 0.8604 - precision: 0.1377 - recall: 1.2326 - f1_metric: 0.2430 - val_loss: 0.3902 - val_acc: 0.8703 - val_precision: 0.1529 - val_recall: 1.7670 - val_f1_metric: 0.2713\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.5234 - acc: 0.8605 - precision: 0.1450 - recall: 1.2340 - f1_metric: 0.2553 - val_loss: 0.3882 - val_acc: 0.8703 - val_precision: 0.1591 - val_recall: 1.6660 - val_f1_metric: 0.2805\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 911us/step - loss: 0.4513 - acc: 0.8562 - precision: 0.1593 - recall: 1.1897 - f1_metric: 0.2740 - val_loss: 0.3846 - val_acc: 0.8703 - val_precision: 0.1533 - val_recall: 0.8851 - val_f1_metric: 0.2473\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.4363 - acc: 0.8594 - precision: 0.1450 - recall: 1.0328 - f1_metric: 0.2483 - val_loss: 0.3893 - val_acc: 0.8703 - val_precision: 0.1537 - val_recall: 0.9027 - val_f1_metric: 0.2483\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.4270 - acc: 0.8577 - precision: 0.1419 - recall: 0.9931 - f1_metric: 0.2408 - val_loss: 0.3857 - val_acc: 0.8703 - val_precision: 0.1536 - val_recall: 0.9027 - val_f1_metric: 0.2482\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.4021 - acc: 0.8649 - precision: 0.1448 - recall: 0.9849 - f1_metric: 0.2453 - val_loss: 0.3850 - val_acc: 0.8703 - val_precision: 0.1534 - val_recall: 0.9091 - val_f1_metric: 0.2482\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 996us/step - loss: 0.4742 - acc: 0.8618 - precision: 0.1437 - recall: 0.9596 - f1_metric: 0.2435 - val_loss: 0.3877 - val_acc: 0.8703 - val_precision: 0.1553 - val_recall: 0.9521 - val_f1_metric: 0.2524\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4281 - acc: 0.8569 - precision: 0.1515 - recall: 0.9944 - f1_metric: 0.2569 - val_loss: 0.3845 - val_acc: 0.8703 - val_precision: 0.1547 - val_recall: 0.9521 - val_f1_metric: 0.2517\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4071 - acc: 0.8611 - precision: 0.1387 - recall: 0.8978 - f1_metric: 0.2357 - val_loss: 0.3853 - val_acc: 0.8703 - val_precision: 0.1543 - val_recall: 0.9521 - val_f1_metric: 0.2510\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4188 - acc: 0.8557 - precision: 0.1435 - recall: 0.9374 - f1_metric: 0.2436 - val_loss: 0.3835 - val_acc: 0.8703 - val_precision: 0.1543 - val_recall: 0.9633 - val_f1_metric: 0.2516\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4265 - acc: 0.8587 - precision: 0.1439 - recall: 0.9349 - f1_metric: 0.2437 - val_loss: 0.3828 - val_acc: 0.8703 - val_precision: 0.1543 - val_recall: 0.9633 - val_f1_metric: 0.2516\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 972us/step - loss: 0.4390 - acc: 0.8570 - precision: 0.1496 - recall: 0.9373 - f1_metric: 0.2515 - val_loss: 0.3829 - val_acc: 0.8703 - val_precision: 0.1545 - val_recall: 0.9633 - val_f1_metric: 0.2518\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 959us/step - loss: 0.4104 - acc: 0.8598 - precision: 0.1469 - recall: 0.9409 - f1_metric: 0.2494 - val_loss: 0.3926 - val_acc: 0.8703 - val_precision: 0.1539 - val_recall: 0.9784 - val_f1_metric: 0.2516\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 920us/step - loss: 0.4139 - acc: 0.8563 - precision: 0.1460 - recall: 0.9388 - f1_metric: 0.2473 - val_loss: 0.3828 - val_acc: 0.8703 - val_precision: 0.1526 - val_recall: 0.9784 - val_f1_metric: 0.2496\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.4093 - acc: 0.8627 - precision: 0.1407 - recall: 0.9516 - f1_metric: 0.2400 - val_loss: 0.3838 - val_acc: 0.8703 - val_precision: 0.1517 - val_recall: 0.9784 - val_f1_metric: 0.2481\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4133 - acc: 0.8621 - precision: 0.1428 - recall: 0.9472 - f1_metric: 0.2421 - val_loss: 0.3844 - val_acc: 0.8703 - val_precision: 0.1512 - val_recall: 0.9808 - val_f1_metric: 0.2476\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4018 - acc: 0.8627 - precision: 0.1379 - recall: 0.9371 - f1_metric: 0.2358 - val_loss: 0.3850 - val_acc: 0.8703 - val_precision: 0.1509 - val_recall: 0.9808 - val_f1_metric: 0.2471\n",
      "Model 5 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 209.6041 - acc: 0.7761 - precision: 0.1399 - recall: 0.7119 - f1_metric: 0.2267 - val_loss: 7.6379 - val_acc: 0.8703 - val_precision: 0.1751 - val_recall: 0.5529 - val_f1_metric: 0.2484\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 974us/step - loss: 19.8385 - acc: 0.8311 - precision: 0.1473 - recall: 0.5953 - f1_metric: 0.2291 - val_loss: 0.6801 - val_acc: 0.8703 - val_precision: 0.1498 - val_recall: 0.9857 - val_f1_metric: 0.2453\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 950us/step - loss: 3.1505 - acc: 0.8389 - precision: 0.1514 - recall: 0.8449 - f1_metric: 0.2498 - val_loss: 0.5491 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 1.0920 - val_f1_metric: 0.2459\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.9876 - acc: 0.8497 - precision: 0.1418 - recall: 0.9245 - f1_metric: 0.2413 - val_loss: 0.4556 - val_acc: 0.8703 - val_precision: 0.1475 - val_recall: 1.1465 - val_f1_metric: 0.2461\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.7452 - acc: 0.8574 - precision: 0.1425 - recall: 1.0070 - f1_metric: 0.2419 - val_loss: 0.4159 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.2018 - val_f1_metric: 0.2463\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.7829 - acc: 0.8518 - precision: 0.1565 - recall: 1.0905 - f1_metric: 0.2692 - val_loss: 0.4073 - val_acc: 0.8703 - val_precision: 0.1480 - val_recall: 1.2869 - val_f1_metric: 0.2497\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.5095 - acc: 0.8543 - precision: 0.1476 - recall: 1.1037 - f1_metric: 0.2553 - val_loss: 0.3947 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.2153 - val_f1_metric: 0.2443\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4741 - acc: 0.8544 - precision: 0.1417 - recall: 1.0591 - f1_metric: 0.2450 - val_loss: 0.3900 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.1470 - val_f1_metric: 0.2405\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.4897 - acc: 0.8599 - precision: 0.1349 - recall: 1.0496 - f1_metric: 0.2327 - val_loss: 0.3855 - val_acc: 0.8703 - val_precision: 0.1443 - val_recall: 1.1349 - val_f1_metric: 0.2407\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.5847 - acc: 0.8614 - precision: 0.1399 - recall: 1.1531 - f1_metric: 0.2451 - val_loss: 0.3913 - val_acc: 0.8703 - val_precision: 0.1442 - val_recall: 1.1285 - val_f1_metric: 0.2405\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 961us/step - loss: 0.5106 - acc: 0.8563 - precision: 0.1491 - recall: 1.1658 - f1_metric: 0.2582 - val_loss: 0.3845 - val_acc: 0.8703 - val_precision: 0.1472 - val_recall: 1.0957 - val_f1_metric: 0.2445\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.4293 - acc: 0.8565 - precision: 0.1412 - recall: 1.0581 - f1_metric: 0.2430 - val_loss: 0.3873 - val_acc: 0.8703 - val_precision: 0.1477 - val_recall: 1.0957 - val_f1_metric: 0.2453\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4797 - acc: 0.8575 - precision: 0.1474 - recall: 1.1361 - f1_metric: 0.2559 - val_loss: 0.3830 - val_acc: 0.8703 - val_precision: 0.1490 - val_recall: 1.0764 - val_f1_metric: 0.2467\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4200 - acc: 0.8626 - precision: 0.1407 - recall: 1.0723 - f1_metric: 0.2440 - val_loss: 0.3853 - val_acc: 0.8703 - val_precision: 0.1473 - val_recall: 1.0588 - val_f1_metric: 0.2439\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.4273 - acc: 0.8579 - precision: 0.1460 - recall: 1.1132 - f1_metric: 0.2540 - val_loss: 0.3838 - val_acc: 0.8703 - val_precision: 0.1450 - val_recall: 1.0133 - val_f1_metric: 0.2393\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.4227 - acc: 0.8656 - precision: 0.1362 - recall: 1.0630 - f1_metric: 0.2366 - val_loss: 0.3840 - val_acc: 0.8703 - val_precision: 0.1464 - val_recall: 1.0223 - val_f1_metric: 0.2417\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4021 - acc: 0.8654 - precision: 0.1407 - recall: 1.0832 - f1_metric: 0.2438 - val_loss: 0.3836 - val_acc: 0.8703 - val_precision: 0.1455 - val_recall: 0.9896 - val_f1_metric: 0.2393\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4900 - acc: 0.8549 - precision: 0.1460 - recall: 1.0478 - f1_metric: 0.2520 - val_loss: 0.3829 - val_acc: 0.8703 - val_precision: 0.1469 - val_recall: 0.9960 - val_f1_metric: 0.2415\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4169 - acc: 0.8607 - precision: 0.1382 - recall: 0.9853 - f1_metric: 0.2378 - val_loss: 0.3844 - val_acc: 0.8703 - val_precision: 0.1456 - val_recall: 0.9960 - val_f1_metric: 0.2395\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4112 - acc: 0.8628 - precision: 0.1364 - recall: 0.9905 - f1_metric: 0.2334 - val_loss: 0.3844 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 0.9960 - val_f1_metric: 0.2399\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4228 - acc: 0.8550 - precision: 0.1470 - recall: 1.0492 - f1_metric: 0.2534 - val_loss: 0.3839 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2392\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4004 - acc: 0.8653 - precision: 0.1375 - recall: 1.0494 - f1_metric: 0.2389 - val_loss: 0.3845 - val_acc: 0.8703 - val_precision: 0.1461 - val_recall: 1.0000 - val_f1_metric: 0.2404\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4172 - acc: 0.8548 - precision: 0.1496 - recall: 1.0563 - f1_metric: 0.2575 - val_loss: 0.3848 - val_acc: 0.8703 - val_precision: 0.1466 - val_recall: 1.0000 - val_f1_metric: 0.2411\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4176 - acc: 0.8617 - precision: 0.1383 - recall: 1.0118 - f1_metric: 0.2395 - val_loss: 0.3837 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.0000 - val_f1_metric: 0.2415\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4568 - acc: 0.8549 - precision: 0.1500 - recall: 1.0521 - f1_metric: 0.2571 - val_loss: 0.3853 - val_acc: 0.8703 - val_precision: 0.1466 - val_recall: 1.0192 - val_f1_metric: 0.2417\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4191 - acc: 0.8533 - precision: 0.1475 - recall: 1.0632 - f1_metric: 0.2549 - val_loss: 0.3834 - val_acc: 0.8703 - val_precision: 0.1471 - val_recall: 0.9936 - val_f1_metric: 0.2418\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4101 - acc: 0.8631 - precision: 0.1382 - recall: 1.0147 - f1_metric: 0.2390 - val_loss: 0.3850 - val_acc: 0.8703 - val_precision: 0.1487 - val_recall: 0.9904 - val_f1_metric: 0.2441\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.4366 - acc: 0.8564 - precision: 0.1484 - recall: 0.9918 - f1_metric: 0.2522 - val_loss: 0.3845 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 0.9904 - val_f1_metric: 0.2438\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4029 - acc: 0.8610 - precision: 0.1437 - recall: 0.9553 - f1_metric: 0.2440 - val_loss: 0.3847 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 0.9904 - val_f1_metric: 0.2435\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.3967 - acc: 0.8657 - precision: 0.1382 - recall: 1.0072 - f1_metric: 0.2375 - val_loss: 0.3841 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 0.9904 - val_f1_metric: 0.2439\n",
      "Model 6 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 207.2778 - acc: 0.7906 - precision: 0.1419 - recall: 1.0189 - f1_metric: 0.2439 - val_loss: 0.6159 - val_acc: 0.8703 - val_precision: 0.1473 - val_recall: 0.9920 - val_f1_metric: 0.2420\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 942us/step - loss: 4.0418 - acc: 0.8500 - precision: 0.1411 - recall: 0.8813 - f1_metric: 0.2376 - val_loss: 0.4939 - val_acc: 0.8703 - val_precision: 0.1488 - val_recall: 0.9872 - val_f1_metric: 0.2441\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 1.4274 - acc: 0.8571 - precision: 0.1348 - recall: 0.8113 - f1_metric: 0.2257 - val_loss: 0.4389 - val_acc: 0.8703 - val_precision: 0.1490 - val_recall: 0.9872 - val_f1_metric: 0.2445\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.9331 - acc: 0.8617 - precision: 0.1407 - recall: 0.8531 - f1_metric: 0.2354 - val_loss: 0.4129 - val_acc: 0.8703 - val_precision: 0.1494 - val_recall: 0.9872 - val_f1_metric: 0.2451\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.8859 - acc: 0.8581 - precision: 0.1432 - recall: 0.8380 - f1_metric: 0.2384 - val_loss: 0.3999 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 0.9904 - val_f1_metric: 0.2438\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.4982 - acc: 0.8498 - precision: 0.1480 - recall: 0.7998 - f1_metric: 0.2445 - val_loss: 0.3923 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 0.9904 - val_f1_metric: 0.2437\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.5571 - acc: 0.8533 - precision: 0.1487 - recall: 0.8554 - f1_metric: 0.2486 - val_loss: 0.3891 - val_acc: 0.8703 - val_precision: 0.1487 - val_recall: 0.9904 - val_f1_metric: 0.2441\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 930us/step - loss: 0.5031 - acc: 0.8596 - precision: 0.1412 - recall: 0.8350 - f1_metric: 0.2362 - val_loss: 0.3882 - val_acc: 0.8703 - val_precision: 0.1487 - val_recall: 0.9952 - val_f1_metric: 0.2443\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 857us/step - loss: 0.5523 - acc: 0.8581 - precision: 0.1440 - recall: 0.8791 - f1_metric: 0.2421 - val_loss: 0.3877 - val_acc: 0.8703 - val_precision: 0.1474 - val_recall: 1.0000 - val_f1_metric: 0.2424\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4544 - acc: 0.8508 - precision: 0.1487 - recall: 0.8702 - f1_metric: 0.2483 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1470 - val_recall: 1.0000 - val_f1_metric: 0.2418\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4118 - acc: 0.8642 - precision: 0.1358 - recall: 0.8873 - f1_metric: 0.2304 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1470 - val_recall: 1.0000 - val_f1_metric: 0.2418\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4261 - acc: 0.8568 - precision: 0.1466 - recall: 0.9176 - f1_metric: 0.2479 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1463 - val_recall: 1.0000 - val_f1_metric: 0.2406\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.4607 - acc: 0.8625 - precision: 0.1375 - recall: 0.9082 - f1_metric: 0.2328 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1462 - val_recall: 1.0000 - val_f1_metric: 0.2405\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.4269 - acc: 0.8621 - precision: 0.1384 - recall: 0.9120 - f1_metric: 0.2352 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1461 - val_recall: 1.0000 - val_f1_metric: 0.2404\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4908 - acc: 0.8524 - precision: 0.1489 - recall: 0.9554 - f1_metric: 0.2533 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4243 - acc: 0.8601 - precision: 0.1410 - recall: 0.9597 - f1_metric: 0.2403 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.4090 - acc: 0.8578 - precision: 0.1450 - recall: 0.9696 - f1_metric: 0.2475 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.4209 - acc: 0.8574 - precision: 0.1418 - recall: 0.9757 - f1_metric: 0.2419 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4020 - acc: 0.8661 - precision: 0.1347 - recall: 0.9763 - f1_metric: 0.2321 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.5900 - acc: 0.8500 - precision: 0.1504 - recall: 0.9722 - f1_metric: 0.2560 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4214 - acc: 0.8578 - precision: 0.1435 - recall: 0.9719 - f1_metric: 0.2453 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.4168 - acc: 0.8599 - precision: 0.1394 - recall: 0.9717 - f1_metric: 0.2396 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.3959 - acc: 0.8655 - precision: 0.1357 - recall: 0.9747 - f1_metric: 0.2343 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4016 - acc: 0.8618 - precision: 0.1392 - recall: 0.9924 - f1_metric: 0.2394 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4323 - acc: 0.8561 - precision: 0.1437 - recall: 0.9827 - f1_metric: 0.2457 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.6140 - acc: 0.8574 - precision: 0.1412 - recall: 0.9792 - f1_metric: 0.2427 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.4131 - acc: 0.8552 - precision: 0.1455 - recall: 0.9888 - f1_metric: 0.2486 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4182 - acc: 0.8528 - precision: 0.1481 - recall: 1.0010 - f1_metric: 0.2538 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.4016 - acc: 0.8618 - precision: 0.1386 - recall: 0.9950 - f1_metric: 0.2383 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3977 - acc: 0.8642 - precision: 0.1363 - recall: 0.9985 - f1_metric: 0.2364 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Model 7 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 85.1843 - acc: 0.8510 - precision: 0.1600 - recall: 0.1250 - f1_metric: 0.1104 - val_loss: 0.6562 - val_acc: 0.8703 - val_precision: 0.1542 - val_recall: 0.9027 - val_f1_metric: 0.2491\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 4.3147 - acc: 0.8347 - precision: 0.1536 - recall: 0.7102 - f1_metric: 0.2456 - val_loss: 0.5143 - val_acc: 0.8703 - val_precision: 0.1516 - val_recall: 0.9808 - val_f1_metric: 0.2482\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 1.3017 - acc: 0.8500 - precision: 0.1528 - recall: 0.8993 - f1_metric: 0.2539 - val_loss: 0.4523 - val_acc: 0.8703 - val_precision: 0.1497 - val_recall: 0.9872 - val_f1_metric: 0.2456\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.6983 - acc: 0.8522 - precision: 0.1467 - recall: 0.9248 - f1_metric: 0.2476 - val_loss: 0.4185 - val_acc: 0.8703 - val_precision: 0.1484 - val_recall: 0.9872 - val_f1_metric: 0.2436\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.9269 - acc: 0.8455 - precision: 0.1577 - recall: 0.9622 - f1_metric: 0.2655 - val_loss: 0.3978 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 0.9904 - val_f1_metric: 0.2435\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.6440 - acc: 0.8515 - precision: 0.1497 - recall: 0.9615 - f1_metric: 0.2522 - val_loss: 0.3889 - val_acc: 0.8703 - val_precision: 0.1482 - val_recall: 0.9904 - val_f1_metric: 0.2434\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 0.5580 - acc: 0.8631 - precision: 0.1407 - recall: 0.9830 - f1_metric: 0.2414 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1488 - val_recall: 1.0000 - val_f1_metric: 0.2446\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 866us/step - loss: 0.5329 - acc: 0.8552 - precision: 0.1463 - recall: 0.9610 - f1_metric: 0.2498 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1475 - val_recall: 1.0000 - val_f1_metric: 0.2425\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.4630 - acc: 0.8535 - precision: 0.1492 - recall: 0.9697 - f1_metric: 0.2544 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1473 - val_recall: 1.0000 - val_f1_metric: 0.2423\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4296 - acc: 0.8610 - precision: 0.1412 - recall: 0.9585 - f1_metric: 0.2415 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1471 - val_recall: 1.0000 - val_f1_metric: 0.2419\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4727 - acc: 0.8608 - precision: 0.1419 - recall: 0.9774 - f1_metric: 0.2434 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.0000 - val_f1_metric: 0.2415\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.4107 - acc: 0.8604 - precision: 0.1422 - recall: 0.9747 - f1_metric: 0.2436 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.0000 - val_f1_metric: 0.2415\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4366 - acc: 0.8595 - precision: 0.1429 - recall: 0.9616 - f1_metric: 0.2435 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1462 - val_recall: 1.0000 - val_f1_metric: 0.2405\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4592 - acc: 0.8577 - precision: 0.1426 - recall: 0.9832 - f1_metric: 0.2447 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1462 - val_recall: 1.0000 - val_f1_metric: 0.2405\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4247 - acc: 0.8577 - precision: 0.1459 - recall: 0.9606 - f1_metric: 0.2487 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1460 - val_recall: 1.0000 - val_f1_metric: 0.2403\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.4078 - acc: 0.8600 - precision: 0.1418 - recall: 0.9934 - f1_metric: 0.2434 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2400\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4115 - acc: 0.8629 - precision: 0.1384 - recall: 0.9787 - f1_metric: 0.2374 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1460 - val_recall: 1.0000 - val_f1_metric: 0.2402\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 863us/step - loss: 0.4116 - acc: 0.8576 - precision: 0.1433 - recall: 0.9731 - f1_metric: 0.2435 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1462 - val_recall: 1.0000 - val_f1_metric: 0.2405\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4175 - acc: 0.8597 - precision: 0.1410 - recall: 0.9545 - f1_metric: 0.2411 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1467 - val_recall: 1.0000 - val_f1_metric: 0.2413\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4052 - acc: 0.8618 - precision: 0.1392 - recall: 0.9706 - f1_metric: 0.2402 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1466 - val_recall: 1.0000 - val_f1_metric: 0.2411\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4005 - acc: 0.8629 - precision: 0.1407 - recall: 0.9824 - f1_metric: 0.2410 - val_loss: 0.3858 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.0000 - val_f1_metric: 0.2415\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4055 - acc: 0.8598 - precision: 0.1416 - recall: 0.9645 - f1_metric: 0.2431 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1469 - val_recall: 1.0000 - val_f1_metric: 0.2417\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4341 - acc: 0.8583 - precision: 0.1429 - recall: 0.9540 - f1_metric: 0.2441 - val_loss: 0.3855 - val_acc: 0.8703 - val_precision: 0.1473 - val_recall: 1.0000 - val_f1_metric: 0.2423\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4112 - acc: 0.8573 - precision: 0.1437 - recall: 0.9784 - f1_metric: 0.2458 - val_loss: 0.3855 - val_acc: 0.8703 - val_precision: 0.1475 - val_recall: 1.0000 - val_f1_metric: 0.2425\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.4173 - acc: 0.8530 - precision: 0.1503 - recall: 0.9941 - f1_metric: 0.2565 - val_loss: 0.3846 - val_acc: 0.8703 - val_precision: 0.1488 - val_recall: 1.0000 - val_f1_metric: 0.2446\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.3849 - acc: 0.8703 - precision: 0.1332 - recall: 0.9900 - f1_metric: 0.2296 - val_loss: 0.3851 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 0.9904 - val_f1_metric: 0.2435\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4103 - acc: 0.8616 - precision: 0.1417 - recall: 0.9757 - f1_metric: 0.2428 - val_loss: 0.3847 - val_acc: 0.8703 - val_precision: 0.1484 - val_recall: 0.9904 - val_f1_metric: 0.2437\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4070 - acc: 0.8590 - precision: 0.1431 - recall: 0.9507 - f1_metric: 0.2446 - val_loss: 0.3839 - val_acc: 0.8703 - val_precision: 0.1488 - val_recall: 0.9872 - val_f1_metric: 0.2441\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 995us/step - loss: 0.3968 - acc: 0.8642 - precision: 0.1416 - recall: 0.9817 - f1_metric: 0.2437 - val_loss: 0.3870 - val_acc: 0.8703 - val_precision: 0.1484 - val_recall: 0.9904 - val_f1_metric: 0.2437\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4010 - acc: 0.8616 - precision: 0.1418 - recall: 0.9821 - f1_metric: 0.2438 - val_loss: 0.3848 - val_acc: 0.8703 - val_precision: 0.1488 - val_recall: 0.9872 - val_f1_metric: 0.2441\n",
      "Model 8 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 137.3138 - acc: 0.8580 - precision: 0.1404 - recall: 1.9287 - f1_metric: 0.2589 - val_loss: 4.7476 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 11.6948 - acc: 0.8600 - precision: 0.1351 - recall: 1.6280 - f1_metric: 0.2469 - val_loss: 0.4546 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 1.5953 - acc: 0.8550 - precision: 0.1469 - recall: 1.7183 - f1_metric: 0.2671 - val_loss: 0.4690 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.7609 - acc: 0.8565 - precision: 0.1423 - recall: 1.6550 - f1_metric: 0.2583 - val_loss: 0.4688 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.8196 - acc: 0.8520 - precision: 0.1520 - recall: 1.7050 - f1_metric: 0.2756 - val_loss: 0.5420 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.6240 - acc: 0.8555 - precision: 0.1455 - recall: 1.7061 - f1_metric: 0.2649 - val_loss: 0.4165 - val_acc: 0.8703 - val_precision: 0.1455 - val_recall: 2.0000 - val_f1_metric: 0.2608\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 912us/step - loss: 0.7223 - acc: 0.8621 - precision: 0.1372 - recall: 1.6520 - f1_metric: 0.2500 - val_loss: 0.4070 - val_acc: 0.8703 - val_precision: 0.1491 - val_recall: 1.9808 - val_f1_metric: 0.2669\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 925us/step - loss: 0.6984 - acc: 0.8619 - precision: 0.1363 - recall: 1.5595 - f1_metric: 0.2473 - val_loss: 0.4045 - val_acc: 0.8703 - val_precision: 0.1488 - val_recall: 1.9728 - val_f1_metric: 0.2663\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4806 - acc: 0.8604 - precision: 0.1418 - recall: 1.5957 - f1_metric: 0.2574 - val_loss: 0.4044 - val_acc: 0.8703 - val_precision: 0.1477 - val_recall: 1.9952 - val_f1_metric: 0.2646\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4563 - acc: 0.8551 - precision: 0.1481 - recall: 1.6109 - f1_metric: 0.2679 - val_loss: 0.3988 - val_acc: 0.8703 - val_precision: 0.1522 - val_recall: 1.9450 - val_f1_metric: 0.2719\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.5602 - acc: 0.8534 - precision: 0.1470 - recall: 1.4939 - f1_metric: 0.2635 - val_loss: 0.3937 - val_acc: 0.8703 - val_precision: 0.1525 - val_recall: 1.8426 - val_f1_metric: 0.2715\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 855us/step - loss: 0.4156 - acc: 0.8639 - precision: 0.1312 - recall: 1.3745 - f1_metric: 0.2360 - val_loss: 0.3900 - val_acc: 0.8703 - val_precision: 0.1537 - val_recall: 1.7323 - val_f1_metric: 0.2722\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4244 - acc: 0.8595 - precision: 0.1414 - recall: 1.4434 - f1_metric: 0.2538 - val_loss: 0.3884 - val_acc: 0.8703 - val_precision: 0.1598 - val_recall: 1.3854 - val_f1_metric: 0.2711\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 860us/step - loss: 0.4173 - acc: 0.8625 - precision: 0.1420 - recall: 1.4198 - f1_metric: 0.2546 - val_loss: 0.3875 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.5814 - acc: 0.8594 - precision: 0.1429 - recall: 1.2640 - f1_metric: 0.2534 - val_loss: 0.3873 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4267 - acc: 0.8609 - precision: 0.1421 - recall: 1.2398 - f1_metric: 0.2503 - val_loss: 0.3869 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4259 - acc: 0.8656 - precision: 0.1364 - recall: 1.2010 - f1_metric: 0.2399 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4154 - acc: 0.8568 - precision: 0.1450 - recall: 1.1580 - f1_metric: 0.2530 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.4187 - acc: 0.8599 - precision: 0.1408 - recall: 1.0864 - f1_metric: 0.2440 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4055 - acc: 0.8642 - precision: 0.1341 - recall: 1.0984 - f1_metric: 0.2357 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.4581 - acc: 0.8533 - precision: 0.1428 - recall: 1.0570 - f1_metric: 0.2474 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4192 - acc: 0.8531 - precision: 0.1485 - recall: 1.0719 - f1_metric: 0.2563 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 1000us/step - loss: 0.4410 - acc: 0.8538 - precision: 0.1444 - recall: 1.0408 - f1_metric: 0.2492 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4346 - acc: 0.8585 - precision: 0.1425 - recall: 1.0636 - f1_metric: 0.2479 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4222 - acc: 0.8524 - precision: 0.1484 - recall: 1.0269 - f1_metric: 0.2556 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4030 - acc: 0.8626 - precision: 0.1370 - recall: 0.9829 - f1_metric: 0.2356 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4157 - acc: 0.8557 - precision: 0.1417 - recall: 1.0005 - f1_metric: 0.2426 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4102 - acc: 0.8578 - precision: 0.1409 - recall: 1.0034 - f1_metric: 0.2424 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.4055 - acc: 0.8629 - precision: 0.1362 - recall: 1.0069 - f1_metric: 0.2353 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4224 - acc: 0.8503 - precision: 0.1489 - recall: 1.0011 - f1_metric: 0.2547 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Model 9 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 405.9912 - acc: 0.7347 - precision: 0.1359 - recall: 0.8029 - f1_metric: 0.2268 - val_loss: 17.4403 - val_acc: 0.8703 - val_precision: 0.0504 - val_recall: 0.1125 - val_f1_metric: 0.0651\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 866us/step - loss: 31.3803 - acc: 0.8101 - precision: 0.1257 - recall: 0.3578 - f1_metric: 0.1796 - val_loss: 0.5778 - val_acc: 0.8703 - val_precision: 0.0227 - val_recall: 0.0497 - val_f1_metric: 0.0299\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 4.6352 - acc: 0.8486 - precision: 0.0961 - recall: 0.1265 - f1_metric: 0.1002 - val_loss: 0.4895 - val_acc: 0.8703 - val_precision: 0.0168 - val_recall: 0.0321 - val_f1_metric: 0.0212\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 1.8986 - acc: 0.8519 - precision: 0.0971 - recall: 0.1049 - f1_metric: 0.0877 - val_loss: 0.4721 - val_acc: 0.8703 - val_precision: 0.0124 - val_recall: 0.0224 - val_f1_metric: 0.0153\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 1.7232 - acc: 0.8463 - precision: 0.1178 - recall: 0.1559 - f1_metric: 0.1256 - val_loss: 0.4164 - val_acc: 0.8703 - val_precision: 0.0064 - val_recall: 0.0096 - val_f1_metric: 0.0077\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 1.2892 - acc: 0.8641 - precision: 0.0960 - recall: 0.1820 - f1_metric: 0.1192 - val_loss: 0.4166 - val_acc: 0.8703 - val_precision: 0.0247 - val_recall: 0.0232 - val_f1_metric: 0.0206\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.6968 - acc: 0.8545 - precision: 0.1225 - recall: 0.2776 - f1_metric: 0.1618 - val_loss: 0.4191 - val_acc: 0.8703 - val_precision: 0.0827 - val_recall: 0.1221 - val_f1_metric: 0.0887\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 938us/step - loss: 0.6212 - acc: 0.8594 - precision: 0.1168 - recall: 0.3461 - f1_metric: 0.1676 - val_loss: 0.3961 - val_acc: 0.8703 - val_precision: 0.0764 - val_recall: 0.1686 - val_f1_metric: 0.0971\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6434 - acc: 0.8567 - precision: 0.1276 - recall: 0.4274 - f1_metric: 0.1910 - val_loss: 0.3914 - val_acc: 0.8703 - val_precision: 0.1135 - val_recall: 0.4693 - val_f1_metric: 0.1647\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.5129 - acc: 0.8525 - precision: 0.1464 - recall: 0.6010 - f1_metric: 0.2280 - val_loss: 0.3877 - val_acc: 0.8703 - val_precision: 0.1449 - val_recall: 0.9992 - val_f1_metric: 0.2385\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.5270 - acc: 0.8631 - precision: 0.1303 - recall: 0.7079 - f1_metric: 0.2151 - val_loss: 0.3840 - val_acc: 0.8703 - val_precision: 0.1449 - val_recall: 1.0120 - val_f1_metric: 0.2388\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.5077 - acc: 0.8523 - precision: 0.1517 - recall: 0.7456 - f1_metric: 0.2460 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1444 - val_recall: 1.0216 - val_f1_metric: 0.2383\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.5471 - acc: 0.8487 - precision: 0.1414 - recall: 0.7610 - f1_metric: 0.2337 - val_loss: 0.3850 - val_acc: 0.8703 - val_precision: 0.1423 - val_recall: 1.0192 - val_f1_metric: 0.2349\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 857us/step - loss: 0.4921 - acc: 0.8545 - precision: 0.1414 - recall: 0.8397 - f1_metric: 0.2363 - val_loss: 0.3858 - val_acc: 0.8703 - val_precision: 0.1426 - val_recall: 1.0192 - val_f1_metric: 0.2353\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 859us/step - loss: 0.4343 - acc: 0.8615 - precision: 0.1343 - recall: 0.9141 - f1_metric: 0.2293 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1413 - val_recall: 1.0192 - val_f1_metric: 0.2334\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4320 - acc: 0.8614 - precision: 0.1347 - recall: 0.9587 - f1_metric: 0.2318 - val_loss: 0.3843 - val_acc: 0.8703 - val_precision: 0.1415 - val_recall: 1.0192 - val_f1_metric: 0.2336\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 958us/step - loss: 0.4279 - acc: 0.8601 - precision: 0.1443 - recall: 1.0155 - f1_metric: 0.2485 - val_loss: 0.3856 - val_acc: 0.8703 - val_precision: 0.1412 - val_recall: 1.0128 - val_f1_metric: 0.2330\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.4278 - acc: 0.8544 - precision: 0.1406 - recall: 0.9777 - f1_metric: 0.2418 - val_loss: 0.3837 - val_acc: 0.8703 - val_precision: 0.1423 - val_recall: 1.0128 - val_f1_metric: 0.2347\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4406 - acc: 0.8593 - precision: 0.1401 - recall: 1.0293 - f1_metric: 0.2413 - val_loss: 0.3838 - val_acc: 0.8703 - val_precision: 0.1415 - val_recall: 1.0128 - val_f1_metric: 0.2334\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.4637 - acc: 0.8592 - precision: 0.1346 - recall: 0.9796 - f1_metric: 0.2318 - val_loss: 0.3872 - val_acc: 0.8703 - val_precision: 0.1418 - val_recall: 1.0128 - val_f1_metric: 0.2340\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4175 - acc: 0.8548 - precision: 0.1444 - recall: 0.9969 - f1_metric: 0.2470 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1422 - val_recall: 1.0128 - val_f1_metric: 0.2346\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.4361 - acc: 0.8579 - precision: 0.1417 - recall: 1.0166 - f1_metric: 0.2439 - val_loss: 0.3836 - val_acc: 0.8703 - val_precision: 0.1421 - val_recall: 1.0128 - val_f1_metric: 0.2345\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4131 - acc: 0.8633 - precision: 0.1373 - recall: 0.9973 - f1_metric: 0.2364 - val_loss: 0.3855 - val_acc: 0.8703 - val_precision: 0.1431 - val_recall: 1.0048 - val_f1_metric: 0.2358\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.4372 - acc: 0.8621 - precision: 0.1383 - recall: 1.0146 - f1_metric: 0.2379 - val_loss: 0.3852 - val_acc: 0.8703 - val_precision: 0.1438 - val_recall: 1.0096 - val_f1_metric: 0.2369\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.4159 - acc: 0.8598 - precision: 0.1411 - recall: 1.0031 - f1_metric: 0.2437 - val_loss: 0.3885 - val_acc: 0.8703 - val_precision: 0.1447 - val_recall: 1.0096 - val_f1_metric: 0.2384\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4158 - acc: 0.8653 - precision: 0.1347 - recall: 0.9651 - f1_metric: 0.2317 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1447 - val_recall: 1.0048 - val_f1_metric: 0.2383\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.3999 - acc: 0.8644 - precision: 0.1364 - recall: 0.9952 - f1_metric: 0.2359 - val_loss: 0.3854 - val_acc: 0.8703 - val_precision: 0.1487 - val_recall: 1.0000 - val_f1_metric: 0.2445\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.4005 - acc: 0.8643 - precision: 0.1341 - recall: 0.9871 - f1_metric: 0.2318 - val_loss: 0.3854 - val_acc: 0.8703 - val_precision: 0.1469 - val_recall: 1.0000 - val_f1_metric: 0.2417\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4061 - acc: 0.8610 - precision: 0.1392 - recall: 0.9688 - f1_metric: 0.2381 - val_loss: 0.3856 - val_acc: 0.8703 - val_precision: 0.1472 - val_recall: 1.0000 - val_f1_metric: 0.2420\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4148 - acc: 0.8549 - precision: 0.1459 - recall: 0.9974 - f1_metric: 0.2502 - val_loss: 0.3847 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 0.9904 - val_f1_metric: 0.2435\n",
      "Model 10 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 129.5939 - acc: 0.8090 - precision: 0.1368 - recall: 1.3646 - f1_metric: 0.2436 - val_loss: 0.6285 - val_acc: 0.8703 - val_precision: 0.1358 - val_recall: 1.1574 - val_f1_metric: 0.2277\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 7.7638 - acc: 0.8376 - precision: 0.1484 - recall: 1.4148 - f1_metric: 0.2637 - val_loss: 0.4847 - val_acc: 0.8703 - val_precision: 0.1339 - val_recall: 1.1902 - val_f1_metric: 0.2251\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 1.5598 - acc: 0.8505 - precision: 0.1455 - recall: 1.4005 - f1_metric: 0.2599 - val_loss: 0.4352 - val_acc: 0.8703 - val_precision: 0.1338 - val_recall: 1.1902 - val_f1_metric: 0.2250\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.9488 - acc: 0.8643 - precision: 0.1312 - recall: 1.3457 - f1_metric: 0.2356 - val_loss: 0.4216 - val_acc: 0.8703 - val_precision: 0.1350 - val_recall: 1.1782 - val_f1_metric: 0.2266\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.6954 - acc: 0.8519 - precision: 0.1419 - recall: 1.3272 - f1_metric: 0.2537 - val_loss: 0.3968 - val_acc: 0.8703 - val_precision: 0.1352 - val_recall: 1.1734 - val_f1_metric: 0.2269\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.5023 - acc: 0.8581 - precision: 0.1372 - recall: 1.2839 - f1_metric: 0.2437 - val_loss: 0.3987 - val_acc: 0.8703 - val_precision: 0.1372 - val_recall: 1.1574 - val_f1_metric: 0.2299\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 939us/step - loss: 0.5240 - acc: 0.8581 - precision: 0.1340 - recall: 1.2509 - f1_metric: 0.2384 - val_loss: 0.3889 - val_acc: 0.8703 - val_precision: 0.1389 - val_recall: 1.1245 - val_f1_metric: 0.2319\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4723 - acc: 0.8567 - precision: 0.1385 - recall: 1.2491 - f1_metric: 0.2458 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1384 - val_recall: 1.1021 - val_f1_metric: 0.2308\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.5002 - acc: 0.8542 - precision: 0.1376 - recall: 1.1598 - f1_metric: 0.2427 - val_loss: 0.3841 - val_acc: 0.8703 - val_precision: 0.1386 - val_recall: 1.0909 - val_f1_metric: 0.2309\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4529 - acc: 0.8560 - precision: 0.1383 - recall: 1.1469 - f1_metric: 0.2428 - val_loss: 0.3850 - val_acc: 0.8703 - val_precision: 0.1372 - val_recall: 1.0518 - val_f1_metric: 0.2279\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 961us/step - loss: 0.4441 - acc: 0.8548 - precision: 0.1407 - recall: 1.1531 - f1_metric: 0.2467 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1383 - val_recall: 1.0367 - val_f1_metric: 0.2291\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4289 - acc: 0.8529 - precision: 0.1432 - recall: 1.1101 - f1_metric: 0.2496 - val_loss: 0.3839 - val_acc: 0.8703 - val_precision: 0.1388 - val_recall: 1.0280 - val_f1_metric: 0.2296\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.4338 - acc: 0.8633 - precision: 0.1300 - recall: 1.0821 - f1_metric: 0.2283 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1386 - val_recall: 1.0280 - val_f1_metric: 0.2294\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4291 - acc: 0.8577 - precision: 0.1365 - recall: 1.0997 - f1_metric: 0.2380 - val_loss: 0.3838 - val_acc: 0.8703 - val_precision: 0.1396 - val_recall: 1.0216 - val_f1_metric: 0.2307\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 857us/step - loss: 0.4117 - acc: 0.8602 - precision: 0.1366 - recall: 1.0705 - f1_metric: 0.2384 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1410 - val_recall: 1.0192 - val_f1_metric: 0.2329\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.4101 - acc: 0.8576 - precision: 0.1381 - recall: 1.0459 - f1_metric: 0.2388 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1415 - val_recall: 1.0192 - val_f1_metric: 0.2336\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.4201 - acc: 0.8612 - precision: 0.1348 - recall: 1.0163 - f1_metric: 0.2340 - val_loss: 0.3858 - val_acc: 0.8703 - val_precision: 0.1415 - val_recall: 1.0192 - val_f1_metric: 0.2336\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4143 - acc: 0.8612 - precision: 0.1356 - recall: 1.0557 - f1_metric: 0.2361 - val_loss: 0.3870 - val_acc: 0.8703 - val_precision: 0.1415 - val_recall: 1.0192 - val_f1_metric: 0.2336\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.4242 - acc: 0.8567 - precision: 0.1402 - recall: 1.0476 - f1_metric: 0.2421 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1421 - val_recall: 1.0128 - val_f1_metric: 0.2344\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4242 - acc: 0.8538 - precision: 0.1442 - recall: 1.0293 - f1_metric: 0.2479 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0000 - val_f1_metric: 0.2365\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.4195 - acc: 0.8623 - precision: 0.1365 - recall: 1.0112 - f1_metric: 0.2359 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1438 - val_recall: 1.0000 - val_f1_metric: 0.2367\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.4190 - acc: 0.8564 - precision: 0.1408 - recall: 1.0118 - f1_metric: 0.2425 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0000 - val_f1_metric: 0.2365\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 0.4180 - acc: 0.8522 - precision: 0.1435 - recall: 1.0169 - f1_metric: 0.2472 - val_loss: 0.3854 - val_acc: 0.8703 - val_precision: 0.1428 - val_recall: 1.0096 - val_f1_metric: 0.2354\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 911us/step - loss: 0.4087 - acc: 0.8589 - precision: 0.1388 - recall: 1.0324 - f1_metric: 0.2400 - val_loss: 0.3857 - val_acc: 0.8703 - val_precision: 0.1426 - val_recall: 1.0048 - val_f1_metric: 0.2349\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4158 - acc: 0.8563 - precision: 0.1407 - recall: 1.0185 - f1_metric: 0.2433 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1426 - val_recall: 1.0048 - val_f1_metric: 0.2349\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4001 - acc: 0.8621 - precision: 0.1350 - recall: 1.0284 - f1_metric: 0.2346 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0000 - val_f1_metric: 0.2365\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4236 - acc: 0.8490 - precision: 0.1489 - recall: 1.0238 - f1_metric: 0.2560 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0000 - val_f1_metric: 0.2365\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4321 - acc: 0.8501 - precision: 0.1487 - recall: 1.0312 - f1_metric: 0.2546 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1448 - val_recall: 1.0000 - val_f1_metric: 0.2383\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.4021 - acc: 0.8613 - precision: 0.1375 - recall: 0.9983 - f1_metric: 0.2371 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1447 - val_recall: 1.0000 - val_f1_metric: 0.2381\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.3971 - acc: 0.8635 - precision: 0.1331 - recall: 1.0047 - f1_metric: 0.2317 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1428 - val_recall: 1.0096 - val_f1_metric: 0.2354\n",
      "Model 11 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 186.7389 - acc: 0.8502 - precision: 0.1411 - recall: 0.3573 - f1_metric: 0.1829 - val_loss: 6.5347 - val_acc: 0.8703 - val_precision: 0.0194 - val_recall: 0.0152 - val_f1_metric: 0.0155\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 14.8374 - acc: 0.7842 - precision: 0.1273 - recall: 0.6357 - f1_metric: 0.2060 - val_loss: 0.5884 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 1.7201 - acc: 0.8585 - precision: 0.1388 - recall: 0.8389 - f1_metric: 0.2326 - val_loss: 0.4822 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.8399 - acc: 0.8715 - precision: 0.1260 - recall: 0.8111 - f1_metric: 0.2123 - val_loss: 0.4329 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 947us/step - loss: 0.7786 - acc: 0.8596 - precision: 0.1412 - recall: 0.8438 - f1_metric: 0.2368 - val_loss: 0.4096 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.5333 - acc: 0.8556 - precision: 0.1477 - recall: 0.8998 - f1_metric: 0.2484 - val_loss: 0.3974 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.4721 - acc: 0.8664 - precision: 0.1356 - recall: 0.9043 - f1_metric: 0.2302 - val_loss: 0.3917 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.5756 - acc: 0.8657 - precision: 0.1356 - recall: 0.9128 - f1_metric: 0.2317 - val_loss: 0.3893 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.5243 - acc: 0.8518 - precision: 0.1503 - recall: 0.9109 - f1_metric: 0.2521 - val_loss: 0.3877 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.4558 - acc: 0.8551 - precision: 0.1477 - recall: 0.9062 - f1_metric: 0.2489 - val_loss: 0.3872 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.4147 - acc: 0.8623 - precision: 0.1389 - recall: 0.9637 - f1_metric: 0.2375 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.4589 - acc: 0.8557 - precision: 0.1453 - recall: 0.9516 - f1_metric: 0.2472 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.4279 - acc: 0.8582 - precision: 0.1433 - recall: 0.9670 - f1_metric: 0.2438 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4145 - acc: 0.8629 - precision: 0.1402 - recall: 0.9984 - f1_metric: 0.2395 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.4015 - acc: 0.8637 - precision: 0.1380 - recall: 0.9912 - f1_metric: 0.2374 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4140 - acc: 0.8569 - precision: 0.1443 - recall: 0.9733 - f1_metric: 0.2457 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 927us/step - loss: 0.4104 - acc: 0.8633 - precision: 0.1370 - recall: 0.9660 - f1_metric: 0.2351 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.4383 - acc: 0.8617 - precision: 0.1370 - recall: 0.9755 - f1_metric: 0.2364 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.4194 - acc: 0.8585 - precision: 0.1409 - recall: 0.9935 - f1_metric: 0.2422 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.4106 - acc: 0.8591 - precision: 0.1430 - recall: 1.0079 - f1_metric: 0.2451 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4196 - acc: 0.8530 - precision: 0.1450 - recall: 0.9822 - f1_metric: 0.2477 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4117 - acc: 0.8566 - precision: 0.1422 - recall: 0.9798 - f1_metric: 0.2440 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.4096 - acc: 0.8579 - precision: 0.1419 - recall: 0.9921 - f1_metric: 0.2439 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 864us/step - loss: 0.4021 - acc: 0.8639 - precision: 0.1351 - recall: 0.9906 - f1_metric: 0.2333 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 0.4081 - acc: 0.8596 - precision: 0.1392 - recall: 0.9891 - f1_metric: 0.2390 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.4210 - acc: 0.8599 - precision: 0.1393 - recall: 0.9745 - f1_metric: 0.2383 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4171 - acc: 0.8550 - precision: 0.1457 - recall: 1.0020 - f1_metric: 0.2500 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.3962 - acc: 0.8646 - precision: 0.1348 - recall: 0.9849 - f1_metric: 0.2326 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4072 - acc: 0.8595 - precision: 0.1408 - recall: 0.9955 - f1_metric: 0.2426 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4105 - acc: 0.8569 - precision: 0.1433 - recall: 0.9915 - f1_metric: 0.2445 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Model 12 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 249.6208 - acc: 0.8518 - precision: 0.1168 - recall: 0.2621 - f1_metric: 0.1410 - val_loss: 8.9602 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 16.9364 - acc: 0.8456 - precision: 0.1359 - recall: 0.1164 - f1_metric: 0.1147 - val_loss: 0.4623 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 2.2996 - acc: 0.8519 - precision: 0.1266 - recall: 0.1545 - f1_metric: 0.1150 - val_loss: 0.4219 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.9633 - acc: 0.8628 - precision: 0.1895 - recall: 0.1777 - f1_metric: 0.1637 - val_loss: 0.4701 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 1.1194 - acc: 0.8597 - precision: 0.1213 - recall: 0.1315 - f1_metric: 0.1124 - val_loss: 0.4084 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.7746 - acc: 0.8507 - precision: 0.0958 - recall: 0.0978 - f1_metric: 0.0882 - val_loss: 0.4433 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.6679 - acc: 0.8642 - precision: 0.0709 - recall: 0.0868 - f1_metric: 0.0716 - val_loss: 0.4477 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.6154 - acc: 0.8537 - precision: 0.1241 - recall: 0.1798 - f1_metric: 0.1350 - val_loss: 0.4318 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 1.0089 - acc: 0.8582 - precision: 0.1272 - recall: 0.1961 - f1_metric: 0.1435 - val_loss: 0.3883 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4839 - acc: 0.8602 - precision: 0.1525 - recall: 0.3495 - f1_metric: 0.2007 - val_loss: 0.4070 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4942 - acc: 0.8569 - precision: 0.1510 - recall: 0.4334 - f1_metric: 0.2124 - val_loss: 0.4070 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4829 - acc: 0.8486 - precision: 0.1448 - recall: 0.5411 - f1_metric: 0.2198 - val_loss: 0.3876 - val_acc: 0.8703 - val_precision: 0.1539 - val_recall: 0.8659 - val_f1_metric: 0.2475\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4479 - acc: 0.8610 - precision: 0.1396 - recall: 0.6057 - f1_metric: 0.2198 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1543 - val_recall: 0.8755 - val_f1_metric: 0.2485\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4468 - acc: 0.8625 - precision: 0.1311 - recall: 0.6709 - f1_metric: 0.2131 - val_loss: 0.3887 - val_acc: 0.8703 - val_precision: 0.1547 - val_recall: 0.9091 - val_f1_metric: 0.2501\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4616 - acc: 0.8611 - precision: 0.1396 - recall: 0.7413 - f1_metric: 0.2286 - val_loss: 0.3857 - val_acc: 0.8703 - val_precision: 0.1543 - val_recall: 0.8979 - val_f1_metric: 0.2491\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 925us/step - loss: 0.4284 - acc: 0.8585 - precision: 0.1495 - recall: 0.7754 - f1_metric: 0.2422 - val_loss: 0.3836 - val_acc: 0.8703 - val_precision: 0.1539 - val_recall: 0.9027 - val_f1_metric: 0.2487\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4112 - acc: 0.8640 - precision: 0.1353 - recall: 0.6875 - f1_metric: 0.2210 - val_loss: 0.3841 - val_acc: 0.8703 - val_precision: 0.1562 - val_recall: 0.9521 - val_f1_metric: 0.2539\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4500 - acc: 0.8658 - precision: 0.1323 - recall: 0.7636 - f1_metric: 0.2201 - val_loss: 0.3830 - val_acc: 0.8703 - val_precision: 0.1552 - val_recall: 0.9569 - val_f1_metric: 0.2526\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 860us/step - loss: 0.4385 - acc: 0.8498 - precision: 0.1552 - recall: 0.8397 - f1_metric: 0.2551 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1546 - val_recall: 0.9720 - val_f1_metric: 0.2524\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4161 - acc: 0.8626 - precision: 0.1449 - recall: 0.8944 - f1_metric: 0.2419 - val_loss: 0.3871 - val_acc: 0.8703 - val_precision: 0.1525 - val_recall: 0.9784 - val_f1_metric: 0.2495\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4214 - acc: 0.8565 - precision: 0.1368 - recall: 0.8481 - f1_metric: 0.2304 - val_loss: 0.3853 - val_acc: 0.8703 - val_precision: 0.1525 - val_recall: 0.9784 - val_f1_metric: 0.2495\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.4204 - acc: 0.8552 - precision: 0.1485 - recall: 0.8699 - f1_metric: 0.2478 - val_loss: 0.3851 - val_acc: 0.8703 - val_precision: 0.1512 - val_recall: 0.9808 - val_f1_metric: 0.2476\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.3918 - acc: 0.8667 - precision: 0.1344 - recall: 0.9522 - f1_metric: 0.2308 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1463 - val_recall: 0.9808 - val_f1_metric: 0.2400\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4006 - acc: 0.8635 - precision: 0.1356 - recall: 0.9335 - f1_metric: 0.2319 - val_loss: 0.3871 - val_acc: 0.8703 - val_precision: 0.1459 - val_recall: 0.9904 - val_f1_metric: 0.2398\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4113 - acc: 0.8570 - precision: 0.1427 - recall: 0.9288 - f1_metric: 0.2429 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1455 - val_recall: 0.9904 - val_f1_metric: 0.2391\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.4819 - acc: 0.8560 - precision: 0.1458 - recall: 0.9508 - f1_metric: 0.2480 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 0.9904 - val_f1_metric: 0.2390\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 931us/step - loss: 0.4256 - acc: 0.8622 - precision: 0.1378 - recall: 0.9534 - f1_metric: 0.2364 - val_loss: 0.3843 - val_acc: 0.8703 - val_precision: 0.1426 - val_recall: 1.0096 - val_f1_metric: 0.2351\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.4182 - acc: 0.8521 - precision: 0.1482 - recall: 0.9807 - f1_metric: 0.2518 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1429 - val_recall: 0.9904 - val_f1_metric: 0.2351\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4089 - acc: 0.8572 - precision: 0.1422 - recall: 0.9582 - f1_metric: 0.2432 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1426 - val_recall: 1.0048 - val_f1_metric: 0.2350\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.4313 - acc: 0.8470 - precision: 0.1499 - recall: 0.9700 - f1_metric: 0.2542 - val_loss: 0.3852 - val_acc: 0.8703 - val_precision: 0.1429 - val_recall: 0.9952 - val_f1_metric: 0.2352\n",
      "Model 13 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 114.9794 - acc: 0.8184 - precision: 0.1384 - recall: 0.5785 - f1_metric: 0.2105 - val_loss: 0.5678 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 2.8658 - acc: 0.8547 - precision: 0.1297 - recall: 0.3336 - f1_metric: 0.1758 - val_loss: 0.4961 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 1.3915 - acc: 0.8574 - precision: 0.1597 - recall: 0.2924 - f1_metric: 0.1916 - val_loss: 0.4455 - val_acc: 0.8703 - val_precision: 0.1927 - val_recall: 0.3671 - val_f1_metric: 0.2420\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 909us/step - loss: 0.5729 - acc: 0.8559 - precision: 0.1415 - recall: 0.3452 - f1_metric: 0.1889 - val_loss: 0.4277 - val_acc: 0.8703 - val_precision: 0.1648 - val_recall: 0.8006 - val_f1_metric: 0.2596\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.6445 - acc: 0.8528 - precision: 0.1608 - recall: 0.4759 - f1_metric: 0.2267 - val_loss: 0.4086 - val_acc: 0.8703 - val_precision: 0.1552 - val_recall: 0.9418 - val_f1_metric: 0.2520\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4732 - acc: 0.8595 - precision: 0.1316 - recall: 0.4830 - f1_metric: 0.1998 - val_loss: 0.3976 - val_acc: 0.8703 - val_precision: 0.1520 - val_recall: 0.9784 - val_f1_metric: 0.2486\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.5165 - acc: 0.8643 - precision: 0.1351 - recall: 0.5892 - f1_metric: 0.2138 - val_loss: 0.3960 - val_acc: 0.8703 - val_precision: 0.1471 - val_recall: 0.9808 - val_f1_metric: 0.2413\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4486 - acc: 0.8606 - precision: 0.1373 - recall: 0.7112 - f1_metric: 0.2230 - val_loss: 0.3879 - val_acc: 0.8703 - val_precision: 0.1443 - val_recall: 0.9872 - val_f1_metric: 0.2370\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 921us/step - loss: 0.5422 - acc: 0.8545 - precision: 0.1369 - recall: 0.7419 - f1_metric: 0.2252 - val_loss: 0.3855 - val_acc: 0.8703 - val_precision: 0.1423 - val_recall: 1.0128 - val_f1_metric: 0.2348\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4352 - acc: 0.8595 - precision: 0.1343 - recall: 0.8067 - f1_metric: 0.2251 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1423 - val_recall: 1.0128 - val_f1_metric: 0.2348\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4803 - acc: 0.8555 - precision: 0.1496 - recall: 0.9167 - f1_metric: 0.2529 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1422 - val_recall: 1.0000 - val_f1_metric: 0.2342\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 944us/step - loss: 0.5322 - acc: 0.8568 - precision: 0.1410 - recall: 0.8987 - f1_metric: 0.2387 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1423 - val_recall: 1.0000 - val_f1_metric: 0.2344\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4560 - acc: 0.8625 - precision: 0.1367 - recall: 0.9080 - f1_metric: 0.2338 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0000 - val_f1_metric: 0.2364\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4143 - acc: 0.8580 - precision: 0.1388 - recall: 0.8802 - f1_metric: 0.2345 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0000 - val_f1_metric: 0.2364\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4346 - acc: 0.8580 - precision: 0.1404 - recall: 0.9224 - f1_metric: 0.2396 - val_loss: 0.3855 - val_acc: 0.8703 - val_precision: 0.1427 - val_recall: 1.0000 - val_f1_metric: 0.2350\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.9087 - acc: 0.8655 - precision: 0.1321 - recall: 0.9154 - f1_metric: 0.2260 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1437 - val_recall: 1.0000 - val_f1_metric: 0.2366\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.4170 - acc: 0.8560 - precision: 0.1447 - recall: 0.9589 - f1_metric: 0.2452 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2371\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.3892 - acc: 0.8700 - precision: 0.1294 - recall: 0.9612 - f1_metric: 0.2221 - val_loss: 0.3856 - val_acc: 0.8703 - val_precision: 0.1427 - val_recall: 1.0000 - val_f1_metric: 0.2350\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4399 - acc: 0.8552 - precision: 0.1423 - recall: 0.9633 - f1_metric: 0.2430 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1446 - val_recall: 1.0000 - val_f1_metric: 0.2380\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.4131 - acc: 0.8574 - precision: 0.1422 - recall: 0.9628 - f1_metric: 0.2422 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1444 - val_recall: 1.0000 - val_f1_metric: 0.2377\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4158 - acc: 0.8535 - precision: 0.1489 - recall: 0.9762 - f1_metric: 0.2523 - val_loss: 0.3857 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.4104 - acc: 0.8581 - precision: 0.1399 - recall: 0.9650 - f1_metric: 0.2403 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0000 - val_f1_metric: 0.2364\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4183 - acc: 0.8519 - precision: 0.1464 - recall: 0.9730 - f1_metric: 0.2504 - val_loss: 0.3845 - val_acc: 0.8703 - val_precision: 0.1422 - val_recall: 1.0000 - val_f1_metric: 0.2342\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 934us/step - loss: 0.4325 - acc: 0.8592 - precision: 0.1382 - recall: 0.9668 - f1_metric: 0.2380 - val_loss: 0.3850 - val_acc: 0.8703 - val_precision: 0.1425 - val_recall: 1.0000 - val_f1_metric: 0.2347\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.4146 - acc: 0.8558 - precision: 0.1430 - recall: 0.9975 - f1_metric: 0.2455 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1439 - val_recall: 1.0000 - val_f1_metric: 0.2369\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.4082 - acc: 0.8590 - precision: 0.1379 - recall: 0.9901 - f1_metric: 0.2362 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1437 - val_recall: 1.0000 - val_f1_metric: 0.2366\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4044 - acc: 0.8600 - precision: 0.1381 - recall: 0.9881 - f1_metric: 0.2386 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1443 - val_recall: 1.0000 - val_f1_metric: 0.2375\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4007 - acc: 0.8636 - precision: 0.1347 - recall: 1.0056 - f1_metric: 0.2330 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1451 - val_recall: 1.0000 - val_f1_metric: 0.2388\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4044 - acc: 0.8631 - precision: 0.1342 - recall: 0.9878 - f1_metric: 0.2308 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1438 - val_recall: 1.0000 - val_f1_metric: 0.2367\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.3962 - acc: 0.8647 - precision: 0.1326 - recall: 0.9889 - f1_metric: 0.2291 - val_loss: 0.3869 - val_acc: 0.8703 - val_precision: 0.1439 - val_recall: 1.0000 - val_f1_metric: 0.2369\n",
      "Model 14 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 288.3678 - acc: 0.7967 - precision: 0.1365 - recall: 0.6565 - f1_metric: 0.2195 - val_loss: 0.8670 - val_acc: 0.8696 - val_precision: 0.1603 - val_recall: 1.4593 - val_f1_metric: 0.2793\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 12.9422 - acc: 0.8570 - precision: 0.1355 - recall: 1.0512 - f1_metric: 0.2354 - val_loss: 0.5850 - val_acc: 0.8703 - val_precision: 0.1473 - val_recall: 0.9904 - val_f1_metric: 0.2419\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 3.6933 - acc: 0.8463 - precision: 0.1508 - recall: 1.2080 - f1_metric: 0.2634 - val_loss: 0.4770 - val_acc: 0.8703 - val_precision: 0.1456 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 922us/step - loss: 1.4999 - acc: 0.8562 - precision: 0.1472 - recall: 1.1756 - f1_metric: 0.2568 - val_loss: 0.4198 - val_acc: 0.8703 - val_precision: 0.1456 - val_recall: 0.9952 - val_f1_metric: 0.2394\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 936us/step - loss: 1.6538 - acc: 0.8562 - precision: 0.1490 - recall: 1.1387 - f1_metric: 0.2585 - val_loss: 0.4001 - val_acc: 0.8703 - val_precision: 0.1479 - val_recall: 1.0000 - val_f1_metric: 0.2432\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 999us/step - loss: 0.6510 - acc: 0.8597 - precision: 0.1391 - recall: 1.0809 - f1_metric: 0.2416 - val_loss: 0.3940 - val_acc: 0.8703 - val_precision: 0.1469 - val_recall: 1.0000 - val_f1_metric: 0.2416\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 982us/step - loss: 0.6195 - acc: 0.8530 - precision: 0.1512 - recall: 1.0813 - f1_metric: 0.2606 - val_loss: 0.3903 - val_acc: 0.8703 - val_precision: 0.1460 - val_recall: 1.0000 - val_f1_metric: 0.2401\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 979us/step - loss: 0.7266 - acc: 0.8635 - precision: 0.1377 - recall: 1.0601 - f1_metric: 0.2394 - val_loss: 0.3886 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2398\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 990us/step - loss: 0.5985 - acc: 0.8565 - precision: 0.1442 - recall: 1.0424 - f1_metric: 0.2483 - val_loss: 0.3877 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 923us/step - loss: 0.5535 - acc: 0.8636 - precision: 0.1359 - recall: 0.9989 - f1_metric: 0.2343 - val_loss: 0.3872 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4440 - acc: 0.8604 - precision: 0.1402 - recall: 1.0087 - f1_metric: 0.2413 - val_loss: 0.3870 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.5116 - acc: 0.8519 - precision: 0.1506 - recall: 1.0356 - f1_metric: 0.2586 - val_loss: 0.3869 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.5064 - acc: 0.8600 - precision: 0.1416 - recall: 1.0291 - f1_metric: 0.2452 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 953us/step - loss: 0.4951 - acc: 0.8580 - precision: 0.1438 - recall: 1.0079 - f1_metric: 0.2480 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4097 - acc: 0.8658 - precision: 0.1332 - recall: 0.9896 - f1_metric: 0.2315 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4416 - acc: 0.8551 - precision: 0.1455 - recall: 0.9997 - f1_metric: 0.2492 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4525 - acc: 0.8579 - precision: 0.1425 - recall: 0.9732 - f1_metric: 0.2442 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 948us/step - loss: 0.4182 - acc: 0.8612 - precision: 0.1397 - recall: 0.9870 - f1_metric: 0.2401 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4413 - acc: 0.8488 - precision: 0.1516 - recall: 0.9926 - f1_metric: 0.2582 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4350 - acc: 0.8496 - precision: 0.1492 - recall: 0.9905 - f1_metric: 0.2524 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 973us/step - loss: 0.4104 - acc: 0.8614 - precision: 0.1383 - recall: 0.9635 - f1_metric: 0.2372 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4486 - acc: 0.8617 - precision: 0.1390 - recall: 0.9849 - f1_metric: 0.2395 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4097 - acc: 0.8605 - precision: 0.1391 - recall: 0.9909 - f1_metric: 0.2393 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4108 - acc: 0.8583 - precision: 0.1419 - recall: 0.9862 - f1_metric: 0.2413 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4092 - acc: 0.8594 - precision: 0.1410 - recall: 0.9919 - f1_metric: 0.2421 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 952us/step - loss: 0.4864 - acc: 0.8551 - precision: 0.1464 - recall: 0.9857 - f1_metric: 0.2509 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 918us/step - loss: 0.4370 - acc: 0.8592 - precision: 0.1414 - recall: 0.9829 - f1_metric: 0.2442 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 946us/step - loss: 0.3952 - acc: 0.8652 - precision: 0.1359 - recall: 0.9833 - f1_metric: 0.2342 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.3967 - acc: 0.8643 - precision: 0.1371 - recall: 0.9975 - f1_metric: 0.2360 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 993us/step - loss: 0.4136 - acc: 0.8598 - precision: 0.1410 - recall: 0.9901 - f1_metric: 0.2421 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Model 15 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 220.4570 - acc: 0.7909 - precision: 0.1603 - recall: 0.1980 - f1_metric: 0.1505 - val_loss: 2.1365 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 6.3115 - acc: 0.8445 - precision: 0.1396 - recall: 0.1151 - f1_metric: 0.1143 - val_loss: 0.5072 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 1.9414 - acc: 0.8498 - precision: 0.1506 - recall: 0.1359 - f1_metric: 0.1230 - val_loss: 0.4564 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.9584 - acc: 0.8582 - precision: 0.1107 - recall: 0.1437 - f1_metric: 0.1170 - val_loss: 0.5121 - val_acc: 0.8703 - val_precision: 0.0192 - val_recall: 0.0064 - val_f1_metric: 0.0096\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.9050 - acc: 0.8575 - precision: 0.1244 - recall: 0.1833 - f1_metric: 0.1394 - val_loss: 0.4119 - val_acc: 0.8703 - val_precision: 0.0304 - val_recall: 0.0120 - val_f1_metric: 0.0151\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6539 - acc: 0.8533 - precision: 0.1170 - recall: 0.2253 - f1_metric: 0.1416 - val_loss: 0.4540 - val_acc: 0.8703 - val_precision: 0.0349 - val_recall: 0.0319 - val_f1_metric: 0.0326\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6331 - acc: 0.8506 - precision: 0.1210 - recall: 0.3050 - f1_metric: 0.1670 - val_loss: 0.3984 - val_acc: 0.8703 - val_precision: 0.0703 - val_recall: 0.1021 - val_f1_metric: 0.0789\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4760 - acc: 0.8613 - precision: 0.1333 - recall: 0.4184 - f1_metric: 0.1941 - val_loss: 0.4116 - val_acc: 0.8703 - val_precision: 0.0707 - val_recall: 0.1574 - val_f1_metric: 0.0938\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.5222 - acc: 0.8567 - precision: 0.1321 - recall: 0.5053 - f1_metric: 0.2028 - val_loss: 0.3901 - val_acc: 0.8703 - val_precision: 0.1171 - val_recall: 0.4332 - val_f1_metric: 0.1664\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.5562 - acc: 0.8615 - precision: 0.1456 - recall: 0.6432 - f1_metric: 0.2312 - val_loss: 0.3886 - val_acc: 0.8703 - val_precision: 0.1478 - val_recall: 1.0000 - val_f1_metric: 0.2431\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.5223 - acc: 0.8584 - precision: 0.1393 - recall: 0.7407 - f1_metric: 0.2280 - val_loss: 0.3873 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4172 - acc: 0.8643 - precision: 0.1265 - recall: 0.7815 - f1_metric: 0.2125 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4996 - acc: 0.8584 - precision: 0.1452 - recall: 0.8745 - f1_metric: 0.2436 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4107 - acc: 0.8639 - precision: 0.1372 - recall: 0.8733 - f1_metric: 0.2322 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 937us/step - loss: 0.4718 - acc: 0.8494 - precision: 0.1535 - recall: 0.9128 - f1_metric: 0.2578 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4179 - acc: 0.8558 - precision: 0.1413 - recall: 0.8942 - f1_metric: 0.2391 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4073 - acc: 0.8621 - precision: 0.1382 - recall: 0.9232 - f1_metric: 0.2349 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4190 - acc: 0.8555 - precision: 0.1449 - recall: 0.9378 - f1_metric: 0.2455 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4129 - acc: 0.8565 - precision: 0.1439 - recall: 0.9567 - f1_metric: 0.2452 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.4268 - acc: 0.8542 - precision: 0.1458 - recall: 0.9529 - f1_metric: 0.2480 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.4274 - acc: 0.8583 - precision: 0.1417 - recall: 0.9641 - f1_metric: 0.2417 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 903us/step - loss: 0.4039 - acc: 0.8622 - precision: 0.1360 - recall: 0.9575 - f1_metric: 0.2330 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4006 - acc: 0.8635 - precision: 0.1371 - recall: 0.9810 - f1_metric: 0.2354 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4037 - acc: 0.8626 - precision: 0.1378 - recall: 0.9763 - f1_metric: 0.2368 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4113 - acc: 0.8578 - precision: 0.1426 - recall: 0.9819 - f1_metric: 0.2425 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4077 - acc: 0.8585 - precision: 0.1422 - recall: 0.9751 - f1_metric: 0.2426 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4017 - acc: 0.8619 - precision: 0.1361 - recall: 0.9710 - f1_metric: 0.2350 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4176 - acc: 0.8533 - precision: 0.1474 - recall: 0.9845 - f1_metric: 0.2499 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.4099 - acc: 0.8574 - precision: 0.1429 - recall: 0.9766 - f1_metric: 0.2439 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4141 - acc: 0.8555 - precision: 0.1456 - recall: 0.9940 - f1_metric: 0.2490 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Model 16 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 318.4002 - acc: 0.8486 - precision: 0.1299 - recall: 1.3239 - f1_metric: 0.2331 - val_loss: 7.9957 - val_acc: 0.8703 - val_precision: 0.1476 - val_recall: 1.9872 - val_f1_metric: 0.2644\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 35.3645 - acc: 0.8227 - precision: 0.1347 - recall: 1.3259 - f1_metric: 0.2404 - val_loss: 0.5541 - val_acc: 0.8703 - val_precision: 0.1462 - val_recall: 2.0000 - val_f1_metric: 0.2620\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 860us/step - loss: 4.2492 - acc: 0.8416 - precision: 0.1442 - recall: 1.5267 - f1_metric: 0.2599 - val_loss: 0.4429 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 930us/step - loss: 1.6286 - acc: 0.8630 - precision: 0.1357 - recall: 1.6448 - f1_metric: 0.2463 - val_loss: 0.4341 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 1.3570 - acc: 0.8578 - precision: 0.1423 - recall: 1.5711 - f1_metric: 0.2575 - val_loss: 0.4048 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 935us/step - loss: 1.5734 - acc: 0.8529 - precision: 0.1405 - recall: 1.4830 - f1_metric: 0.2535 - val_loss: 0.3993 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 2.0000 - val_f1_metric: 0.2607\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.7159 - acc: 0.8590 - precision: 0.1382 - recall: 1.4918 - f1_metric: 0.2493 - val_loss: 0.4022 - val_acc: 0.8703 - val_precision: 0.1338 - val_recall: 1.1998 - val_f1_metric: 0.2253\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.8794 - acc: 0.8515 - precision: 0.1461 - recall: 1.3449 - f1_metric: 0.2600 - val_loss: 0.3956 - val_acc: 0.8703 - val_precision: 0.1380 - val_recall: 1.0646 - val_f1_metric: 0.2295\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.4549 - acc: 0.8521 - precision: 0.1422 - recall: 1.2486 - f1_metric: 0.2503 - val_loss: 0.3916 - val_acc: 0.8703 - val_precision: 0.1389 - val_recall: 1.0216 - val_f1_metric: 0.2296\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4619 - acc: 0.8525 - precision: 0.1470 - recall: 1.3173 - f1_metric: 0.2606 - val_loss: 0.3890 - val_acc: 0.8703 - val_precision: 0.1415 - val_recall: 1.0192 - val_f1_metric: 0.2335\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 904us/step - loss: 0.5518 - acc: 0.8653 - precision: 0.1338 - recall: 1.2147 - f1_metric: 0.2364 - val_loss: 0.3887 - val_acc: 0.8703 - val_precision: 0.1435 - val_recall: 1.0000 - val_f1_metric: 0.2363\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 953us/step - loss: 0.4465 - acc: 0.8547 - precision: 0.1462 - recall: 1.2109 - f1_metric: 0.2563 - val_loss: 0.3873 - val_acc: 0.8703 - val_precision: 0.1455 - val_recall: 1.0000 - val_f1_metric: 0.2394\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 919us/step - loss: 0.5136 - acc: 0.8665 - precision: 0.1332 - recall: 1.1292 - f1_metric: 0.2338 - val_loss: 0.3874 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.0000 - val_f1_metric: 0.2415\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 942us/step - loss: 0.5262 - acc: 0.8524 - precision: 0.1387 - recall: 1.0186 - f1_metric: 0.2405 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.0000 - val_f1_metric: 0.2415\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4717 - acc: 0.8597 - precision: 0.1410 - recall: 1.1214 - f1_metric: 0.2457 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1463 - val_recall: 1.0000 - val_f1_metric: 0.2406\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4375 - acc: 0.8597 - precision: 0.1373 - recall: 1.0180 - f1_metric: 0.2372 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1460 - val_recall: 1.0000 - val_f1_metric: 0.2403\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 975us/step - loss: 0.4417 - acc: 0.8635 - precision: 0.1354 - recall: 1.0234 - f1_metric: 0.2344 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1460 - val_recall: 1.0000 - val_f1_metric: 0.2403\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 966us/step - loss: 0.4591 - acc: 0.8474 - precision: 0.1549 - recall: 1.0409 - f1_metric: 0.2636 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1460 - val_recall: 1.0000 - val_f1_metric: 0.2403\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4890 - acc: 0.8510 - precision: 0.1503 - recall: 1.0207 - f1_metric: 0.2585 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 895us/step - loss: 0.4414 - acc: 0.8566 - precision: 0.1457 - recall: 1.0214 - f1_metric: 0.2505 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4251 - acc: 0.8536 - precision: 0.1480 - recall: 1.0098 - f1_metric: 0.2530 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 875us/step - loss: 0.4171 - acc: 0.8622 - precision: 0.1397 - recall: 0.9990 - f1_metric: 0.2406 - val_loss: 0.3869 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.4114 - acc: 0.8586 - precision: 0.1407 - recall: 0.9645 - f1_metric: 0.2413 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 866us/step - loss: 0.4030 - acc: 0.8614 - precision: 0.1408 - recall: 0.9998 - f1_metric: 0.2416 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 876us/step - loss: 0.4158 - acc: 0.8551 - precision: 0.1438 - recall: 0.9634 - f1_metric: 0.2457 - val_loss: 0.3867 - val_acc: 0.8703 - val_precision: 0.1458 - val_recall: 1.0000 - val_f1_metric: 0.2399\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.4106 - acc: 0.8610 - precision: 0.1393 - recall: 0.9748 - f1_metric: 0.2390 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4051 - acc: 0.8651 - precision: 0.1363 - recall: 0.9747 - f1_metric: 0.2351 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4106 - acc: 0.8599 - precision: 0.1427 - recall: 0.9742 - f1_metric: 0.2449 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.4052 - acc: 0.8596 - precision: 0.1405 - recall: 0.9679 - f1_metric: 0.2402 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.4369 - acc: 0.8499 - precision: 0.1519 - recall: 0.9806 - f1_metric: 0.2580 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1457 - val_recall: 1.0000 - val_f1_metric: 0.2397\n",
      "Model 17 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 195.6270 - acc: 0.8096 - precision: 0.1511 - recall: 1.0320 - f1_metric: 0.2573 - val_loss: 8.5592 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 9.5604 - acc: 0.8285 - precision: 0.1404 - recall: 0.3511 - f1_metric: 0.1874 - val_loss: 0.4162 - val_acc: 0.8703 - val_precision: 0.2153 - val_recall: 0.7616 - val_f1_metric: 0.3233\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 2.0677 - acc: 0.8576 - precision: 0.1466 - recall: 0.6782 - f1_metric: 0.2290 - val_loss: 0.4622 - val_acc: 0.8703 - val_precision: 0.1647 - val_recall: 1.3476 - val_f1_metric: 0.2841\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.7070 - acc: 0.8511 - precision: 0.1477 - recall: 0.7144 - f1_metric: 0.2384 - val_loss: 0.4150 - val_acc: 0.8703 - val_precision: 0.1665 - val_recall: 1.5612 - val_f1_metric: 0.2909\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 942us/step - loss: 0.7759 - acc: 0.8572 - precision: 0.1532 - recall: 0.9333 - f1_metric: 0.2546 - val_loss: 0.3975 - val_acc: 0.8703 - val_precision: 0.1545 - val_recall: 1.7574 - val_f1_metric: 0.2739\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.5016 - acc: 0.8598 - precision: 0.1430 - recall: 1.0658 - f1_metric: 0.2471 - val_loss: 0.3972 - val_acc: 0.8703 - val_precision: 0.1541 - val_recall: 1.9567 - val_f1_metric: 0.2753\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 884us/step - loss: 0.5484 - acc: 0.8583 - precision: 0.1474 - recall: 1.2662 - f1_metric: 0.2600 - val_loss: 0.4014 - val_acc: 0.8703 - val_precision: 0.1523 - val_recall: 1.9567 - val_f1_metric: 0.2723\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.5763 - acc: 0.8646 - precision: 0.1408 - recall: 1.3551 - f1_metric: 0.2505 - val_loss: 0.3941 - val_acc: 0.8703 - val_precision: 0.1520 - val_recall: 1.9567 - val_f1_metric: 0.2716\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.7324 - acc: 0.8535 - precision: 0.1476 - recall: 1.3025 - f1_metric: 0.2612 - val_loss: 0.3895 - val_acc: 0.8703 - val_precision: 0.1521 - val_recall: 1.9567 - val_f1_metric: 0.2719\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.4451 - acc: 0.8627 - precision: 0.1383 - recall: 1.2672 - f1_metric: 0.2458 - val_loss: 0.3901 - val_acc: 0.8703 - val_precision: 0.1449 - val_recall: 1.1141 - val_f1_metric: 0.2413\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4302 - acc: 0.8562 - precision: 0.1462 - recall: 1.2474 - f1_metric: 0.2559 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1461 - val_recall: 1.0159 - val_f1_metric: 0.2410\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4494 - acc: 0.8575 - precision: 0.1428 - recall: 1.2312 - f1_metric: 0.2519 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1502 - val_recall: 0.9872 - val_f1_metric: 0.2464\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.5291 - acc: 0.8637 - precision: 0.1351 - recall: 1.1607 - f1_metric: 0.2376 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1489 - val_recall: 0.9872 - val_f1_metric: 0.2444\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4608 - acc: 0.8594 - precision: 0.1422 - recall: 1.1704 - f1_metric: 0.2490 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1502 - val_recall: 0.9872 - val_f1_metric: 0.2464\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 899us/step - loss: 0.4210 - acc: 0.8585 - precision: 0.1369 - recall: 1.0929 - f1_metric: 0.2393 - val_loss: 0.3871 - val_acc: 0.8703 - val_precision: 0.1489 - val_recall: 0.9872 - val_f1_metric: 0.2444\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4249 - acc: 0.8539 - precision: 0.1495 - recall: 1.1162 - f1_metric: 0.2579 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1489 - val_recall: 0.9904 - val_f1_metric: 0.2445\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4254 - acc: 0.8548 - precision: 0.1503 - recall: 1.1274 - f1_metric: 0.2596 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 0.9904 - val_f1_metric: 0.2436\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4153 - acc: 0.8602 - precision: 0.1390 - recall: 1.0631 - f1_metric: 0.2402 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1470 - val_recall: 1.0000 - val_f1_metric: 0.2418\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 0.4265 - acc: 0.8600 - precision: 0.1435 - recall: 1.0759 - f1_metric: 0.2483 - val_loss: 0.3855 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 0.9904 - val_f1_metric: 0.2439\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4086 - acc: 0.8608 - precision: 0.1399 - recall: 1.0127 - f1_metric: 0.2411 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1476 - val_recall: 1.0000 - val_f1_metric: 0.2427\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 869us/step - loss: 0.4129 - acc: 0.8665 - precision: 0.1383 - recall: 1.0398 - f1_metric: 0.2379 - val_loss: 0.3851 - val_acc: 0.8703 - val_precision: 0.1492 - val_recall: 0.9872 - val_f1_metric: 0.2448\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 866us/step - loss: 0.4183 - acc: 0.8527 - precision: 0.1482 - recall: 0.9797 - f1_metric: 0.2537 - val_loss: 0.3856 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 0.9904 - val_f1_metric: 0.2436\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.4038 - acc: 0.8632 - precision: 0.1411 - recall: 1.0112 - f1_metric: 0.2436 - val_loss: 0.3849 - val_acc: 0.8703 - val_precision: 0.1499 - val_recall: 0.9872 - val_f1_metric: 0.2459\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.4167 - acc: 0.8535 - precision: 0.1498 - recall: 0.9902 - f1_metric: 0.2546 - val_loss: 0.3836 - val_acc: 0.8703 - val_precision: 0.1511 - val_recall: 0.9808 - val_f1_metric: 0.2474\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4108 - acc: 0.8570 - precision: 0.1478 - recall: 0.9911 - f1_metric: 0.2522 - val_loss: 0.3847 - val_acc: 0.8703 - val_precision: 0.1500 - val_recall: 0.9872 - val_f1_metric: 0.2460\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 861us/step - loss: 0.3969 - acc: 0.8635 - precision: 0.1442 - recall: 1.0115 - f1_metric: 0.2474 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1493 - val_recall: 0.9872 - val_f1_metric: 0.2449\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4027 - acc: 0.8614 - precision: 0.1403 - recall: 0.9591 - f1_metric: 0.2400 - val_loss: 0.3842 - val_acc: 0.8703 - val_precision: 0.1499 - val_recall: 0.9872 - val_f1_metric: 0.2459\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.3952 - acc: 0.8664 - precision: 0.1355 - recall: 0.9708 - f1_metric: 0.2325 - val_loss: 0.3838 - val_acc: 0.8703 - val_precision: 0.1502 - val_recall: 0.9872 - val_f1_metric: 0.2464\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 873us/step - loss: 0.4138 - acc: 0.8552 - precision: 0.1490 - recall: 0.9771 - f1_metric: 0.2529 - val_loss: 0.3849 - val_acc: 0.8703 - val_precision: 0.1497 - val_recall: 0.9808 - val_f1_metric: 0.2454\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 917us/step - loss: 0.4019 - acc: 0.8617 - precision: 0.1426 - recall: 0.9625 - f1_metric: 0.2426 - val_loss: 0.3856 - val_acc: 0.8703 - val_precision: 0.1502 - val_recall: 0.9872 - val_f1_metric: 0.2464\n",
      "Model 18 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 98.1173 - acc: 0.8337 - precision: 0.1560 - recall: 0.7418 - f1_metric: 0.2500 - val_loss: 0.6660 - val_acc: 0.6206 - val_precision: 0.1696 - val_recall: 0.4711 - val_f1_metric: 0.2430\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 868us/step - loss: 5.9906 - acc: 0.8333 - precision: 0.1572 - recall: 1.0286 - f1_metric: 0.2655 - val_loss: 0.5289 - val_acc: 0.8703 - val_precision: 0.1571 - val_recall: 0.9482 - val_f1_metric: 0.2551\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 1.4313 - acc: 0.8502 - precision: 0.1471 - recall: 0.9453 - f1_metric: 0.2494 - val_loss: 0.4589 - val_acc: 0.8703 - val_precision: 0.1526 - val_recall: 0.9720 - val_f1_metric: 0.2494\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 910us/step - loss: 0.6666 - acc: 0.8577 - precision: 0.1419 - recall: 1.0400 - f1_metric: 0.2443 - val_loss: 0.4181 - val_acc: 0.8703 - val_precision: 0.1490 - val_recall: 0.9744 - val_f1_metric: 0.2440\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.7011 - acc: 0.8540 - precision: 0.1566 - recall: 1.1461 - f1_metric: 0.2708 - val_loss: 0.4046 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 0.9808 - val_f1_metric: 0.2432\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 0.5442 - acc: 0.8575 - precision: 0.1413 - recall: 1.0987 - f1_metric: 0.2455 - val_loss: 0.3972 - val_acc: 0.8703 - val_precision: 0.1493 - val_recall: 0.9808 - val_f1_metric: 0.2447\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 880us/step - loss: 0.5123 - acc: 0.8618 - precision: 0.1408 - recall: 1.1764 - f1_metric: 0.2466 - val_loss: 0.3940 - val_acc: 0.8703 - val_precision: 0.1470 - val_recall: 0.9872 - val_f1_metric: 0.2414\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 874us/step - loss: 0.5113 - acc: 0.8515 - precision: 0.1542 - recall: 1.2549 - f1_metric: 0.2705 - val_loss: 0.3888 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.0192 - val_f1_metric: 0.2419\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4734 - acc: 0.8569 - precision: 0.1422 - recall: 1.1901 - f1_metric: 0.2498 - val_loss: 0.3870 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 1.0128 - val_f1_metric: 0.2445\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4314 - acc: 0.8645 - precision: 0.1366 - recall: 1.1776 - f1_metric: 0.2412 - val_loss: 0.3871 - val_acc: 0.8703 - val_precision: 0.1474 - val_recall: 0.9904 - val_f1_metric: 0.2420\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 872us/step - loss: 0.4171 - acc: 0.8655 - precision: 0.1351 - recall: 1.1521 - f1_metric: 0.2381 - val_loss: 0.3875 - val_acc: 0.8703 - val_precision: 0.1486 - val_recall: 0.9872 - val_f1_metric: 0.2439\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 906us/step - loss: 0.5418 - acc: 0.8590 - precision: 0.1422 - recall: 1.1043 - f1_metric: 0.2477 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1502 - val_recall: 0.9808 - val_f1_metric: 0.2461\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4233 - acc: 0.8582 - precision: 0.1369 - recall: 1.0210 - f1_metric: 0.2368 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1499 - val_recall: 0.9808 - val_f1_metric: 0.2456\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.4015 - acc: 0.8657 - precision: 0.1361 - recall: 1.0919 - f1_metric: 0.2374 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1484 - val_recall: 0.9872 - val_f1_metric: 0.2436\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 890us/step - loss: 0.4280 - acc: 0.8558 - precision: 0.1506 - recall: 1.0911 - f1_metric: 0.2604 - val_loss: 0.3853 - val_acc: 0.8703 - val_precision: 0.1484 - val_recall: 0.9872 - val_f1_metric: 0.2436\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 888us/step - loss: 0.4252 - acc: 0.8576 - precision: 0.1456 - recall: 1.1067 - f1_metric: 0.2526 - val_loss: 0.3874 - val_acc: 0.8703 - val_precision: 0.1490 - val_recall: 1.0000 - val_f1_metric: 0.2449\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 913us/step - loss: 0.4147 - acc: 0.8583 - precision: 0.1430 - recall: 1.1044 - f1_metric: 0.2488 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1488 - val_recall: 1.0000 - val_f1_metric: 0.2446\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 887us/step - loss: 0.4322 - acc: 0.8627 - precision: 0.1414 - recall: 1.1076 - f1_metric: 0.2466 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1459 - val_recall: 1.0000 - val_f1_metric: 0.2400\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 877us/step - loss: 0.4114 - acc: 0.8579 - precision: 0.1428 - recall: 1.0710 - f1_metric: 0.2467 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1468 - val_recall: 1.0000 - val_f1_metric: 0.2414\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4339 - acc: 0.8572 - precision: 0.1457 - recall: 1.0874 - f1_metric: 0.2517 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1471 - val_recall: 1.0000 - val_f1_metric: 0.2420\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 871us/step - loss: 0.4070 - acc: 0.8617 - precision: 0.1414 - recall: 1.0521 - f1_metric: 0.2449 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1483 - val_recall: 1.0000 - val_f1_metric: 0.2438\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 902us/step - loss: 0.4161 - acc: 0.8537 - precision: 0.1469 - recall: 1.0632 - f1_metric: 0.2530 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 1.0000 - val_f1_metric: 0.2442\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 882us/step - loss: 0.4189 - acc: 0.8511 - precision: 0.1496 - recall: 1.0445 - f1_metric: 0.2568 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1486 - val_recall: 0.9952 - val_f1_metric: 0.2441\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 881us/step - loss: 0.4178 - acc: 0.8561 - precision: 0.1451 - recall: 1.0127 - f1_metric: 0.2495 - val_loss: 0.3843 - val_acc: 0.8703 - val_precision: 0.1486 - val_recall: 0.9904 - val_f1_metric: 0.2440\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 878us/step - loss: 0.4322 - acc: 0.8522 - precision: 0.1492 - recall: 1.0160 - f1_metric: 0.2548 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1473 - val_recall: 1.0000 - val_f1_metric: 0.2423\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 914us/step - loss: 0.4201 - acc: 0.8516 - precision: 0.1515 - recall: 1.0471 - f1_metric: 0.2609 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1472 - val_recall: 1.0000 - val_f1_metric: 0.2420\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 912us/step - loss: 0.4246 - acc: 0.8532 - precision: 0.1474 - recall: 1.0293 - f1_metric: 0.2523 - val_loss: 0.3837 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 0.9904 - val_f1_metric: 0.2438\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 870us/step - loss: 0.3993 - acc: 0.8651 - precision: 0.1344 - recall: 0.9824 - f1_metric: 0.2321 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1474 - val_recall: 1.0000 - val_f1_metric: 0.2424\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4055 - acc: 0.8593 - precision: 0.1433 - recall: 1.0409 - f1_metric: 0.2471 - val_loss: 0.3834 - val_acc: 0.8703 - val_precision: 0.1496 - val_recall: 0.9872 - val_f1_metric: 0.2453\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.4186 - acc: 0.8531 - precision: 0.1494 - recall: 0.9946 - f1_metric: 0.2548 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1472 - val_recall: 1.0000 - val_f1_metric: 0.2422\n",
      "Model 19 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 421.1362 - acc: 0.7080 - precision: 0.1453 - recall: 0.7165 - f1_metric: 0.2316 - val_loss: 31.0898 - val_acc: 0.8703 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1_metric: 0.0000e+00\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 892us/step - loss: 43.7166 - acc: 0.8049 - precision: 0.1581 - recall: 0.6187 - f1_metric: 0.2439 - val_loss: 0.8221 - val_acc: 0.8703 - val_precision: 0.1516 - val_recall: 0.9720 - val_f1_metric: 0.2480\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 865us/step - loss: 6.1716 - acc: 0.8287 - precision: 0.1533 - recall: 1.0346 - f1_metric: 0.2612 - val_loss: 0.5371 - val_acc: 0.8703 - val_precision: 0.1481 - val_recall: 0.9872 - val_f1_metric: 0.2431\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 3.0477 - acc: 0.8429 - precision: 0.1452 - recall: 1.0469 - f1_metric: 0.2492 - val_loss: 0.4565 - val_acc: 0.8703 - val_precision: 0.1478 - val_recall: 0.9968 - val_f1_metric: 0.2429\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 883us/step - loss: 2.3957 - acc: 0.8522 - precision: 0.1468 - recall: 1.0784 - f1_metric: 0.2539 - val_loss: 0.4224 - val_acc: 0.8703 - val_precision: 0.1472 - val_recall: 1.0000 - val_f1_metric: 0.2421\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 1.1677 - acc: 0.8548 - precision: 0.1437 - recall: 1.0290 - f1_metric: 0.2477 - val_loss: 0.4012 - val_acc: 0.8703 - val_precision: 0.1485 - val_recall: 1.0032 - val_f1_metric: 0.2441\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 891us/step - loss: 0.9215 - acc: 0.8587 - precision: 0.1385 - recall: 1.0257 - f1_metric: 0.2397 - val_loss: 0.3920 - val_acc: 0.8703 - val_precision: 0.1455 - val_recall: 1.0096 - val_f1_metric: 0.2397\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.8629 - acc: 0.8522 - precision: 0.1468 - recall: 1.0684 - f1_metric: 0.2550 - val_loss: 0.3884 - val_acc: 0.8703 - val_precision: 0.1452 - val_recall: 1.0096 - val_f1_metric: 0.2392\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.7177 - acc: 0.8535 - precision: 0.1441 - recall: 1.0533 - f1_metric: 0.2481 - val_loss: 0.3871 - val_acc: 0.8703 - val_precision: 0.1436 - val_recall: 1.0048 - val_f1_metric: 0.2365\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 894us/step - loss: 0.9301 - acc: 0.8537 - precision: 0.1443 - recall: 1.0482 - f1_metric: 0.2486 - val_loss: 0.3870 - val_acc: 0.8703 - val_precision: 0.1442 - val_recall: 1.0000 - val_f1_metric: 0.2375\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.5733 - acc: 0.8582 - precision: 0.1401 - recall: 1.0535 - f1_metric: 0.2421 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1442 - val_recall: 1.0000 - val_f1_metric: 0.2374\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 867us/step - loss: 0.4472 - acc: 0.8574 - precision: 0.1424 - recall: 1.0240 - f1_metric: 0.2461 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1442 - val_recall: 1.0000 - val_f1_metric: 0.2374\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 916us/step - loss: 0.4541 - acc: 0.8598 - precision: 0.1418 - recall: 1.0530 - f1_metric: 0.2441 - val_loss: 0.3856 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 889us/step - loss: 0.4864 - acc: 0.8596 - precision: 0.1372 - recall: 1.0306 - f1_metric: 0.2375 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1442 - val_recall: 1.0000 - val_f1_metric: 0.2374\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 879us/step - loss: 0.4532 - acc: 0.8560 - precision: 0.1427 - recall: 1.0253 - f1_metric: 0.2448 - val_loss: 0.3860 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 941us/step - loss: 0.9097 - acc: 0.8558 - precision: 0.1426 - recall: 1.0452 - f1_metric: 0.2465 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4256 - acc: 0.8630 - precision: 0.1363 - recall: 1.0346 - f1_metric: 0.2356 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1439 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4483 - acc: 0.8593 - precision: 0.1405 - recall: 1.0325 - f1_metric: 0.2428 - val_loss: 0.3866 - val_acc: 0.8703 - val_precision: 0.1444 - val_recall: 1.0000 - val_f1_metric: 0.2378\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.4561 - acc: 0.8584 - precision: 0.1403 - recall: 1.0269 - f1_metric: 0.2430 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1448 - val_recall: 1.0000 - val_f1_metric: 0.2383\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 1.5370 - acc: 0.8589 - precision: 0.1392 - recall: 1.0057 - f1_metric: 0.2398 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1448 - val_recall: 1.0000 - val_f1_metric: 0.2383\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 917us/step - loss: 0.4086 - acc: 0.8661 - precision: 0.1319 - recall: 0.9975 - f1_metric: 0.2284 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1447 - val_recall: 1.0000 - val_f1_metric: 0.2381\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 900us/step - loss: 0.4180 - acc: 0.8527 - precision: 0.1450 - recall: 1.0109 - f1_metric: 0.2487 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1447 - val_recall: 1.0000 - val_f1_metric: 0.2381\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 938us/step - loss: 0.4273 - acc: 0.8582 - precision: 0.1412 - recall: 1.0059 - f1_metric: 0.2431 - val_loss: 0.3861 - val_acc: 0.8703 - val_precision: 0.1444 - val_recall: 1.0000 - val_f1_metric: 0.2377\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 943us/step - loss: 0.4011 - acc: 0.8632 - precision: 0.1358 - recall: 1.0137 - f1_metric: 0.2354 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2371\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 908us/step - loss: 0.4149 - acc: 0.8537 - precision: 0.1434 - recall: 0.9947 - f1_metric: 0.2464 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1445 - val_recall: 1.0000 - val_f1_metric: 0.2379\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 942us/step - loss: 0.4186 - acc: 0.8611 - precision: 0.1381 - recall: 0.9860 - f1_metric: 0.2376 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1451 - val_recall: 1.0000 - val_f1_metric: 0.2388\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 896us/step - loss: 0.4182 - acc: 0.8516 - precision: 0.1465 - recall: 0.9802 - f1_metric: 0.2501 - val_loss: 0.3859 - val_acc: 0.8703 - val_precision: 0.1447 - val_recall: 1.0000 - val_f1_metric: 0.2381\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 944us/step - loss: 0.4316 - acc: 0.8546 - precision: 0.1431 - recall: 1.0087 - f1_metric: 0.2460 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1440 - val_recall: 1.0000 - val_f1_metric: 0.2370\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 956us/step - loss: 0.4044 - acc: 0.8615 - precision: 0.1360 - recall: 1.0056 - f1_metric: 0.2344 - val_loss: 0.3863 - val_acc: 0.8703 - val_precision: 0.1447 - val_recall: 1.0000 - val_f1_metric: 0.2381\n",
      "Epoch 30/30\n",
      "205/205 [==============================] - 0s 955us/step - loss: 0.4132 - acc: 0.8563 - precision: 0.1421 - recall: 0.9984 - f1_metric: 0.2426 - val_loss: 0.3862 - val_acc: 0.8703 - val_precision: 0.1441 - val_recall: 1.0000 - val_f1_metric: 0.2373\n",
      "Model 20 Fitting\n",
      "Epoch 1/30\n",
      "205/205 [==============================] - 1s 2ms/step - loss: 177.3759 - acc: 0.8285 - precision: 0.1200 - recall: 0.3428 - f1_metric: 0.1692 - val_loss: 20.4586 - val_acc: 0.8703 - val_precision: 0.0320 - val_recall: 0.0647 - val_f1_metric: 0.0416\n",
      "Epoch 2/30\n",
      "205/205 [==============================] - 0s 893us/step - loss: 24.4759 - acc: 0.8463 - precision: 0.1295 - recall: 0.5595 - f1_metric: 0.2016 - val_loss: 0.9291 - val_acc: 0.8611 - val_precision: 0.0464 - val_recall: 0.0582 - val_f1_metric: 0.0493\n",
      "Epoch 3/30\n",
      "205/205 [==============================] - 0s 940us/step - loss: 4.2401 - acc: 0.8502 - precision: 0.1187 - recall: 0.4186 - f1_metric: 0.1756 - val_loss: 0.6073 - val_acc: 0.8703 - val_precision: 0.0668 - val_recall: 0.1149 - val_f1_metric: 0.0792\n",
      "Epoch 4/30\n",
      "205/205 [==============================] - 0s 935us/step - loss: 1.8828 - acc: 0.8614 - precision: 0.1159 - recall: 0.4592 - f1_metric: 0.1797 - val_loss: 0.4486 - val_acc: 0.8703 - val_precision: 0.0650 - val_recall: 0.1574 - val_f1_metric: 0.0878\n",
      "Epoch 5/30\n",
      "205/205 [==============================] - 0s 947us/step - loss: 0.9006 - acc: 0.8597 - precision: 0.1365 - recall: 0.6770 - f1_metric: 0.2183 - val_loss: 0.4277 - val_acc: 0.8703 - val_precision: 0.0817 - val_recall: 0.2487 - val_f1_metric: 0.1180\n",
      "Epoch 6/30\n",
      "205/205 [==============================] - 0s 919us/step - loss: 0.6877 - acc: 0.8585 - precision: 0.1213 - recall: 0.6261 - f1_metric: 0.1979 - val_loss: 0.4139 - val_acc: 0.8703 - val_precision: 0.1132 - val_recall: 0.4769 - val_f1_metric: 0.1649\n",
      "Epoch 7/30\n",
      "205/205 [==============================] - 0s 946us/step - loss: 0.5212 - acc: 0.8624 - precision: 0.1324 - recall: 0.8130 - f1_metric: 0.2217 - val_loss: 0.3979 - val_acc: 0.8703 - val_precision: 0.1224 - val_recall: 0.6354 - val_f1_metric: 0.1914\n",
      "Epoch 8/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.4430 - acc: 0.8614 - precision: 0.1362 - recall: 0.8232 - f1_metric: 0.2282 - val_loss: 0.3931 - val_acc: 0.8703 - val_precision: 0.1467 - val_recall: 0.9561 - val_f1_metric: 0.2399\n",
      "Epoch 9/30\n",
      "205/205 [==============================] - 0s 945us/step - loss: 0.4793 - acc: 0.8567 - precision: 0.1397 - recall: 0.8525 - f1_metric: 0.2345 - val_loss: 0.3904 - val_acc: 0.8703 - val_precision: 0.1473 - val_recall: 1.0000 - val_f1_metric: 0.2422\n",
      "Epoch 10/30\n",
      "205/205 [==============================] - 0s 916us/step - loss: 0.4626 - acc: 0.8570 - precision: 0.1450 - recall: 0.8941 - f1_metric: 0.2436 - val_loss: 0.3887 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 11/30\n",
      "205/205 [==============================] - 0s 905us/step - loss: 0.5163 - acc: 0.8594 - precision: 0.1421 - recall: 0.9427 - f1_metric: 0.2427 - val_loss: 0.3878 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 12/30\n",
      "205/205 [==============================] - 0s 924us/step - loss: 0.4559 - acc: 0.8555 - precision: 0.1435 - recall: 0.9338 - f1_metric: 0.2436 - val_loss: 0.3871 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 13/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.5445 - acc: 0.8563 - precision: 0.1494 - recall: 0.9906 - f1_metric: 0.2537 - val_loss: 0.3870 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 14/30\n",
      "205/205 [==============================] - 0s 919us/step - loss: 0.4259 - acc: 0.8633 - precision: 0.1372 - recall: 0.9924 - f1_metric: 0.2363 - val_loss: 0.3868 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 15/30\n",
      "205/205 [==============================] - 0s 898us/step - loss: 0.4365 - acc: 0.8504 - precision: 0.1438 - recall: 0.9497 - f1_metric: 0.2443 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 16/30\n",
      "205/205 [==============================] - 0s 941us/step - loss: 0.4406 - acc: 0.8529 - precision: 0.1454 - recall: 0.9969 - f1_metric: 0.2494 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 17/30\n",
      "205/205 [==============================] - 0s 919us/step - loss: 0.4059 - acc: 0.8667 - precision: 0.1325 - recall: 1.0087 - f1_metric: 0.2298 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 18/30\n",
      "205/205 [==============================] - 0s 1ms/step - loss: 0.6568 - acc: 0.8602 - precision: 0.1404 - recall: 1.0272 - f1_metric: 0.2431 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 19/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.4206 - acc: 0.8590 - precision: 0.1410 - recall: 0.9787 - f1_metric: 0.2411 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 20/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.8190 - acc: 0.8569 - precision: 0.1435 - recall: 1.0035 - f1_metric: 0.2464 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 21/30\n",
      "205/205 [==============================] - 0s 885us/step - loss: 0.4261 - acc: 0.8542 - precision: 0.1444 - recall: 0.9659 - f1_metric: 0.2460 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 22/30\n",
      "205/205 [==============================] - 0s 928us/step - loss: 0.4154 - acc: 0.8581 - precision: 0.1448 - recall: 0.9915 - f1_metric: 0.2479 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 23/30\n",
      "205/205 [==============================] - 0s 933us/step - loss: 0.4057 - acc: 0.8619 - precision: 0.1374 - recall: 0.9756 - f1_metric: 0.2368 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 24/30\n",
      "205/205 [==============================] - 0s 897us/step - loss: 0.4239 - acc: 0.8576 - precision: 0.1420 - recall: 0.9956 - f1_metric: 0.2442 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 25/30\n",
      "205/205 [==============================] - 0s 915us/step - loss: 0.4199 - acc: 0.8582 - precision: 0.1409 - recall: 0.9833 - f1_metric: 0.2420 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 26/30\n",
      "205/205 [==============================] - 0s 907us/step - loss: 0.4016 - acc: 0.8623 - precision: 0.1346 - recall: 0.9808 - f1_metric: 0.2316 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 27/30\n",
      "205/205 [==============================] - 0s 901us/step - loss: 0.4217 - acc: 0.8576 - precision: 0.1421 - recall: 0.9970 - f1_metric: 0.2441 - val_loss: 0.3864 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 28/30\n",
      "205/205 [==============================] - 0s 886us/step - loss: 0.4165 - acc: 0.8561 - precision: 0.1431 - recall: 0.9889 - f1_metric: 0.2453 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 29/30\n",
      "205/205 [==============================] - 0s 937us/step - loss: 0.4240 - acc: 0.8587 - precision: 0.1423 - recall: 0.9946 - f1_metric: 0.2436 - val_loss: 0.3865 - val_acc: 0.8703 - val_precision: 0.1454 - val_recall: 1.0000 - val_f1_metric: 0.2393\n",
      "Epoch 30/30\n",
      "140/205 [===================>..........] - ETA: 0s - loss: 0.4131 - acc: 0.8568 - precision: 0.1444 - recall: 1.0017 - f1_metric: 0.2482"
     ]
    }
   ],
   "source": [
    "#Run the model\n",
    "\n",
    "print(\"Model 1 Fitting\")\n",
    "history1 = model1.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 2 Fitting\")\n",
    "history2 = model2.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 3 Fitting\")\n",
    "history3 = model3.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 4 Fitting\")\n",
    "history4 = model4.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 5 Fitting\")\n",
    "history5 = model5.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 6 Fitting\")\n",
    "history6 = model6.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 7 Fitting\")\n",
    "history7 = model7.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 8 Fitting\")\n",
    "history8 = model8.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 9 Fitting\")\n",
    "history9 = model9.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 10 Fitting\")\n",
    "history10 = model10.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 11 Fitting\")\n",
    "history11 = model11.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 12 Fitting\")\n",
    "history12 = model12.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 13 Fitting\")\n",
    "history13 = model13.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 14 Fitting\")\n",
    "history14 = model14.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 15 Fitting\")\n",
    "history15 = model15.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 16 Fitting\")\n",
    "history16 = model16.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 17 Fitting\")\n",
    "history17 = model17.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 18 Fitting\")\n",
    "history18 = model18.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 19 Fitting\")\n",
    "history19 = model19.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 20 Fitting\")\n",
    "history20 = model20.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 21 Fitting\")\n",
    "history21 = model21.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 22 Fitting\")\n",
    "history22 = model22.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 23 Fitting\")\n",
    "history23 = model23.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 24 Fitting\")\n",
    "history24 = model24.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 25 Fitting\")\n",
    "history25 = model25.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "\n",
    "print(\"Model 26 Fitting\")\n",
    "history26 = model26.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 27 Fitting\")\n",
    "history27 = model27.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 28 Fitting\")\n",
    "history28 = model28.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 29 Fitting\")\n",
    "history29 = model29.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
    "print(\"Model 30 Fitting\")\n",
    "history30 = model30.fit(features_train, labels_train, epochs=30, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfHp9ww0FWvB"
   },
   "outputs": [],
   "source": [
    "#Run model predictions\n",
    "\n",
    "# Model 1 \n",
    "prediction_features_1 = model1.predict(features_test)\n",
    "performance1 = model1.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 2 \n",
    "prediction_features_2 = model2.predict(features_test)\n",
    "performance2 = model2.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 3 \n",
    "prediction_features_3 = model3.predict(features_test)\n",
    "performance3 = model3.evaluate(features_test, labels_test)\n",
    "print(performance1)\n",
    "# Model 4 \n",
    "prediction_features_4 = model4.predict(features_test)\n",
    "performance4 = model4.evaluate(features_test, labels_test)\n",
    "print(performance4)\n",
    "# Model 5 \n",
    "prediction_features_5 = model5.predict(features_test)\n",
    "performance5 = model5.evaluate(features_test, labels_test)\n",
    "print(performance5)\n",
    "\n",
    "# Model 6 \n",
    "prediction_features_6 = model6.predict(features_test)\n",
    "performance6 = model6.evaluate(features_test, labels_test)\n",
    "print(performance6)\n",
    "# Model 7\n",
    "prediction_features_7 = model7.predict(features_test)\n",
    "performance7 = model7.evaluate(features_test, labels_test)\n",
    "print(performance7)\n",
    "# Model 8 \n",
    "prediction_features_8 = model8.predict(features_test)\n",
    "performance8 = model8.evaluate(features_test, labels_test)\n",
    "print(performance8)\n",
    "# Model 9\n",
    "prediction_features_9 = model9.predict(features_test)\n",
    "performance9 = model9.evaluate(features_test, labels_test)\n",
    "print(performance9)\n",
    "# Model 10\n",
    "prediction_features_10 = model10.predict(features_test)\n",
    "performance10 = model10.evaluate(features_test, labels_test)\n",
    "print(performance10)\n",
    "\n",
    "# Model 11 \n",
    "prediction_features_11 = model11.predict(features_test)\n",
    "performance11 = model11.evaluate(features_test, labels_test)\n",
    "print(performance11)\n",
    "# Model 12 \n",
    "prediction_features_12 = model12.predict(features_test)\n",
    "performance12 = model12.evaluate(features_test, labels_test)\n",
    "print(performance12)\n",
    "# Model 13 \n",
    "prediction_features_13 = model13.predict(features_test)\n",
    "performance13 = model13.evaluate(features_test, labels_test)\n",
    "print(performance13)\n",
    "# Model 14 \n",
    "prediction_features_14 = model14.predict(features_test)\n",
    "performance14 = model14.evaluate(features_test, labels_test)\n",
    "print(performance14)\n",
    "# Model 15 \n",
    "prediction_features_15 = model15.predict(features_test)\n",
    "performance15 = model15.evaluate(features_test, labels_test)\n",
    "print(performance15)\n",
    "\n",
    "# Model 16\n",
    "prediction_features_16 = model16.predict(features_test)\n",
    "performance16 = model16.evaluate(features_test, labels_test)\n",
    "print(performance16)\n",
    "# Model 17\n",
    "prediction_features_17 = model17.predict(features_test)\n",
    "performance17 = model17.evaluate(features_test, labels_test)\n",
    "print(performance17)\n",
    "# Model 18 \n",
    "prediction_features_18 = model18.predict(features_test)\n",
    "performance18 = model18.evaluate(features_test, labels_test)\n",
    "print(performance18)\n",
    "# Model 19\n",
    "prediction_features_19 = model19.predict(features_test)\n",
    "performance19 = model19.evaluate(features_test, labels_test)\n",
    "print(performance19)\n",
    "# Model 20 \n",
    "prediction_features_20 = model20.predict(features_test)\n",
    "performance20 = model20.evaluate(features_test, labels_test)\n",
    "print(performance20)\n",
    "\n",
    "# Model 21\n",
    "prediction_features_21 = model21.predict(features_test)\n",
    "performance21 = model21.evaluate(features_test, labels_test)\n",
    "print(performance21)\n",
    "# Model 22\n",
    "prediction_features_22 = model22.predict(features_test)\n",
    "performance22 = model22.evaluate(features_test, labels_test)\n",
    "print(performance22)\n",
    "# Model 23\n",
    "prediction_features_23 = model23.predict(features_test)\n",
    "performance23 = model23.evaluate(features_test, labels_test)\n",
    "print(performance23)\n",
    "# Model 24 \n",
    "prediction_features_24 = model24.predict(features_test)\n",
    "performance24 = model24.evaluate(features_test, labels_test)\n",
    "print(performance24)\n",
    "# Model 25\n",
    "prediction_features_25 = model25.predict(features_test)\n",
    "performance25 = model25.evaluate(features_test, labels_test)\n",
    "print(performance25)\n",
    "\n",
    "# Model 26 \n",
    "prediction_features_26 = model26.predict(features_test)\n",
    "performance26 = model26.evaluate(features_test, labels_test)\n",
    "print(performance26)\n",
    "# Model 27 \n",
    "prediction_features_27 = model27.predict(features_test)\n",
    "performance27 = model27.evaluate(features_test, labels_test)\n",
    "print(performance27)\n",
    "# Model 28 \n",
    "prediction_features_28 = model28.predict(features_test)\n",
    "performance28 = model28.evaluate(features_test, labels_test)\n",
    "print(performance28)\n",
    "# Model 29\n",
    "prediction_features_29 = model29.predict(features_test)\n",
    "performance29 = model29.evaluate(features_test, labels_test)\n",
    "print(performance29)\n",
    "# Model 30 \n",
    "prediction_features_30 = model30.predict(features_test)\n",
    "performance30 = model30.evaluate(features_test, labels_test)\n",
    "print(performance30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kn2xq4Ts7s-"
   },
   "outputs": [],
   "source": [
    "# Averages\n",
    "\n",
    "# Loss\n",
    "loss_avg = (performance1[0] + performance2[0] + performance3[0] + performance4[0] + performance5[0] + performance6[0] + performance7[0] + performance8[0] + performance9[0] + performance10[0]\n",
    "            + performance11[0] + performance12[0] + performance13[0] + performance14[0] + performance15[0] + performance16[0] + performance17[0] + performance18[0] + performance19[0] + performance20[0]\n",
    "            + performance21[0] + performance22[0] + performance23[0] + performance24[0] + performance25[0] + performance26[0] + performance27[0] + performance28[0] + performance29[0] + performance30[0])/30\n",
    "print(\"Loss Average: \", loss_avg)\n",
    "\n",
    "# Accuracy\n",
    "acc_avg =(performance1[1] + performance2[1] + performance3[1] + performance4[1] + performance5[1] + performance6[1] + performance7[1] + performance8[1] + performance9[1] + performance10[1]\n",
    "            + performance11[1] + performance12[1] + performance13[1] + performance14[1] + performance15[1] + performance16[1] + performance17[1] + performance18[1] + performance19[1] + performance20[1]\n",
    "            + performance21[1] + performance22[1] + performance23[1] + performance24[1] + performance25[1] + performance26[1] + performance27[1] + performance28[1] + performance29[1] + performance30[1])/30\n",
    "print(\"Accuraccy Average: \", acc_avg)\n",
    "\n",
    "# Precision\n",
    "precision_avg = (performance1[2] + performance2[2] + performance3[2] + performance4[2] + performance5[2] + performance6[2] + performance7[2] + performance8[2] + performance9[2] + performance10[2]\n",
    "            + performance11[2] + performance12[2] + performance13[2] + performance14[2] + performance15[2] + performance16[2] + performance17[2] + performance18[2] + performance19[2] + performance20[2]\n",
    "            + performance21[2] + performance22[2] + performance23[2] + performance24[2] + performance25[2] + performance26[2] + performance27[2] + performance28[2] + performance29[2] + performance30[2])/30\n",
    "print(\"Precision Average: \", precision_avg)\n",
    "\n",
    "# Recall\n",
    "recall_avg = (performance1[3] + performance2[3] + performance3[3] + performance4[3] + performance5[3] + performance6[3] + performance7[3] + performance8[3] + performance9[3] + performance10[3]\n",
    "            + performance11[3] + performance12[3] + performance13[3] + performance14[3] + performance15[3] + performance16[3] + performance17[3] + performance18[3] + performance19[3] + performance20[3]\n",
    "            + performance21[3] + performance22[3] + performance23[3] + performance24[3] + performance25[3] + performance26[3] + performance27[3] + performance28[3] + performance29[3] + performance30[3])/30\n",
    "print(\"Recall Average: \", recall_avg)\n",
    "\n",
    "# f1_metric\n",
    "f1_avg = (performance1[4] + performance2[4] + performance3[4] + performance4[4] + performance5[4] + performance6[4] + performance7[4] + performance8[4] + performance9[4] + performance10[4]\n",
    "            + performance11[4] + performance12[4] + performance13[4] + performance14[4] + performance15[4] + performance16[4] + performance17[4] + performance18[4] + performance19[4] + performance20[4]\n",
    "            + performance21[4] + performance22[4] + performance23[4] + performance24[4] + performance25[4] + performance26[4] + performance27[4] + performance28[4] + performance29[4] + performance30[4])/30\n",
    "print(\"F1 Average: \", f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuwAUg5_KOz4"
   },
   "outputs": [],
   "source": [
    "#Take the standard deviation of the model samples\n",
    "\n",
    "#Loss SE\n",
    "Loss_SE = statistics.stdev([performance1[0], performance2[0], performance3[0], performance4[0], performance5[0],\n",
    "                  performance6[0], performance7[0], performance8[0], performance9[0], performance10[0],\n",
    "                  performance11[0], performance12[0], performance13[0], performance14[0], performance15[0],\n",
    "                  performance16[0], performance17[0], performance18[0], performance19[0], performance20[0], \n",
    "                  performance21[0], performance22[0], performance23[0], performance24[0], performance25[0],\n",
    "                  performance26[0], performance27[0], performance28[0], performance29[0], performance30[0]])\n",
    "print(\"Loss SE:\", Loss_SE)\n",
    "\n",
    "#Accuracy SE\n",
    "Acc_SE = statistics.stdev([performance1[1], performance2[1], performance3[1], performance4[1], performance5[1],\n",
    "                  performance6[1], performance7[1], performance8[1], performance9[1], performance10[1],\n",
    "                  performance11[1], performance12[1], performance13[1], performance14[1], performance15[1],\n",
    "                  performance16[1], performance17[1], performance18[1], performance19[1], performance20[1], \n",
    "                  performance21[1], performance22[1], performance23[1], performance24[1], performance25[1],\n",
    "                  performance26[1], performance27[1], performance28[1], performance29[1], performance30[1]])\n",
    "print(\"Accuraccy SE: \", Acc_SE)\n",
    "\n",
    "#Precision SE\n",
    "precision_SE = statistics.stdev([performance1[2], performance2[2], performance3[2], performance4[2], performance5[2],\n",
    "                  performance6[2], performance7[2], performance8[2], performance9[2], performance10[2],\n",
    "                  performance11[2], performance12[2], performance13[2], performance14[2], performance15[2],\n",
    "                  performance16[2], performance17[2], performance18[2], performance19[2], performance20[2], \n",
    "                  performance21[2], performance22[2], performance23[2], performance24[2], performance25[2],\n",
    "                  performance26[2], performance27[2], performance28[2], performance29[2], performance30[2]])\n",
    "print(\"Precision SE: \", precision_SE)\n",
    "\n",
    "#Recall \n",
    "Recall_SE = statistics.stdev([performance1[3], performance2[3], performance3[3], performance4[3], performance5[3],\n",
    "                  performance6[3], performance7[3], performance8[3], performance9[3], performance10[3],\n",
    "                  performance11[3], performance12[3], performance13[3], performance14[3], performance15[3],\n",
    "                  performance16[3], performance17[3], performance18[3], performance19[3], performance20[3], \n",
    "                  performance21[3], performance22[3], performance23[3], performance24[3], performance25[3],\n",
    "                  performance26[3], performance27[3], performance28[3], performance29[3], performance30[3]])\n",
    "print(\"Recall SE: \", Recall_SE)\n",
    "\n",
    "#F1 Score\n",
    "F1_Score_SE = statistics.stdev([performance1[4], performance2[4], performance3[4], performance4[4], performance5[4],\n",
    "                  performance6[4], performance7[4], performance8[4], performance9[4], performance10[4],\n",
    "                  performance11[4], performance12[4], performance13[4], performance14[4], performance15[4],\n",
    "                  performance16[4], performance17[4], performance18[4], performance19[4], performance20[4], \n",
    "                  performance21[4], performance22[4], performance23[4], performance24[4], performance25[4],\n",
    "                  performance26[4], performance27[4], performance28[4], performance29[4], performance30[4]])\n",
    "print(\"F1_Score_SE: \", F1_Score_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMpLKJ4OKt-X"
   },
   "outputs": [],
   "source": [
    "#Tensorflow Graphics\n",
    "\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GA_ParliHeg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

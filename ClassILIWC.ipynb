{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassILIWC 2.0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9kxq01msMg9"
      },
      "source": [
        "### BEWARE:Tensorflow is stochastic - this means the model will not be replicated exactly. \n",
        "### Use GA_Load_Model for reproduction\n",
        "\n",
        "#!pip install mlxtend\n",
        "\n",
        "#!pip install h5py pyyaml\n",
        "\n",
        "#!pip install tensorboard\n",
        "\n",
        "#!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLftnb5sBe7B"
      },
      "source": [
        "#Load packages\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBsKz40OAGDU"
      },
      "source": [
        "### Packages necessary for model construction \n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.callbacks\n",
        "import datetime \n",
        "import statistics\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import os \n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn\n",
        "\n",
        "import pydot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "FrEcBaVWVVkb",
        "outputId": "75f4e391-f050-4d36-e61a-03efdd56ef10"
      },
      "source": [
        "#Read the Data\n",
        "\n",
        "UN_Data = pd.read_csv('GA_Query_CleanLIWC')\n",
        "\n",
        "UN_Data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>date</th>\n",
              "      <th>Class M</th>\n",
              "      <th>Class S</th>\n",
              "      <th>Class I</th>\n",
              "      <th>Class P</th>\n",
              "      <th>Class B</th>\n",
              "      <th>Policy Passed</th>\n",
              "      <th>Conflict Indicator</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>Authentic</th>\n",
              "      <th>Tone</th>\n",
              "      <th>WPS</th>\n",
              "      <th>Sixltr</th>\n",
              "      <th>Dic</th>\n",
              "      <th>function</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>ppron</th>\n",
              "      <th>i</th>\n",
              "      <th>we</th>\n",
              "      <th>you</th>\n",
              "      <th>shehe</th>\n",
              "      <th>they</th>\n",
              "      <th>ipron</th>\n",
              "      <th>article</th>\n",
              "      <th>prep</th>\n",
              "      <th>auxverb</th>\n",
              "      <th>adverb</th>\n",
              "      <th>conj</th>\n",
              "      <th>negate</th>\n",
              "      <th>verb</th>\n",
              "      <th>adj</th>\n",
              "      <th>compare</th>\n",
              "      <th>interrog</th>\n",
              "      <th>number</th>\n",
              "      <th>quant</th>\n",
              "      <th>affect</th>\n",
              "      <th>posemo</th>\n",
              "      <th>...</th>\n",
              "      <th>health</th>\n",
              "      <th>sexual</th>\n",
              "      <th>ingest</th>\n",
              "      <th>drives</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>achieve</th>\n",
              "      <th>power</th>\n",
              "      <th>reward</th>\n",
              "      <th>risk</th>\n",
              "      <th>focuspast</th>\n",
              "      <th>focuspresent</th>\n",
              "      <th>focusfuture</th>\n",
              "      <th>relativ</th>\n",
              "      <th>motion</th>\n",
              "      <th>space</th>\n",
              "      <th>time</th>\n",
              "      <th>work</th>\n",
              "      <th>leisure</th>\n",
              "      <th>home</th>\n",
              "      <th>money</th>\n",
              "      <th>relig</th>\n",
              "      <th>death</th>\n",
              "      <th>informal</th>\n",
              "      <th>swear</th>\n",
              "      <th>netspeak</th>\n",
              "      <th>assent</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>filler</th>\n",
              "      <th>AllPunc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Comma</th>\n",
              "      <th>Colon</th>\n",
              "      <th>SemiC</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Dash</th>\n",
              "      <th>Quote</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>Parenth</th>\n",
              "      <th>OtherP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>20075.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>68.60</td>\n",
              "      <td>14.49</td>\n",
              "      <td>96.27</td>\n",
              "      <td>27.24</td>\n",
              "      <td>34.34</td>\n",
              "      <td>77.56</td>\n",
              "      <td>42.78</td>\n",
              "      <td>5.44</td>\n",
              "      <td>2.15</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.24</td>\n",
              "      <td>3.29</td>\n",
              "      <td>11.21</td>\n",
              "      <td>16.88</td>\n",
              "      <td>3.82</td>\n",
              "      <td>1.51</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.26</td>\n",
              "      <td>6.50</td>\n",
              "      <td>6.69</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.35</td>\n",
              "      <td>2.43</td>\n",
              "      <td>1.01</td>\n",
              "      <td>7.11</td>\n",
              "      <td>5.98</td>\n",
              "      <td>...</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>11.52</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.91</td>\n",
              "      <td>3.37</td>\n",
              "      <td>1.12</td>\n",
              "      <td>2.16</td>\n",
              "      <td>1.37</td>\n",
              "      <td>4.82</td>\n",
              "      <td>0.97</td>\n",
              "      <td>12.55</td>\n",
              "      <td>1.22</td>\n",
              "      <td>8.62</td>\n",
              "      <td>2.69</td>\n",
              "      <td>6.46</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.80</td>\n",
              "      <td>4.04</td>\n",
              "      <td>4.34</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>822.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>74.84</td>\n",
              "      <td>17.74</td>\n",
              "      <td>41.15</td>\n",
              "      <td>20.05</td>\n",
              "      <td>26.64</td>\n",
              "      <td>72.38</td>\n",
              "      <td>43.19</td>\n",
              "      <td>5.23</td>\n",
              "      <td>2.92</td>\n",
              "      <td>1.46</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.31</td>\n",
              "      <td>12.77</td>\n",
              "      <td>16.55</td>\n",
              "      <td>5.35</td>\n",
              "      <td>1.70</td>\n",
              "      <td>2.19</td>\n",
              "      <td>0.36</td>\n",
              "      <td>10.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.73</td>\n",
              "      <td>5.72</td>\n",
              "      <td>1.46</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.22</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>8.64</td>\n",
              "      <td>2.31</td>\n",
              "      <td>1.09</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.49</td>\n",
              "      <td>1.95</td>\n",
              "      <td>6.08</td>\n",
              "      <td>1.70</td>\n",
              "      <td>12.90</td>\n",
              "      <td>1.34</td>\n",
              "      <td>7.54</td>\n",
              "      <td>4.26</td>\n",
              "      <td>5.84</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.60</td>\n",
              "      <td>11.56</td>\n",
              "      <td>3.04</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.12</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2003</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>66.07</td>\n",
              "      <td>4.02</td>\n",
              "      <td>31.19</td>\n",
              "      <td>19.62</td>\n",
              "      <td>29.30</td>\n",
              "      <td>72.93</td>\n",
              "      <td>38.54</td>\n",
              "      <td>2.55</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.23</td>\n",
              "      <td>13.38</td>\n",
              "      <td>17.20</td>\n",
              "      <td>4.14</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.37</td>\n",
              "      <td>3.82</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.00</td>\n",
              "      <td>13.06</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.32</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.96</td>\n",
              "      <td>8.92</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.96</td>\n",
              "      <td>7.32</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.46</td>\n",
              "      <td>2.55</td>\n",
              "      <td>0.32</td>\n",
              "      <td>10.83</td>\n",
              "      <td>0.64</td>\n",
              "      <td>6.05</td>\n",
              "      <td>3.82</td>\n",
              "      <td>10.51</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.08</td>\n",
              "      <td>20.70</td>\n",
              "      <td>3.50</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.23</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>3.18</td>\n",
              "      <td>2.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1995</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17021.0</td>\n",
              "      <td>98.45</td>\n",
              "      <td>62.93</td>\n",
              "      <td>24.54</td>\n",
              "      <td>58.00</td>\n",
              "      <td>32.30</td>\n",
              "      <td>30.04</td>\n",
              "      <td>75.49</td>\n",
              "      <td>46.31</td>\n",
              "      <td>6.64</td>\n",
              "      <td>2.17</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.45</td>\n",
              "      <td>4.48</td>\n",
              "      <td>11.69</td>\n",
              "      <td>16.96</td>\n",
              "      <td>5.22</td>\n",
              "      <td>2.20</td>\n",
              "      <td>4.48</td>\n",
              "      <td>0.57</td>\n",
              "      <td>7.80</td>\n",
              "      <td>4.04</td>\n",
              "      <td>1.46</td>\n",
              "      <td>0.79</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.33</td>\n",
              "      <td>4.85</td>\n",
              "      <td>3.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.09</td>\n",
              "      <td>7.50</td>\n",
              "      <td>2.27</td>\n",
              "      <td>1.74</td>\n",
              "      <td>2.61</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1.59</td>\n",
              "      <td>5.12</td>\n",
              "      <td>1.11</td>\n",
              "      <td>12.84</td>\n",
              "      <td>0.95</td>\n",
              "      <td>8.61</td>\n",
              "      <td>3.24</td>\n",
              "      <td>4.57</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.34</td>\n",
              "      <td>3.44</td>\n",
              "      <td>4.91</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.64</td>\n",
              "      <td>2.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9289.0</td>\n",
              "      <td>98.94</td>\n",
              "      <td>60.45</td>\n",
              "      <td>8.85</td>\n",
              "      <td>70.17</td>\n",
              "      <td>22.82</td>\n",
              "      <td>32.95</td>\n",
              "      <td>78.23</td>\n",
              "      <td>44.38</td>\n",
              "      <td>4.40</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.22</td>\n",
              "      <td>3.58</td>\n",
              "      <td>13.27</td>\n",
              "      <td>15.19</td>\n",
              "      <td>5.80</td>\n",
              "      <td>1.71</td>\n",
              "      <td>4.61</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.08</td>\n",
              "      <td>3.90</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0.44</td>\n",
              "      <td>4.94</td>\n",
              "      <td>1.35</td>\n",
              "      <td>3.93</td>\n",
              "      <td>3.07</td>\n",
              "      <td>...</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.01</td>\n",
              "      <td>1.17</td>\n",
              "      <td>3.18</td>\n",
              "      <td>3.89</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.70</td>\n",
              "      <td>5.12</td>\n",
              "      <td>2.73</td>\n",
              "      <td>0.86</td>\n",
              "      <td>11.68</td>\n",
              "      <td>1.35</td>\n",
              "      <td>7.12</td>\n",
              "      <td>3.12</td>\n",
              "      <td>7.94</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.38</td>\n",
              "      <td>5.37</td>\n",
              "      <td>3.80</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.42</td>\n",
              "      <td>1.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10207</th>\n",
              "      <td>10207</td>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4059.0</td>\n",
              "      <td>98.95</td>\n",
              "      <td>58.85</td>\n",
              "      <td>3.46</td>\n",
              "      <td>71.55</td>\n",
              "      <td>23.46</td>\n",
              "      <td>31.44</td>\n",
              "      <td>75.41</td>\n",
              "      <td>45.31</td>\n",
              "      <td>4.73</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.12</td>\n",
              "      <td>3.99</td>\n",
              "      <td>12.59</td>\n",
              "      <td>16.38</td>\n",
              "      <td>6.16</td>\n",
              "      <td>1.58</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.01</td>\n",
              "      <td>9.04</td>\n",
              "      <td>4.09</td>\n",
              "      <td>1.21</td>\n",
              "      <td>0.64</td>\n",
              "      <td>5.15</td>\n",
              "      <td>1.13</td>\n",
              "      <td>4.51</td>\n",
              "      <td>3.40</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>7.71</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.49</td>\n",
              "      <td>1.03</td>\n",
              "      <td>4.88</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.83</td>\n",
              "      <td>0.64</td>\n",
              "      <td>6.01</td>\n",
              "      <td>3.13</td>\n",
              "      <td>6.43</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.46</td>\n",
              "      <td>5.57</td>\n",
              "      <td>3.72</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1.72</td>\n",
              "      <td>1.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10208</th>\n",
              "      <td>10208</td>\n",
              "      <td>1994</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8210.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>56.93</td>\n",
              "      <td>10.94</td>\n",
              "      <td>62.72</td>\n",
              "      <td>25.42</td>\n",
              "      <td>30.78</td>\n",
              "      <td>77.17</td>\n",
              "      <td>45.35</td>\n",
              "      <td>4.31</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.21</td>\n",
              "      <td>3.74</td>\n",
              "      <td>13.01</td>\n",
              "      <td>17.15</td>\n",
              "      <td>5.57</td>\n",
              "      <td>1.49</td>\n",
              "      <td>4.71</td>\n",
              "      <td>0.35</td>\n",
              "      <td>7.94</td>\n",
              "      <td>3.67</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.88</td>\n",
              "      <td>5.31</td>\n",
              "      <td>1.32</td>\n",
              "      <td>3.02</td>\n",
              "      <td>2.46</td>\n",
              "      <td>...</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.13</td>\n",
              "      <td>7.10</td>\n",
              "      <td>0.71</td>\n",
              "      <td>2.02</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.55</td>\n",
              "      <td>4.95</td>\n",
              "      <td>2.36</td>\n",
              "      <td>0.82</td>\n",
              "      <td>12.30</td>\n",
              "      <td>0.94</td>\n",
              "      <td>7.00</td>\n",
              "      <td>4.26</td>\n",
              "      <td>8.36</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.06</td>\n",
              "      <td>3.74</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.63</td>\n",
              "      <td>4.53</td>\n",
              "      <td>3.58</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10209</th>\n",
              "      <td>10209</td>\n",
              "      <td>2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>583.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>67.21</td>\n",
              "      <td>10.22</td>\n",
              "      <td>31.63</td>\n",
              "      <td>27.76</td>\n",
              "      <td>25.39</td>\n",
              "      <td>71.70</td>\n",
              "      <td>43.91</td>\n",
              "      <td>6.52</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.34</td>\n",
              "      <td>4.97</td>\n",
              "      <td>13.72</td>\n",
              "      <td>15.95</td>\n",
              "      <td>3.95</td>\n",
              "      <td>1.37</td>\n",
              "      <td>3.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.20</td>\n",
              "      <td>2.74</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.17</td>\n",
              "      <td>10.46</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.34</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.38</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.34</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.40</td>\n",
              "      <td>0.17</td>\n",
              "      <td>2.23</td>\n",
              "      <td>4.63</td>\n",
              "      <td>1.54</td>\n",
              "      <td>10.29</td>\n",
              "      <td>0.51</td>\n",
              "      <td>5.49</td>\n",
              "      <td>3.43</td>\n",
              "      <td>7.55</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.62</td>\n",
              "      <td>11.32</td>\n",
              "      <td>3.09</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.80</td>\n",
              "      <td>4.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10210</th>\n",
              "      <td>10210</td>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1562.0</td>\n",
              "      <td>98.88</td>\n",
              "      <td>68.89</td>\n",
              "      <td>8.78</td>\n",
              "      <td>84.49</td>\n",
              "      <td>17.16</td>\n",
              "      <td>31.63</td>\n",
              "      <td>74.65</td>\n",
              "      <td>41.36</td>\n",
              "      <td>4.67</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3.59</td>\n",
              "      <td>11.84</td>\n",
              "      <td>15.04</td>\n",
              "      <td>4.55</td>\n",
              "      <td>1.73</td>\n",
              "      <td>4.67</td>\n",
              "      <td>0.13</td>\n",
              "      <td>8.58</td>\n",
              "      <td>4.87</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.19</td>\n",
              "      <td>6.15</td>\n",
              "      <td>1.15</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.33</td>\n",
              "      <td>...</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>8.19</td>\n",
              "      <td>1.09</td>\n",
              "      <td>2.94</td>\n",
              "      <td>3.46</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.32</td>\n",
              "      <td>4.74</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.64</td>\n",
              "      <td>12.04</td>\n",
              "      <td>1.09</td>\n",
              "      <td>8.07</td>\n",
              "      <td>3.27</td>\n",
              "      <td>8.64</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.82</td>\n",
              "      <td>9.41</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10211</th>\n",
              "      <td>10211</td>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22700.0</td>\n",
              "      <td>98.26</td>\n",
              "      <td>67.19</td>\n",
              "      <td>42.82</td>\n",
              "      <td>62.20</td>\n",
              "      <td>29.99</td>\n",
              "      <td>30.45</td>\n",
              "      <td>77.54</td>\n",
              "      <td>43.37</td>\n",
              "      <td>5.77</td>\n",
              "      <td>2.15</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.43</td>\n",
              "      <td>3.63</td>\n",
              "      <td>9.59</td>\n",
              "      <td>17.15</td>\n",
              "      <td>4.29</td>\n",
              "      <td>1.96</td>\n",
              "      <td>5.48</td>\n",
              "      <td>0.44</td>\n",
              "      <td>7.26</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0.51</td>\n",
              "      <td>3.48</td>\n",
              "      <td>1.81</td>\n",
              "      <td>5.41</td>\n",
              "      <td>3.65</td>\n",
              "      <td>...</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>9.35</td>\n",
              "      <td>2.83</td>\n",
              "      <td>2.54</td>\n",
              "      <td>2.45</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.54</td>\n",
              "      <td>1.09</td>\n",
              "      <td>5.43</td>\n",
              "      <td>0.67</td>\n",
              "      <td>17.40</td>\n",
              "      <td>1.61</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.62</td>\n",
              "      <td>4.56</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.08</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.69</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.44</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.48</td>\n",
              "      <td>1.01</td>\n",
              "      <td>1.58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10212 rows × 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  date  Class M  Class S  ...  Quote  Apostro  Parenth  OtherP\n",
              "0               0  2012        1        0  ...   0.07     0.64     0.82    0.60\n",
              "1               1  2012        0        0  ...   0.49     0.12     4.38    1.46\n",
              "2               2  2003        0        0  ...   0.00     0.64     3.18    2.87\n",
              "3               3  1995        0        0  ...   0.22     0.16     0.64    2.18\n",
              "4               4  2007        0        0  ...   0.28     0.75     1.42    1.52\n",
              "...           ...   ...      ...      ...  ...    ...      ...      ...     ...\n",
              "10207       10207  2004        0        0  ...   0.00     0.76     1.72    1.23\n",
              "10208       10208  1994        0        0  ...   0.02     0.29     1.05    1.75\n",
              "10209       10209  2013        0        0  ...   0.34     0.00     4.80    4.63\n",
              "10210       10210  2009        0        0  ...   0.00     0.77     1.66    2.18\n",
              "10211       10211  2016        0        0  ...   0.10     0.48     1.01    1.58\n",
              "\n",
              "[10212 rows x 102 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "FNUqnWTFS4y2",
        "outputId": "c1fc1a18-f918-4a89-873a-aa3b1110f1a4"
      },
      "source": [
        "#Inspect and Clean the Data\n",
        "\n",
        "UN_Data.head(5)\n",
        "\n",
        "UN_Data.drop(['Unnamed: 0'], axis = 1, inplace= True)\n",
        "\n",
        "UN_Data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Class M</th>\n",
              "      <th>Class S</th>\n",
              "      <th>Class I</th>\n",
              "      <th>Class P</th>\n",
              "      <th>Class B</th>\n",
              "      <th>Policy Passed</th>\n",
              "      <th>Conflict Indicator</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>Authentic</th>\n",
              "      <th>Tone</th>\n",
              "      <th>WPS</th>\n",
              "      <th>Sixltr</th>\n",
              "      <th>Dic</th>\n",
              "      <th>function</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>ppron</th>\n",
              "      <th>i</th>\n",
              "      <th>we</th>\n",
              "      <th>you</th>\n",
              "      <th>shehe</th>\n",
              "      <th>they</th>\n",
              "      <th>ipron</th>\n",
              "      <th>article</th>\n",
              "      <th>prep</th>\n",
              "      <th>auxverb</th>\n",
              "      <th>adverb</th>\n",
              "      <th>conj</th>\n",
              "      <th>negate</th>\n",
              "      <th>verb</th>\n",
              "      <th>adj</th>\n",
              "      <th>compare</th>\n",
              "      <th>interrog</th>\n",
              "      <th>number</th>\n",
              "      <th>quant</th>\n",
              "      <th>affect</th>\n",
              "      <th>posemo</th>\n",
              "      <th>negemo</th>\n",
              "      <th>...</th>\n",
              "      <th>health</th>\n",
              "      <th>sexual</th>\n",
              "      <th>ingest</th>\n",
              "      <th>drives</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>achieve</th>\n",
              "      <th>power</th>\n",
              "      <th>reward</th>\n",
              "      <th>risk</th>\n",
              "      <th>focuspast</th>\n",
              "      <th>focuspresent</th>\n",
              "      <th>focusfuture</th>\n",
              "      <th>relativ</th>\n",
              "      <th>motion</th>\n",
              "      <th>space</th>\n",
              "      <th>time</th>\n",
              "      <th>work</th>\n",
              "      <th>leisure</th>\n",
              "      <th>home</th>\n",
              "      <th>money</th>\n",
              "      <th>relig</th>\n",
              "      <th>death</th>\n",
              "      <th>informal</th>\n",
              "      <th>swear</th>\n",
              "      <th>netspeak</th>\n",
              "      <th>assent</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>filler</th>\n",
              "      <th>AllPunc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Comma</th>\n",
              "      <th>Colon</th>\n",
              "      <th>SemiC</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Dash</th>\n",
              "      <th>Quote</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>Parenth</th>\n",
              "      <th>OtherP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2012</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>20075.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>68.60</td>\n",
              "      <td>14.49</td>\n",
              "      <td>96.27</td>\n",
              "      <td>27.24</td>\n",
              "      <td>34.34</td>\n",
              "      <td>77.56</td>\n",
              "      <td>42.78</td>\n",
              "      <td>5.44</td>\n",
              "      <td>2.15</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.24</td>\n",
              "      <td>3.29</td>\n",
              "      <td>11.21</td>\n",
              "      <td>16.88</td>\n",
              "      <td>3.82</td>\n",
              "      <td>1.51</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.26</td>\n",
              "      <td>6.50</td>\n",
              "      <td>6.69</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.35</td>\n",
              "      <td>2.43</td>\n",
              "      <td>1.01</td>\n",
              "      <td>7.11</td>\n",
              "      <td>5.98</td>\n",
              "      <td>1.11</td>\n",
              "      <td>...</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>11.52</td>\n",
              "      <td>3.10</td>\n",
              "      <td>2.91</td>\n",
              "      <td>3.37</td>\n",
              "      <td>1.12</td>\n",
              "      <td>2.16</td>\n",
              "      <td>1.37</td>\n",
              "      <td>4.82</td>\n",
              "      <td>0.97</td>\n",
              "      <td>12.55</td>\n",
              "      <td>1.22</td>\n",
              "      <td>8.62</td>\n",
              "      <td>2.69</td>\n",
              "      <td>6.46</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.80</td>\n",
              "      <td>4.04</td>\n",
              "      <td>4.34</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>822.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>74.84</td>\n",
              "      <td>17.74</td>\n",
              "      <td>41.15</td>\n",
              "      <td>20.05</td>\n",
              "      <td>26.64</td>\n",
              "      <td>72.38</td>\n",
              "      <td>43.19</td>\n",
              "      <td>5.23</td>\n",
              "      <td>2.92</td>\n",
              "      <td>1.46</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.31</td>\n",
              "      <td>12.77</td>\n",
              "      <td>16.55</td>\n",
              "      <td>5.35</td>\n",
              "      <td>1.70</td>\n",
              "      <td>2.19</td>\n",
              "      <td>0.36</td>\n",
              "      <td>10.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.73</td>\n",
              "      <td>5.72</td>\n",
              "      <td>1.46</td>\n",
              "      <td>1.70</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.36</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>8.64</td>\n",
              "      <td>2.31</td>\n",
              "      <td>1.09</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.49</td>\n",
              "      <td>1.95</td>\n",
              "      <td>6.08</td>\n",
              "      <td>1.70</td>\n",
              "      <td>12.90</td>\n",
              "      <td>1.34</td>\n",
              "      <td>7.54</td>\n",
              "      <td>4.26</td>\n",
              "      <td>5.84</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.60</td>\n",
              "      <td>11.56</td>\n",
              "      <td>3.04</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.12</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>66.07</td>\n",
              "      <td>4.02</td>\n",
              "      <td>31.19</td>\n",
              "      <td>19.62</td>\n",
              "      <td>29.30</td>\n",
              "      <td>72.93</td>\n",
              "      <td>38.54</td>\n",
              "      <td>2.55</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.23</td>\n",
              "      <td>13.38</td>\n",
              "      <td>17.20</td>\n",
              "      <td>4.14</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.37</td>\n",
              "      <td>3.82</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.00</td>\n",
              "      <td>13.06</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.96</td>\n",
              "      <td>8.92</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.96</td>\n",
              "      <td>7.32</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.46</td>\n",
              "      <td>2.55</td>\n",
              "      <td>0.32</td>\n",
              "      <td>10.83</td>\n",
              "      <td>0.64</td>\n",
              "      <td>6.05</td>\n",
              "      <td>3.82</td>\n",
              "      <td>10.51</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.08</td>\n",
              "      <td>20.70</td>\n",
              "      <td>3.50</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.23</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>3.18</td>\n",
              "      <td>2.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1995</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>17021.0</td>\n",
              "      <td>98.45</td>\n",
              "      <td>62.93</td>\n",
              "      <td>24.54</td>\n",
              "      <td>58.00</td>\n",
              "      <td>32.30</td>\n",
              "      <td>30.04</td>\n",
              "      <td>75.49</td>\n",
              "      <td>46.31</td>\n",
              "      <td>6.64</td>\n",
              "      <td>2.17</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.45</td>\n",
              "      <td>4.48</td>\n",
              "      <td>11.69</td>\n",
              "      <td>16.96</td>\n",
              "      <td>5.22</td>\n",
              "      <td>2.20</td>\n",
              "      <td>4.48</td>\n",
              "      <td>0.57</td>\n",
              "      <td>7.80</td>\n",
              "      <td>4.04</td>\n",
              "      <td>1.46</td>\n",
              "      <td>0.79</td>\n",
              "      <td>4.00</td>\n",
              "      <td>1.33</td>\n",
              "      <td>4.85</td>\n",
              "      <td>3.25</td>\n",
              "      <td>1.55</td>\n",
              "      <td>...</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.09</td>\n",
              "      <td>7.50</td>\n",
              "      <td>2.27</td>\n",
              "      <td>1.74</td>\n",
              "      <td>2.61</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1.59</td>\n",
              "      <td>5.12</td>\n",
              "      <td>1.11</td>\n",
              "      <td>12.84</td>\n",
              "      <td>0.95</td>\n",
              "      <td>8.61</td>\n",
              "      <td>3.24</td>\n",
              "      <td>4.57</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.34</td>\n",
              "      <td>3.44</td>\n",
              "      <td>4.91</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.64</td>\n",
              "      <td>2.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2007</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9289.0</td>\n",
              "      <td>98.94</td>\n",
              "      <td>60.45</td>\n",
              "      <td>8.85</td>\n",
              "      <td>70.17</td>\n",
              "      <td>22.82</td>\n",
              "      <td>32.95</td>\n",
              "      <td>78.23</td>\n",
              "      <td>44.38</td>\n",
              "      <td>4.40</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.22</td>\n",
              "      <td>3.58</td>\n",
              "      <td>13.27</td>\n",
              "      <td>15.19</td>\n",
              "      <td>5.80</td>\n",
              "      <td>1.71</td>\n",
              "      <td>4.61</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.08</td>\n",
              "      <td>3.90</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0.44</td>\n",
              "      <td>4.94</td>\n",
              "      <td>1.35</td>\n",
              "      <td>3.93</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.71</td>\n",
              "      <td>...</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.01</td>\n",
              "      <td>1.17</td>\n",
              "      <td>3.18</td>\n",
              "      <td>3.89</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.70</td>\n",
              "      <td>5.12</td>\n",
              "      <td>2.73</td>\n",
              "      <td>0.86</td>\n",
              "      <td>11.68</td>\n",
              "      <td>1.35</td>\n",
              "      <td>7.12</td>\n",
              "      <td>3.12</td>\n",
              "      <td>7.94</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.38</td>\n",
              "      <td>5.37</td>\n",
              "      <td>3.80</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.42</td>\n",
              "      <td>1.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10207</th>\n",
              "      <td>2004</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4059.0</td>\n",
              "      <td>98.95</td>\n",
              "      <td>58.85</td>\n",
              "      <td>3.46</td>\n",
              "      <td>71.55</td>\n",
              "      <td>23.46</td>\n",
              "      <td>31.44</td>\n",
              "      <td>75.41</td>\n",
              "      <td>45.31</td>\n",
              "      <td>4.73</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.12</td>\n",
              "      <td>3.99</td>\n",
              "      <td>12.59</td>\n",
              "      <td>16.38</td>\n",
              "      <td>6.16</td>\n",
              "      <td>1.58</td>\n",
              "      <td>4.07</td>\n",
              "      <td>1.01</td>\n",
              "      <td>9.04</td>\n",
              "      <td>4.09</td>\n",
              "      <td>1.21</td>\n",
              "      <td>0.64</td>\n",
              "      <td>5.15</td>\n",
              "      <td>1.13</td>\n",
              "      <td>4.51</td>\n",
              "      <td>3.40</td>\n",
              "      <td>0.96</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>7.71</td>\n",
              "      <td>1.43</td>\n",
              "      <td>1.90</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.49</td>\n",
              "      <td>1.03</td>\n",
              "      <td>4.88</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.83</td>\n",
              "      <td>0.64</td>\n",
              "      <td>6.01</td>\n",
              "      <td>3.13</td>\n",
              "      <td>6.43</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>2.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.46</td>\n",
              "      <td>5.57</td>\n",
              "      <td>3.72</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1.72</td>\n",
              "      <td>1.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10208</th>\n",
              "      <td>1994</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8210.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>56.93</td>\n",
              "      <td>10.94</td>\n",
              "      <td>62.72</td>\n",
              "      <td>25.42</td>\n",
              "      <td>30.78</td>\n",
              "      <td>77.17</td>\n",
              "      <td>45.35</td>\n",
              "      <td>4.31</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.21</td>\n",
              "      <td>3.74</td>\n",
              "      <td>13.01</td>\n",
              "      <td>17.15</td>\n",
              "      <td>5.57</td>\n",
              "      <td>1.49</td>\n",
              "      <td>4.71</td>\n",
              "      <td>0.35</td>\n",
              "      <td>7.94</td>\n",
              "      <td>3.67</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.88</td>\n",
              "      <td>5.31</td>\n",
              "      <td>1.32</td>\n",
              "      <td>3.02</td>\n",
              "      <td>2.46</td>\n",
              "      <td>0.51</td>\n",
              "      <td>...</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.13</td>\n",
              "      <td>7.10</td>\n",
              "      <td>0.71</td>\n",
              "      <td>2.02</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.55</td>\n",
              "      <td>4.95</td>\n",
              "      <td>2.36</td>\n",
              "      <td>0.82</td>\n",
              "      <td>12.30</td>\n",
              "      <td>0.94</td>\n",
              "      <td>7.00</td>\n",
              "      <td>4.26</td>\n",
              "      <td>8.36</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.06</td>\n",
              "      <td>3.74</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.63</td>\n",
              "      <td>4.53</td>\n",
              "      <td>3.58</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1.05</td>\n",
              "      <td>1.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10209</th>\n",
              "      <td>2013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>583.0</td>\n",
              "      <td>99.00</td>\n",
              "      <td>67.21</td>\n",
              "      <td>10.22</td>\n",
              "      <td>31.63</td>\n",
              "      <td>27.76</td>\n",
              "      <td>25.39</td>\n",
              "      <td>71.70</td>\n",
              "      <td>43.91</td>\n",
              "      <td>6.52</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.34</td>\n",
              "      <td>4.97</td>\n",
              "      <td>13.72</td>\n",
              "      <td>15.95</td>\n",
              "      <td>3.95</td>\n",
              "      <td>1.37</td>\n",
              "      <td>3.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.20</td>\n",
              "      <td>2.74</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.17</td>\n",
              "      <td>10.46</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.38</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.34</td>\n",
              "      <td>3.43</td>\n",
              "      <td>2.40</td>\n",
              "      <td>0.17</td>\n",
              "      <td>2.23</td>\n",
              "      <td>4.63</td>\n",
              "      <td>1.54</td>\n",
              "      <td>10.29</td>\n",
              "      <td>0.51</td>\n",
              "      <td>5.49</td>\n",
              "      <td>3.43</td>\n",
              "      <td>7.55</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.62</td>\n",
              "      <td>11.32</td>\n",
              "      <td>3.09</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.80</td>\n",
              "      <td>4.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10210</th>\n",
              "      <td>2009</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1562.0</td>\n",
              "      <td>98.88</td>\n",
              "      <td>68.89</td>\n",
              "      <td>8.78</td>\n",
              "      <td>84.49</td>\n",
              "      <td>17.16</td>\n",
              "      <td>31.63</td>\n",
              "      <td>74.65</td>\n",
              "      <td>41.36</td>\n",
              "      <td>4.67</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.45</td>\n",
              "      <td>3.59</td>\n",
              "      <td>11.84</td>\n",
              "      <td>15.04</td>\n",
              "      <td>4.55</td>\n",
              "      <td>1.73</td>\n",
              "      <td>4.67</td>\n",
              "      <td>0.13</td>\n",
              "      <td>8.58</td>\n",
              "      <td>4.87</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.19</td>\n",
              "      <td>6.15</td>\n",
              "      <td>1.15</td>\n",
              "      <td>3.33</td>\n",
              "      <td>3.33</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>8.19</td>\n",
              "      <td>1.09</td>\n",
              "      <td>2.94</td>\n",
              "      <td>3.46</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.32</td>\n",
              "      <td>4.74</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.64</td>\n",
              "      <td>12.04</td>\n",
              "      <td>1.09</td>\n",
              "      <td>8.07</td>\n",
              "      <td>3.27</td>\n",
              "      <td>8.64</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.82</td>\n",
              "      <td>9.41</td>\n",
              "      <td>2.82</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.77</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10211</th>\n",
              "      <td>2016</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>22700.0</td>\n",
              "      <td>98.26</td>\n",
              "      <td>67.19</td>\n",
              "      <td>42.82</td>\n",
              "      <td>62.20</td>\n",
              "      <td>29.99</td>\n",
              "      <td>30.45</td>\n",
              "      <td>77.54</td>\n",
              "      <td>43.37</td>\n",
              "      <td>5.77</td>\n",
              "      <td>2.15</td>\n",
              "      <td>0.35</td>\n",
              "      <td>1.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.43</td>\n",
              "      <td>3.63</td>\n",
              "      <td>9.59</td>\n",
              "      <td>17.15</td>\n",
              "      <td>4.29</td>\n",
              "      <td>1.96</td>\n",
              "      <td>5.48</td>\n",
              "      <td>0.44</td>\n",
              "      <td>7.26</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0.51</td>\n",
              "      <td>3.48</td>\n",
              "      <td>1.81</td>\n",
              "      <td>5.41</td>\n",
              "      <td>3.65</td>\n",
              "      <td>1.73</td>\n",
              "      <td>...</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>9.35</td>\n",
              "      <td>2.83</td>\n",
              "      <td>2.54</td>\n",
              "      <td>2.45</td>\n",
              "      <td>1.25</td>\n",
              "      <td>1.54</td>\n",
              "      <td>1.09</td>\n",
              "      <td>5.43</td>\n",
              "      <td>0.67</td>\n",
              "      <td>17.40</td>\n",
              "      <td>1.61</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.62</td>\n",
              "      <td>4.56</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.08</td>\n",
              "      <td>3.67</td>\n",
              "      <td>3.69</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.44</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.48</td>\n",
              "      <td>1.01</td>\n",
              "      <td>1.58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10212 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       date  Class M  Class S  Class I  ...  Quote  Apostro  Parenth  OtherP\n",
              "0      2012        1        0        0  ...   0.07     0.64     0.82    0.60\n",
              "1      2012        0        0        3  ...   0.49     0.12     4.38    1.46\n",
              "2      2003        0        0        0  ...   0.00     0.64     3.18    2.87\n",
              "3      1995        0        0        0  ...   0.22     0.16     0.64    2.18\n",
              "4      2007        0        0        0  ...   0.28     0.75     1.42    1.52\n",
              "...     ...      ...      ...      ...  ...    ...      ...      ...     ...\n",
              "10207  2004        0        0        0  ...   0.00     0.76     1.72    1.23\n",
              "10208  1994        0        0        0  ...   0.02     0.29     1.05    1.75\n",
              "10209  2013        0        0        0  ...   0.34     0.00     4.80    4.63\n",
              "10210  2009        0        0        0  ...   0.00     0.77     1.66    2.18\n",
              "10211  2016        0        0        0  ...   0.10     0.48     1.01    1.58\n",
              "\n",
              "[10212 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "tkn1c4RqeEdO",
        "outputId": "e24cc6fc-2a9c-44ce-d8dc-240a18519d98"
      },
      "source": [
        "#Inspect the data by key descriptive statistics\n",
        "\n",
        "UN_Data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Class M</th>\n",
              "      <th>Class S</th>\n",
              "      <th>Class I</th>\n",
              "      <th>Class P</th>\n",
              "      <th>Class B</th>\n",
              "      <th>Policy Passed</th>\n",
              "      <th>Conflict Indicator</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>Authentic</th>\n",
              "      <th>Tone</th>\n",
              "      <th>WPS</th>\n",
              "      <th>Sixltr</th>\n",
              "      <th>Dic</th>\n",
              "      <th>function</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>ppron</th>\n",
              "      <th>i</th>\n",
              "      <th>we</th>\n",
              "      <th>you</th>\n",
              "      <th>shehe</th>\n",
              "      <th>they</th>\n",
              "      <th>ipron</th>\n",
              "      <th>article</th>\n",
              "      <th>prep</th>\n",
              "      <th>auxverb</th>\n",
              "      <th>adverb</th>\n",
              "      <th>conj</th>\n",
              "      <th>negate</th>\n",
              "      <th>verb</th>\n",
              "      <th>adj</th>\n",
              "      <th>compare</th>\n",
              "      <th>interrog</th>\n",
              "      <th>number</th>\n",
              "      <th>quant</th>\n",
              "      <th>affect</th>\n",
              "      <th>posemo</th>\n",
              "      <th>negemo</th>\n",
              "      <th>...</th>\n",
              "      <th>health</th>\n",
              "      <th>sexual</th>\n",
              "      <th>ingest</th>\n",
              "      <th>drives</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>achieve</th>\n",
              "      <th>power</th>\n",
              "      <th>reward</th>\n",
              "      <th>risk</th>\n",
              "      <th>focuspast</th>\n",
              "      <th>focuspresent</th>\n",
              "      <th>focusfuture</th>\n",
              "      <th>relativ</th>\n",
              "      <th>motion</th>\n",
              "      <th>space</th>\n",
              "      <th>time</th>\n",
              "      <th>work</th>\n",
              "      <th>leisure</th>\n",
              "      <th>home</th>\n",
              "      <th>money</th>\n",
              "      <th>relig</th>\n",
              "      <th>death</th>\n",
              "      <th>informal</th>\n",
              "      <th>swear</th>\n",
              "      <th>netspeak</th>\n",
              "      <th>assent</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>filler</th>\n",
              "      <th>AllPunc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Comma</th>\n",
              "      <th>Colon</th>\n",
              "      <th>SemiC</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Dash</th>\n",
              "      <th>Quote</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>Parenth</th>\n",
              "      <th>OtherP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10212.000000</td>\n",
              "      <td>10212.000000</td>\n",
              "      <td>10212.000000</td>\n",
              "      <td>10212.000000</td>\n",
              "      <td>10212.000000</td>\n",
              "      <td>10212.000000</td>\n",
              "      <td>10212.00000</td>\n",
              "      <td>10212.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2005.852135</td>\n",
              "      <td>0.032805</td>\n",
              "      <td>0.009205</td>\n",
              "      <td>0.168625</td>\n",
              "      <td>0.051900</td>\n",
              "      <td>0.021641</td>\n",
              "      <td>0.13964</td>\n",
              "      <td>0.457697</td>\n",
              "      <td>9442.204220</td>\n",
              "      <td>98.190282</td>\n",
              "      <td>66.875703</td>\n",
              "      <td>12.898395</td>\n",
              "      <td>56.709126</td>\n",
              "      <td>25.304285</td>\n",
              "      <td>32.377559</td>\n",
              "      <td>74.262346</td>\n",
              "      <td>43.025112</td>\n",
              "      <td>5.245421</td>\n",
              "      <td>1.742787</td>\n",
              "      <td>0.269233</td>\n",
              "      <td>0.502975</td>\n",
              "      <td>0.028574</td>\n",
              "      <td>0.499537</td>\n",
              "      <td>0.442485</td>\n",
              "      <td>3.502526</td>\n",
              "      <td>10.834657</td>\n",
              "      <td>16.137896</td>\n",
              "      <td>5.045694</td>\n",
              "      <td>1.724823</td>\n",
              "      <td>4.734316</td>\n",
              "      <td>0.481271</td>\n",
              "      <td>8.091436</td>\n",
              "      <td>3.773042</td>\n",
              "      <td>1.521560</td>\n",
              "      <td>0.574251</td>\n",
              "      <td>4.922159</td>\n",
              "      <td>1.371374</td>\n",
              "      <td>4.117795</td>\n",
              "      <td>2.865873</td>\n",
              "      <td>1.176426</td>\n",
              "      <td>...</td>\n",
              "      <td>0.387821</td>\n",
              "      <td>0.057502</td>\n",
              "      <td>0.123219</td>\n",
              "      <td>8.555518</td>\n",
              "      <td>1.881719</td>\n",
              "      <td>1.977253</td>\n",
              "      <td>3.718665</td>\n",
              "      <td>0.936162</td>\n",
              "      <td>0.786315</td>\n",
              "      <td>3.606229</td>\n",
              "      <td>3.371183</td>\n",
              "      <td>0.627266</td>\n",
              "      <td>11.893252</td>\n",
              "      <td>1.004798</td>\n",
              "      <td>7.944224</td>\n",
              "      <td>2.889733</td>\n",
              "      <td>6.591531</td>\n",
              "      <td>0.289160</td>\n",
              "      <td>0.150839</td>\n",
              "      <td>1.037590</td>\n",
              "      <td>0.151080</td>\n",
              "      <td>0.091241</td>\n",
              "      <td>0.195391</td>\n",
              "      <td>0.001237</td>\n",
              "      <td>0.098407</td>\n",
              "      <td>0.024945</td>\n",
              "      <td>0.070841</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>16.753078</td>\n",
              "      <td>5.740774</td>\n",
              "      <td>5.193233</td>\n",
              "      <td>0.303105</td>\n",
              "      <td>0.144628</td>\n",
              "      <td>0.031364</td>\n",
              "      <td>0.014043</td>\n",
              "      <td>1.154469</td>\n",
              "      <td>0.260464</td>\n",
              "      <td>0.397916</td>\n",
              "      <td>1.744508</td>\n",
              "      <td>1.768528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.542111</td>\n",
              "      <td>0.229542</td>\n",
              "      <td>0.099521</td>\n",
              "      <td>0.861341</td>\n",
              "      <td>0.307019</td>\n",
              "      <td>0.156540</td>\n",
              "      <td>0.34663</td>\n",
              "      <td>0.498232</td>\n",
              "      <td>7786.325195</td>\n",
              "      <td>1.085791</td>\n",
              "      <td>6.906079</td>\n",
              "      <td>7.298162</td>\n",
              "      <td>18.251199</td>\n",
              "      <td>11.234321</td>\n",
              "      <td>4.117168</td>\n",
              "      <td>7.467984</td>\n",
              "      <td>5.098223</td>\n",
              "      <td>1.688333</td>\n",
              "      <td>1.063348</td>\n",
              "      <td>0.414985</td>\n",
              "      <td>0.757474</td>\n",
              "      <td>0.087247</td>\n",
              "      <td>0.419017</td>\n",
              "      <td>0.233972</td>\n",
              "      <td>0.874889</td>\n",
              "      <td>1.535397</td>\n",
              "      <td>1.917897</td>\n",
              "      <td>1.070806</td>\n",
              "      <td>0.565009</td>\n",
              "      <td>1.088154</td>\n",
              "      <td>0.249289</td>\n",
              "      <td>1.463034</td>\n",
              "      <td>0.816011</td>\n",
              "      <td>0.440595</td>\n",
              "      <td>0.251958</td>\n",
              "      <td>3.102461</td>\n",
              "      <td>0.412431</td>\n",
              "      <td>1.603232</td>\n",
              "      <td>1.095339</td>\n",
              "      <td>0.849716</td>\n",
              "      <td>...</td>\n",
              "      <td>0.528318</td>\n",
              "      <td>0.273305</td>\n",
              "      <td>0.185681</td>\n",
              "      <td>2.023621</td>\n",
              "      <td>1.005218</td>\n",
              "      <td>0.811497</td>\n",
              "      <td>1.141328</td>\n",
              "      <td>0.374087</td>\n",
              "      <td>0.474275</td>\n",
              "      <td>1.683420</td>\n",
              "      <td>1.691763</td>\n",
              "      <td>0.418849</td>\n",
              "      <td>2.030896</td>\n",
              "      <td>0.389119</td>\n",
              "      <td>1.632927</td>\n",
              "      <td>0.833989</td>\n",
              "      <td>2.061655</td>\n",
              "      <td>0.225462</td>\n",
              "      <td>0.138124</td>\n",
              "      <td>1.012639</td>\n",
              "      <td>0.200418</td>\n",
              "      <td>0.143312</td>\n",
              "      <td>0.164434</td>\n",
              "      <td>0.005468</td>\n",
              "      <td>0.147668</td>\n",
              "      <td>0.030712</td>\n",
              "      <td>0.051828</td>\n",
              "      <td>0.001865</td>\n",
              "      <td>8.792071</td>\n",
              "      <td>2.753868</td>\n",
              "      <td>3.290078</td>\n",
              "      <td>0.466117</td>\n",
              "      <td>0.190811</td>\n",
              "      <td>0.105541</td>\n",
              "      <td>0.713727</td>\n",
              "      <td>0.643019</td>\n",
              "      <td>0.439789</td>\n",
              "      <td>1.273977</td>\n",
              "      <td>1.466890</td>\n",
              "      <td>4.337472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1993.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>80.460000</td>\n",
              "      <td>27.030000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.070000</td>\n",
              "      <td>3.650000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.210000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1999.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3838.500000</td>\n",
              "      <td>97.880000</td>\n",
              "      <td>62.192500</td>\n",
              "      <td>7.172500</td>\n",
              "      <td>43.650000</td>\n",
              "      <td>22.610000</td>\n",
              "      <td>30.550000</td>\n",
              "      <td>73.130000</td>\n",
              "      <td>41.420000</td>\n",
              "      <td>4.070000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>2.970000</td>\n",
              "      <td>9.920000</td>\n",
              "      <td>15.690000</td>\n",
              "      <td>4.460000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>4.160000</td>\n",
              "      <td>0.310000</td>\n",
              "      <td>7.380000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>1.260000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>3.142500</td>\n",
              "      <td>1.140000</td>\n",
              "      <td>3.070000</td>\n",
              "      <td>2.190000</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>7.180000</td>\n",
              "      <td>1.130000</td>\n",
              "      <td>1.400000</td>\n",
              "      <td>3.022500</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>1.770000</td>\n",
              "      <td>2.070000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>10.620000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>6.930000</td>\n",
              "      <td>2.320000</td>\n",
              "      <td>5.040000</td>\n",
              "      <td>0.170000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>12.650000</td>\n",
              "      <td>4.290000</td>\n",
              "      <td>3.980000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.060000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.740000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2006.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7439.500000</td>\n",
              "      <td>98.440000</td>\n",
              "      <td>66.540000</td>\n",
              "      <td>11.630000</td>\n",
              "      <td>57.030000</td>\n",
              "      <td>24.940000</td>\n",
              "      <td>32.105000</td>\n",
              "      <td>75.960000</td>\n",
              "      <td>43.910000</td>\n",
              "      <td>5.070000</td>\n",
              "      <td>1.390000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>0.430000</td>\n",
              "      <td>3.550000</td>\n",
              "      <td>10.880000</td>\n",
              "      <td>16.510000</td>\n",
              "      <td>5.080000</td>\n",
              "      <td>1.770000</td>\n",
              "      <td>4.940000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>8.130000</td>\n",
              "      <td>3.780000</td>\n",
              "      <td>1.550000</td>\n",
              "      <td>0.570000</td>\n",
              "      <td>4.110000</td>\n",
              "      <td>1.390000</td>\n",
              "      <td>4.160000</td>\n",
              "      <td>2.890000</td>\n",
              "      <td>1.010000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>8.540000</td>\n",
              "      <td>1.620000</td>\n",
              "      <td>2.040000</td>\n",
              "      <td>3.540000</td>\n",
              "      <td>0.910000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>4.070000</td>\n",
              "      <td>2.680000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>12.060000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.810000</td>\n",
              "      <td>6.260000</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.510000</td>\n",
              "      <td>5.150000</td>\n",
              "      <td>4.440000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.040000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>1.330000</td>\n",
              "      <td>1.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2012.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12438.750000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>71.617500</td>\n",
              "      <td>18.040000</td>\n",
              "      <td>69.637500</td>\n",
              "      <td>27.527500</td>\n",
              "      <td>33.800000</td>\n",
              "      <td>78.250000</td>\n",
              "      <td>45.987500</td>\n",
              "      <td>6.400000</td>\n",
              "      <td>2.410000</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>1.120000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>4.070000</td>\n",
              "      <td>11.840000</td>\n",
              "      <td>17.140000</td>\n",
              "      <td>5.670000</td>\n",
              "      <td>2.110000</td>\n",
              "      <td>5.470000</td>\n",
              "      <td>0.630000</td>\n",
              "      <td>8.897500</td>\n",
              "      <td>4.260000</td>\n",
              "      <td>1.820000</td>\n",
              "      <td>0.730000</td>\n",
              "      <td>5.710000</td>\n",
              "      <td>1.610000</td>\n",
              "      <td>5.110000</td>\n",
              "      <td>3.560000</td>\n",
              "      <td>1.630000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.440000</td>\n",
              "      <td>0.030000</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>9.930000</td>\n",
              "      <td>2.620000</td>\n",
              "      <td>2.550000</td>\n",
              "      <td>4.140000</td>\n",
              "      <td>1.160000</td>\n",
              "      <td>1.070000</td>\n",
              "      <td>4.970000</td>\n",
              "      <td>5.040000</td>\n",
              "      <td>0.870000</td>\n",
              "      <td>13.300000</td>\n",
              "      <td>1.230000</td>\n",
              "      <td>8.990000</td>\n",
              "      <td>3.360000</td>\n",
              "      <td>8.030000</td>\n",
              "      <td>0.370000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>1.320000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.040000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17.770000</td>\n",
              "      <td>6.010000</td>\n",
              "      <td>4.990000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.190000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.380000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.490000</td>\n",
              "      <td>2.090000</td>\n",
              "      <td>2.070000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2020.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>74776.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>93.720000</td>\n",
              "      <td>62.870000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>1041.000000</td>\n",
              "      <td>70.930000</td>\n",
              "      <td>87.450000</td>\n",
              "      <td>55.150000</td>\n",
              "      <td>13.330000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>3.950000</td>\n",
              "      <td>2.650000</td>\n",
              "      <td>3.820000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>16.870000</td>\n",
              "      <td>20.300000</td>\n",
              "      <td>14.230000</td>\n",
              "      <td>4.200000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>15.900000</td>\n",
              "      <td>8.390000</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>4.420000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>8.010000</td>\n",
              "      <td>6.480000</td>\n",
              "      <td>...</td>\n",
              "      <td>6.180000</td>\n",
              "      <td>4.150000</td>\n",
              "      <td>3.560000</td>\n",
              "      <td>15.700000</td>\n",
              "      <td>7.370000</td>\n",
              "      <td>5.210000</td>\n",
              "      <td>9.460000</td>\n",
              "      <td>3.070000</td>\n",
              "      <td>4.140000</td>\n",
              "      <td>9.430000</td>\n",
              "      <td>10.040000</td>\n",
              "      <td>4.170000</td>\n",
              "      <td>20.690000</td>\n",
              "      <td>4.800000</td>\n",
              "      <td>15.610000</td>\n",
              "      <td>14.810000</td>\n",
              "      <td>14.510000</td>\n",
              "      <td>4.380000</td>\n",
              "      <td>3.290000</td>\n",
              "      <td>7.370000</td>\n",
              "      <td>3.380000</td>\n",
              "      <td>3.780000</td>\n",
              "      <td>3.380000</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>3.280000</td>\n",
              "      <td>0.720000</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>265.190000</td>\n",
              "      <td>37.020000</td>\n",
              "      <td>47.760000</td>\n",
              "      <td>22.710000</td>\n",
              "      <td>11.150000</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>49.920000</td>\n",
              "      <td>22.880000</td>\n",
              "      <td>14.030000</td>\n",
              "      <td>89.620000</td>\n",
              "      <td>17.900000</td>\n",
              "      <td>213.040000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               date       Class M  ...       Parenth        OtherP\n",
              "count  10212.000000  10212.000000  ...  10190.000000  10190.000000\n",
              "mean    2005.852135      0.032805  ...      1.744508      1.768528\n",
              "std        7.542111      0.229542  ...      1.466890      4.337472\n",
              "min     1993.000000      0.000000  ...      0.130000      0.010000\n",
              "25%     1999.000000      0.000000  ...      0.860000      0.740000\n",
              "50%     2006.000000      0.000000  ...      1.330000      1.170000\n",
              "75%     2012.000000      0.000000  ...      2.090000      2.070000\n",
              "max     2020.000000      9.000000  ...     17.900000    213.040000\n",
              "\n",
              "[8 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "lUEe-fLzAZ8a",
        "outputId": "35afe24b-f0e6-4364-faae-663f216883f1"
      },
      "source": [
        "#Normalize the data \n",
        "\n",
        "UN_Data1 = tf.keras.utils.normalize(UN_Data.drop(columns = ['Class I']))\n",
        "\n",
        "UN_Data1[\"Class I\"] = UN_Data['Class I']\n",
        "\n",
        "UN_Data1.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>Class M</th>\n",
              "      <th>Class S</th>\n",
              "      <th>Class P</th>\n",
              "      <th>Class B</th>\n",
              "      <th>Policy Passed</th>\n",
              "      <th>Conflict Indicator</th>\n",
              "      <th>WC</th>\n",
              "      <th>Analytic</th>\n",
              "      <th>Clout</th>\n",
              "      <th>Authentic</th>\n",
              "      <th>Tone</th>\n",
              "      <th>WPS</th>\n",
              "      <th>Sixltr</th>\n",
              "      <th>Dic</th>\n",
              "      <th>function</th>\n",
              "      <th>pronoun</th>\n",
              "      <th>ppron</th>\n",
              "      <th>i</th>\n",
              "      <th>we</th>\n",
              "      <th>you</th>\n",
              "      <th>shehe</th>\n",
              "      <th>they</th>\n",
              "      <th>ipron</th>\n",
              "      <th>article</th>\n",
              "      <th>prep</th>\n",
              "      <th>auxverb</th>\n",
              "      <th>adverb</th>\n",
              "      <th>conj</th>\n",
              "      <th>negate</th>\n",
              "      <th>verb</th>\n",
              "      <th>adj</th>\n",
              "      <th>compare</th>\n",
              "      <th>interrog</th>\n",
              "      <th>number</th>\n",
              "      <th>quant</th>\n",
              "      <th>affect</th>\n",
              "      <th>posemo</th>\n",
              "      <th>negemo</th>\n",
              "      <th>anx</th>\n",
              "      <th>...</th>\n",
              "      <th>sexual</th>\n",
              "      <th>ingest</th>\n",
              "      <th>drives</th>\n",
              "      <th>affiliation</th>\n",
              "      <th>achieve</th>\n",
              "      <th>power</th>\n",
              "      <th>reward</th>\n",
              "      <th>risk</th>\n",
              "      <th>focuspast</th>\n",
              "      <th>focuspresent</th>\n",
              "      <th>focusfuture</th>\n",
              "      <th>relativ</th>\n",
              "      <th>motion</th>\n",
              "      <th>space</th>\n",
              "      <th>time</th>\n",
              "      <th>work</th>\n",
              "      <th>leisure</th>\n",
              "      <th>home</th>\n",
              "      <th>money</th>\n",
              "      <th>relig</th>\n",
              "      <th>death</th>\n",
              "      <th>informal</th>\n",
              "      <th>swear</th>\n",
              "      <th>netspeak</th>\n",
              "      <th>assent</th>\n",
              "      <th>nonflu</th>\n",
              "      <th>filler</th>\n",
              "      <th>AllPunc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Comma</th>\n",
              "      <th>Colon</th>\n",
              "      <th>SemiC</th>\n",
              "      <th>QMark</th>\n",
              "      <th>Exclam</th>\n",
              "      <th>Dash</th>\n",
              "      <th>Quote</th>\n",
              "      <th>Apostro</th>\n",
              "      <th>Parenth</th>\n",
              "      <th>OtherP</th>\n",
              "      <th>Class I</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>1.019000e+04</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>1.019000e+04</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>10190.000000</td>\n",
              "      <td>1.019000e+04</td>\n",
              "      <td>10212.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.345714</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.881989</td>\n",
              "      <td>0.016961</td>\n",
              "      <td>0.011428</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.009263</td>\n",
              "      <td>0.004283</td>\n",
              "      <td>0.005549</td>\n",
              "      <td>0.012424</td>\n",
              "      <td>0.007155</td>\n",
              "      <td>0.000806</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.001912</td>\n",
              "      <td>0.002711</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000735</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.001106</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.001355</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.001978</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.001291</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>0.001211</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>1.666496e-07</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>1.862753e-08</td>\n",
              "      <td>0.003400</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>4.345803e-04</td>\n",
              "      <td>0.168625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.252698</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>0.193529</td>\n",
              "      <td>0.012448</td>\n",
              "      <td>0.008500</td>\n",
              "      <td>0.001883</td>\n",
              "      <td>0.006951</td>\n",
              "      <td>0.005428</td>\n",
              "      <td>0.004126</td>\n",
              "      <td>0.008650</td>\n",
              "      <td>0.004983</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.001952</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.001017</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.001544</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000910</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.001433</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>1.366956e-06</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>6.284069e-07</td>\n",
              "      <td>0.004333</td>\n",
              "      <td>0.001652</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.000308</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>1.748792e-03</td>\n",
              "      <td>0.861341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.026924</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021901</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.001053</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>4.473887e-07</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.159519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.885374</td>\n",
              "      <td>0.007792</td>\n",
              "      <td>0.005150</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.004105</td>\n",
              "      <td>0.002035</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.005943</td>\n",
              "      <td>0.003449</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.000372</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>6.634241e-05</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.260213</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.965297</td>\n",
              "      <td>0.012765</td>\n",
              "      <td>0.008537</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.007303</td>\n",
              "      <td>0.003101</td>\n",
              "      <td>0.004253</td>\n",
              "      <td>0.009673</td>\n",
              "      <td>0.005503</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.002074</td>\n",
              "      <td>0.000661</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.001106</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000482</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000559</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000842</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.001940</td>\n",
              "      <td>0.000678</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>1.553934e-04</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.463364</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.987100</td>\n",
              "      <td>0.022704</td>\n",
              "      <td>0.015068</td>\n",
              "      <td>0.002197</td>\n",
              "      <td>0.012538</td>\n",
              "      <td>0.005526</td>\n",
              "      <td>0.007345</td>\n",
              "      <td>0.016570</td>\n",
              "      <td>0.009498</td>\n",
              "      <td>0.001015</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.002524</td>\n",
              "      <td>0.003560</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.001826</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.001766</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000373</td>\n",
              "      <td>0.000822</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>0.000581</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.002584</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.000627</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003955</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.001102</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>3.987904e-04</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.997075</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.001449</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.999635</td>\n",
              "      <td>0.049344</td>\n",
              "      <td>0.040877</td>\n",
              "      <td>0.027255</td>\n",
              "      <td>0.049080</td>\n",
              "      <td>0.418849</td>\n",
              "      <td>0.032069</td>\n",
              "      <td>0.040841</td>\n",
              "      <td>0.023847</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>0.003338</td>\n",
              "      <td>0.001471</td>\n",
              "      <td>0.001468</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.005515</td>\n",
              "      <td>0.008143</td>\n",
              "      <td>0.009868</td>\n",
              "      <td>0.007035</td>\n",
              "      <td>0.001735</td>\n",
              "      <td>0.003817</td>\n",
              "      <td>0.002548</td>\n",
              "      <td>0.007860</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.001735</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>0.025482</td>\n",
              "      <td>0.002037</td>\n",
              "      <td>0.003568</td>\n",
              "      <td>0.003012</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.000995</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.006725</td>\n",
              "      <td>0.003621</td>\n",
              "      <td>0.002588</td>\n",
              "      <td>0.004689</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.004056</td>\n",
              "      <td>0.004085</td>\n",
              "      <td>0.002070</td>\n",
              "      <td>0.010298</td>\n",
              "      <td>0.002212</td>\n",
              "      <td>0.006174</td>\n",
              "      <td>0.007355</td>\n",
              "      <td>0.006882</td>\n",
              "      <td>0.001133</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.002704</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001812</td>\n",
              "      <td>0.000953</td>\n",
              "      <td>3.947587e-05</td>\n",
              "      <td>0.000953</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>5.077514e-05</td>\n",
              "      <td>0.119130</td>\n",
              "      <td>0.018193</td>\n",
              "      <td>0.015233</td>\n",
              "      <td>0.005398</td>\n",
              "      <td>0.004388</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.010615</td>\n",
              "      <td>0.010232</td>\n",
              "      <td>0.006303</td>\n",
              "      <td>0.019591</td>\n",
              "      <td>0.008041</td>\n",
              "      <td>9.570280e-02</td>\n",
              "      <td>28.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               date       Class M  ...        OtherP       Class I\n",
              "count  10190.000000  10190.000000  ...  1.019000e+04  10212.000000\n",
              "mean       0.345714      0.000004  ...  4.345803e-04      0.168625\n",
              "std        0.252698      0.000030  ...  1.748792e-03      0.861341\n",
              "min        0.026924      0.000000  ...  4.473887e-07      0.000000\n",
              "25%        0.159519      0.000000  ...  6.634241e-05      0.000000\n",
              "50%        0.260213      0.000000  ...  1.553934e-04      0.000000\n",
              "75%        0.463364      0.000000  ...  3.987904e-04      0.000000\n",
              "max        0.997075      0.001033  ...  9.570280e-02     28.000000\n",
              "\n",
              "[8 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6BF8hmXGEnF"
      },
      "source": [
        "#Divide our variables between the independent variables (features) and dependent variables (policy passage)\n",
        "\n",
        "labels = UN_Data1 ['Class I']\n",
        "features = UN_Data1.drop(columns= ['Policy Passed', 'date', 'Class M', 'Class S', 'Class I', 'Class P', 'Class B', 'Conflict Indicator'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWD_YiigAZ8a"
      },
      "source": [
        "#Drop Null Values\n",
        "\n",
        "features = features.fillna(0)\n",
        "labels = labels.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FJRWpUNH8Dk",
        "outputId": "d8719d40-28d8-4655-a7ba-8c8501773d62"
      },
      "source": [
        "#Inspect shape of features\n",
        "\n",
        "features = pd.get_dummies(features)\n",
        "features.shape[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10212, 93)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oGgPi4CG0xx"
      },
      "source": [
        "#Define type of feature and label values\n",
        "\n",
        "features = features.values.astype('float32')\n",
        "labels = labels.values.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOea_aP_HPSE"
      },
      "source": [
        "#Data Sets for Training\n",
        "\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)\n",
        "features_train, features_validation, labels_train, labels_validation = train_test_split(features_train, labels_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSYOIsh5vq4"
      },
      "source": [
        "#Define Precision, Recall, and F1 score metrics\n",
        "import keras.backend as K\n",
        "\n",
        "def f1_metric(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall_keras\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision_keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VQ_OqkCHkLM"
      },
      "source": [
        "#Create your model\n",
        "\n",
        "model1 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)), keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'), keras.layers.Dense(2, activation='softmax')])\n",
        "model2 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model3 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model4 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model5 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "\n",
        "model6 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model7 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model8 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model9 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model10 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "\n",
        "model11 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model12 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model13 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model14 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model15 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "\n",
        "model16 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model17 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model18 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model19 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model20 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "\n",
        "model21 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model22 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model23 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model24 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model25 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "\n",
        "model26 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model27 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model28 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model29 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])\n",
        "model30 = tf.keras.Sequential([keras.layers.Dense(132, input_shape=(93,)),keras.layers.Dense(64, activation= 'relu'),keras.layers.Dropout(.10),keras.layers.Dense(32, activation= 'relu'),keras.layers.Dense(16, activation= 'relu'),keras.layers.Dense(2, activation='softmax')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "7JTlq8aH8mzi",
        "outputId": "4b95d6ca-43d6-4520-a140-1bb14ed8de96"
      },
      "source": [
        "### Inspect form of model\n",
        "\n",
        "tf.keras.utils.plot_model(model1, to_file='model.png', show_shapes = True, show_dtype=True, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAALhCAIAAAD9/GtKAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdeVwTZ/448GdC7pBwySWHAhEpigerVlCXuraslRUFRFO1lvrSxaNFvKqIUAREEUUWhLJalnWrlUMpWhUParH6q3bdFSoL1SIKCB4cckmCBJjfH7Od72yAEEJO/Lz/cuYZZj7zSD5MnnnmMxiO4wgAAIDeomk7AAAAAMMCeRwAAPQb5HEAANBvkMcBAEC/0akLt27dSkxM1FYoAAAAFOHh4bFlyxZy8X+ux588eXL69GmNhwTedLdv3759+7a2o1CvN+EcEUK1tbWQQ9Tt9u3bt27doq6h990oNzdXU/EAgBBCgYGBaKT/4r0J54gQysnJWbZs2Yg/Te0ifpeoYHwcAAD0G+RxAADQb5DHAQBAv0EeBwAA/QZ5HAAA9BvkcQB018WLF42MjL799lttB6Ji69atw36zcuVKalNhYWFYWNiZM2ccHR2JDT788EPqBt7e3nw+38DAYMKECXfv3tVs4P8llUojIyMdHR2ZTKaNjc22bdskEgnZGh8f7+LiwuFweDyei4tLREREW1sb0XTu3Ln4+Pienh5y4/z8fLIrRo0apVw8kMcB0F0juBypqalpQUHBgwcPMjIyyJWff/55cnLyrl27AgICHj165OTkZGZmduLEiQsXLpDbXLlyJTc3d+HChWVlZe7u7tqIHYWGhiYkJMTFxTU1NZ08efLYsWNr1qwhW2/cuLF27dqampoXL17ExMTEx8cvWbKEaPL19WWz2fPmzWtpaSHWLFq0qLa29ocffliwYIHS8UAeB0B3+fj4tLa2Lly4UN0Hkkgknp6e6j4KFYfDmT9/vrOzM4vFItbs378/KysrJyeHz+eTmyUnJ9NotODg4NbWVk2GJ8ejR4/S09NXrVolEon4fP4777wTEhLy9ddf//LLL8QGTCZz48aN5ubmhoaGgYGBixcvvnr16rNnz4jWTZs2TZ48ecGCBd3d3QghDMNsbGzmzJkzbtw4pUOCPA4AQBkZGfX19VoM4OHDhxEREXv27GGz2dT1np6eoaGhdXV127Zt01ZsMu7cudPb2/v222+Ta+bPn48Qunz5MrGYl5dHPQsbGxuE0KtXr8g1UVFRJSUlSUlJqgoJ8jgAOurmzZv29vYYhh05cgQhlJaWxuPxuFzu2bNn33//fYFAYGtre+rUKWLj5ORkNpttYWGxbt06a2trNpvt6en5008/Ea0hISFMJtPKyopY3LhxI4/HwzCssbERIRQaGrp169bKykoMw4RCIULo0qVLAoFg7969GjvZ5ORkHMd9fX37NsXGxjo7O3/55ZeFhYX9/iyO44mJiW+99RaLxTIxMVm8ePH9+/eJJvmdhhDq6emJjIy0t7fncDiTJk3Kzs4eNFQajYYQ4nA45BriUpq8HpdRUVFhbGw8ZswYco2JiYmXl1dSUpKqxs0gjwOgo2bPnv3jjz+Sixs2bNi8ebNEIuHz+dnZ2ZWVlY6OjmvXrpVKpQihkJCQoKAgsVi8adOmqqqqu3fvdnd3v/fee0+ePEEIJScnL126lNxVamrqnj17yMWkpKSFCxc6OTnhOP7w4UOEEHEjrre3V2Mne+HChfHjx3O53L5NHA7n73//O41GW7t2bUdHR98NoqKiwsLCwsPD6+vrf/jhhydPnsyZM+fFixdosE5DCO3cufPAgQOHDx9+9uzZwoULly9f/q9//Ut+qC4uLuh/s7aZmRlCqKGhgbqZVCqtq6s7cuRIYWFhSkoKk8mktk6dOrWuru7nn39WpHMGBXkcAD3j6ekpEAjMzc1FIlFHR0dNTQ3ZRKfTictSV1fXtLS09vb2zMxMJQ7h4+PT1tYWERGhuqjl6ejoePz4sZOT00AbeHh4bN68uaqqaufOnTJNEokkMTHR399/5cqVRkZGbm5u6enpjY2NR48epW7Wb6d1dnampaX5+fkFBAQYGxvv3r2bwWAM2mNubm7z589PTU29du1aZ2fn8+fP8/LyMAwj/zYQ7OzsbG1to6KiDhw4sGzZMpmdEJfwpaWlg/WNQiCPA6CviEs8mfRBmjZtGpfLJUcYdFl9fT2O4/1ejJNiY2PHjx+fmpp68+ZN6vqysrJXr15NmzaNXDN9+nQmk0mOKcmgdtqDBw/EYvHEiROJJg6HY2VlpUiPZWVlBQYGrlq1ytTUdNasWd988w2O48RVOenJkyf19fVff/318ePHp06dKnP7gThZ4kvD8EEeB2DEYrFYMl/2dVNnZydCiJy40i82m52ZmYlh2OrVq6mTtYkJfIaGhtSNjY2N29vbBz0uMUqze/ducgZ3dXW1WCwe9AeNjIzS09Nra2vFYnFlZeWhQ4cQQqNHj6Zuw2AwzM3Nvb29s7KyysrK4uLiqK3E8Dpx4sMHeRyAkUkqlba0tNja2mo7kMERSY36dEy/iJcnVFRUxMTEkCuNjY0RQjJZW8ETNzc3RwgdPnwYp5Ap7a2IO3fuIITmzp3bb6tQKDQwMCgrK6Ou7OrqQv97s3Q4II8DMDIVFRXhOD5z5kxikU6nDzQCo3UWFhYYhikyQzwmJsbFxaW4uJhcM3HiRENDQ+rNyZ9++qmrq+t3v/vdoHuzs7Njs9klJSXKhU06duyYg4ODl5cXQqipqWn58uXU1oqKip6eHjs7O+pK4mQtLS2HeWgC5HEARo7e3t7m5ubu7u579+6Fhoba29sHBQURTUKh8OXLl/n5+VKptKGhobq6mvqDpqamT58+raqqam9vl0qlBQUFmpx3yOVyHR0da2trB92SGF0xMDCgrtm6dWteXt6JEyfa2tpKS0vXr19vbW0dHBysyN4+/vjjU6dOpaWltbW19fT01NbWEg/siEQiS0vLgZ77nzFjRnV1dXd3d1VV1bZt2woLCzMyMoiRdx6Pd+XKlWvXrrW1tUml0uLi4o8++ojH41Ffw4YQIk7Wzc1t0CAVAXkcAB115MiR6dOnI4R27NixaNGitLS0w4cPI4QmTZr06NGjY8eObd26FSE0f/78iooK4kc6Ozvd3Nw4HM6cOXOcnZ2///57ctB5w4YNc+fO/eCDD8aPHx8TE0N8o/fw8CAmJq5fv97CwsLV1XXBggUvX77U/Mn6+PiUlZWRA9/ffPONUCisrKycPn36p59+St1y5syZMjnx888/j4uLi46OHjVqlJeX19ixY4uKing8HkJo0E5LSkravHlzfHy8mZmZtbV1aGhoc3MzQqirq6u+vv7s2bP9RmtsbDxlyhQOh+Pu7n7//v0bN26QgypsNnvWrFlr1qyxsbHh8/mBgYFjx469ffs2eTeVcOfOHRsbm0mTJqmi8xCiDgwRc+BxADRryZIlS5Ys0XYU6qWBcwwODjY1NVXrIQalYA4JDg62sbGhrqmoqKDT6V999ZXaQhuanp6eOXPmZGRkqGPnjY2NbDb74MGD1JWbNm0yMzNT5Mf7/i7B9TgAI8egtwp1h0QiuXz5ckVFBXHHTygURkdHR0dHU59f15aenp78/Pz29naRSKSO/UdFRU2ZMiUkJAQhhOP406dPb968STyBpRzI4wAALXj58iVRJ2v16tXEmrCwsMDAQJFIpPWSWEVFRWfOnCkoKJA/pV05iYmJJSUlFy9eZDAYCKGzZ88SdbKoNR2Harh5fM2aNXw+H8Ow4d/zVRUdLNl8+/btt956i0ajYRhmaWkZGxursUNT6zhbWVnJ1HrWO69fv960aZOVlRWXy3333XeJeQ7p6ekqPISc4tEIoejoaFdXV4FAwGKxhELhZ599pgvXjwihXbt2ZWZmtra2Ojg4nD59WtvhDCI9PZ0cEzhx4gS5fu/evSEhIfv27dNibAihefPmnTx5kixHo0Jnz559/fp1UVGRiYkJsWbx4sXU8RYl90sdZFFufJwoOlNcXDzUH1ST8+fPCwSCc+fOaTsQWX/84x8RQs3NzZo/tJOTk5GRkeaPqyDFx4737t3r7Ozc3Nz817/+NTc3l7hV9cUXX6gwGB8fn4MHD9bX17e3t+fk5DAYjPfee49s9fLySk1NbWpqamtry87OZjAY8+fPV2S3b8I9ABzusWnEGzE+PoJLNitIZwMbvvz8/GnTphkbG//5z38ma/MPk0x3yS8ebWhoSNxO5PP5S5cu9fPzu3TpEjHlAwBtUUEexzBs+DvRR1ov2TwQnQ1s+Gpra4lRRRWS6S75xaPPnz9PnbxMvIhLkSe5AVAfZfI4juMJCQnjx49nsVhGRkbbt2+ntvZbz3fQKsDXr1+fMWMGl8sVCARubm7EiKQSpYH1pWSzJgNTxI0bN1xdXY2MjNhstpubG1ERf82aNcTAupOTE/EE3ccff8zlco2MjM6dO4cG+A86cOAAl8vl8/n19fVbt261sbF58OCBgmHIcfXqVaFQ+OzZs+PHj2MYJlNPg4APXId6oHMctLv6Fo+mqqur43A4Dg4Owz9BAJRHHWRRcGwrPDwcw7BDhw41NzeLxeLU1FREGR/ftm0bi8U6ffp0c3Pzrl27aDTanTt3iJ9CCH333Xetra319fVz5szh8XhdXV04jr969UogEMTHx0skkufPn/v7+zc0NMjZlXzEl9yUlBQy2oGOi+N4cHAwj8crLy/v7OwsKyubPn06n8+vqakhWlesWGFpaUnuOSEhASFExIbjeEBAAFGymXD+/Hk+nx8dHT1QYDLj4xoLDFdgfDw3NzcqKurly5dNTU0zZ84k57EGBAQYGBjU1dWRWy5fvpy89yD//3rTpk0pKSn+/v6//PKLnEPjQxk7trS0/Oijj8hFmfHxyMhIJpP51VdftbS03Lt3z93dfdSoUc+fPx/0HGW6C8fxrq6u2tralJQUFos10KTmjo4OPp8fEhKiSOQwPg5Upe/v0pDzuFgs5nK51Ds/1PucEomEy+WKRCJyYxaLtWHDBvy3z7ZEIiGaiOz/8OFDHMf/85//IITOnz9PPZCcXcnXbx7v97g4jgcHB1MTHFHvZs+ePcTiUNOlfP3mcc0ENqT7nERhNqKUKPEGltjYWKKptbV13Lhx3d3d+FD+rwelkjwuFosNDQ3JeHAc/+c//4kQ6vcvK/Uc+/1/JApfmJmZ/eUvfyH/uMoIDw93dnZua2tTJHLI40BVVHCf8+HDh2KxeN68ef22Kl7Pl1oF2NHR0cLCYuXKlVFRUVVVVUPd1ZDobMlm3QmMGIAmnij5wx/+4Ozs/Le//Q3HcYRQVlaWSCQiBojV9B+ktCHVoaaeY7/kF49GCOXl5eXk5Fy+fJn6UmD5Tp8+jY10xAsTtB3FCNd3XildwV9BElHehaj32BdZz3f37t3kSmtra/n75HA4165d27lz5969e6Ojo5cuXZqZmancroZPZ0s2qzWwCxcuJCQklJWVEcV9yPUYhq1bt27Lli3ffffdu++++49//OPkyZNEk7b+gwYyaB3qgc6xX2TxaAcHB2dn57i4OOpbcbOyshITE4uKimRKTss3c+bMzZs3K769Prp161ZSUpIit7KA0oiKMVRDzuPErfzXr1/320rW8w0NDR3SbidMmPDtt982NDQkJibu379/woQJxBOxSuxqOHS2ZLM6Avvhhx/+/e9/b968uaamxs/Pz9/f/29/+9vo0aNTUlI+++wzcrOgoKBdu3Z9+eWXdnZ2AoGAvOOn9P+1msivQy3/HOXoWzw6JSXl8uXL165d6/deqxy2trbUl2SOVElJSW/CaWpRbm6uzJohj6tMnDiRRqNdv36931bl6vk+ffq0vLwcIWRubr5v3z53d/fy8nJVlQYeEp0t2ayOwP79738TNeFKS0ulUumGDRscHR3ZbDb2vxNJTUxMli1blp+ff/DgwbVr15LrtfIfJIf8OtTyz5Ekv3g0juM7duwoLS3Nz88fahIHQH2GnMfNzc0DAgJOnz6dkZHR1tZ279496vtM5dTzlePp06fr1q27f/9+V1dXcXFxdXX1zJkzlduVEnS2ZLOqAuu7Z6lU+uLFC7K2p729PUKosLCws7OzoqKi74Dy+vXrX79+ff78eerTVRr7D1KQ/DrUcs6R2l1MJlNO8ejy8vIDBw4cO3aMwWBQxysPHjyorbMGACGl5h22t7evWbPGzMzM0NBw9uzZkZGRCCFbW9uff/4Zx/HXr1/v2LHD3t6eTqcTSb+srCw1NZWoODNu3LjKysqjR48KBAKE0JgxY3799deqqipPT08TExMDA4PRo0eHh4cTMyL63ZX82FJSUoiJ1Vwu19fXV/5xcRwPDg5mMBg2NjZ0Ol0gECxevLiyspLcW1NT09y5c9lstoODw6effkrMlBcKhcT8v7t3744ZM4bD4cyePfv58+cXL17k8/nk1A6q27dvT5gwgUajIYSsrKz27t2rscC++OILOa8hz8vLI3a4Y8cOU1NTY2PjwMBAYuq9k5MTOc0Rx/GpU6eGhYXJnFe//0Hx8fFEbWs7OzsFy5AqMpejqqpq6tSpCCE6ne7u7n769OlDhw4Rs0p4PJ6/vz+O4729vQkJCePGjWMwGCYmJn5+fg8ePCD3MNA5yvw/+vr6Ojg4GBoaslgsJycnkUhUWlpK7GGgt5snJCSo5BxHAJivogF9f5cwHMfJX8ecnJxly5ZR14x469aty83NbWpq0nYgsnQtMB8fnyNHjqjpgZfAwEDU36jfSPImnCN6I3OI5vX9XRqB9VWGSmdLNms9MHJM5t69e8S1v3bjAQD0S8/y+P379+VMq1RT0fc31o4dOyoqKn799dePP/6Y+oZyAIZp3bp15MdWppZyYWFhWFgYtd7yhx9+SN3A29ubz+cbGBhMmDBhoPdnqptUKo2MjHR0dGQymTY2Ntu2bSPfSIfklj4+d+5cfHw89RItPz+f7AqiXI8yqIMsb9rYVlhYGPH0zdixY3Nzc7Udzv/RkcDCw8NpNJqdnZ26iwC/CWPHb8I54kN5r5upqWlBQcGDBw86OzvJ9ZGRkQsXLiQfkXVycjIzM0N9HvYuKChYtGiRaiMfkg0bNrDZ7FOnTrW1tX3//fcCgWD58uVkq/zSx0lJSV5eXuRz3b29vbW1tT/88MOCBQuUfq/bG53HgY54E3KcBs5RLBZ7eHhod1dKv58Tx/F9+/Y5OztTyzk4OTmdPHmSRqPZ2Ni0tLSQ67WbxysrK2k02p///GdyDfEoXHl5ObHo5+dHPQtiOPvp06fkmpCQEA8PD6lUSt0tvJ8TAKDKesWaL3388OHDiIiIPXv2UIsGI4Q8PT1DQ0Pr6uq2bdumyXjkuHPnTm9v79tvv02umT9/PkKIqKCJBit9jBCKiooqKSmhPiE8TJDHAdAh+MCld4dUr1iLNZmVk5ycjOO4r69v36bY2FhnZ+cvv/ySqNrWl5xOG7RithLFsYkJxMTkWsK4ceMQQr/88ku/2/ctfWxiYuLl5ZWUlISraGIP5HEAdEhUVFRYWFh4eHh9ff0PP/zw5MmTOXPmvHjxAiGUnJxMfd49NTV1z5495GJSUtLChQuJwo0PHz4MCQkJCgoSi8WbNm2qqqq6e/dud3f3e++9R1QDHdKu0G9Tp3p7e9V34hcuXBg/fny/7zXmcDh///vfaTTa2rVriao+MuR02oYNGzZv3iyRSPh8fnZ2dmVlpaOj49q1a8m5WDt37jxw4MDhw4efPXu2cOHC5cuXUx8J7peLiwv636xNDOLLlD+SSqV1dXVHjhwpLCxMSUkh7niRpk6dWldX9/PPPyvSOYOCPA6ArpBIJImJif7+/itXrjQyMnJzc0tPT29sbKQ+Mj0kdDqduEp1dXVNS0trb2/PzMxUYj8+Pj5tbW0RERHKhTGojo6Ox48fy3lgzcPDY/PmzVVVVTt37pRpUrDTPD09BQKBubm5SCTq6OioqalBCHV2dqalpfn5+QUEBBgbG+/evZvBYAzaRW5ubvPnz09NTb127VpnZ+fz58/z8vIwDJN5dtrOzs7W1jYqKurAgQNEGUgq4hJ+oCfLhgryOAC6Ykild4dKizWZB0UUgu/3YpwUGxs7fvz41NTUmzdvUtcPtdOoBaKVrr2clZUVGBi4atUqU1PTWbNmffPNNziOE1flJPmlj4mTJb40DB/kcQB0xaCld4dJZ2syd3Z2IoRYLJacbdhsdmZmJoZhq1evpk7WHk6nkbWXyRnc1dXVirxt1cjIKD09vba2ViwWV1ZWHjp0CCEkU8SYLH2clZVVVlZGvLqERAyvEyc+fJDHAdAV8kvvDpPO1mRGvyW1QR9g9vDw2LJlS0VFBfWptOF0Gll7mTqH79atW0ONn3hd19y5c/tt7Vv6GCHU1dWF/vdm6XBAHgdAV8gvvYuGV69YZ2syI4QsLCwwDGttbR10y5iYGBcXF+Kt34RBO00OVdVePnbsmIODg5eXFxqs9DGJOFmi0NvwQR4HQFfIL72Lhl6vWGdrMsvgcrmOjo7Eu8bkI0ZXiDcLkmvkd5r8vQ1Ue1kkEllaWg703P+MGTOqq6u7u7urqqq2bdtWWFiYkZFBjLzzeDw5pY9JxMm6ubkNGqQiII8DoEM+//zzuLi46OjoUaNGeXl5jR07liwTjxDasGHD3LlzP/jgg/Hjx8fExBDfyj08PIjZhOvXr7ewsHB1dV2wYMHLly8RQp2dnW5ubhwOZ86cOc7Ozt9//z05Bj3UXambj49PWVkZOfD9zTffCIXCysrK6dOnf/rpp9QtZ86cKZMT5XRaWloa8Ra0SZMmPXr06NixY1u3bkUIzZ8/n3hJd1JS0ubNm+Pj483MzKytrUNDQ5ubmxFCXV1d9fX1Z8+e7TdaY2PjKVOmcDgcd3f3+/fv37hxgxxUYbPZs2bNWrNmjY2NDZ/PDwwMHDt27O3bt8m7qYQ7d+7Y2NhMmjRJFZ33ZtdXAToCnstXB6KGiSaPiA/jufyKigo6na5gwXoN6OnpmTNnTkZGhjp23tjYyGazDx48SF0Jz+UDAPqh9dLHckgkksuXL1dUVBB3/IRCYXR0dHR0NPX5dW3p6enJz89vb29XUwnVqKioKVOmhISEIIRwHH/69OnNmzeJR66UA3kcAKAFL1++nD9/vrOz8+rVq4k1YWFhgYGBIpFIkRuealVUVHTmzJmCggL5U9qVk5iYWFJScvHiRQaDgRA6e/asjY3NnDlzLly4oPQ+IY8DMALt2rUrMzOztbXVwcHh9OnT2g5HVnp6OjkmcOLECXL93r17Q0JC9u3bp8XYEELz5s07efIkWX9Ghc6ePfv69euioiITExNizeLFi6njLcrtlq66CAEAuiIuLk7mwRN94e3t7e3tre0o1GXRokWLFi1S+W7hehwAAPQb5HEAANBvkMcBAEC/QR4HAAD91s99zpycHM3HAd5kxDPKI/sX7004R4QQUWRqxJ+mdtXW1spWAaM+FKTIO40AAABol8zznBiuohfEAaCbMAzLzs6mvsYMgBEGxscBAEC/QR4HAAD9BnkcAAD0G+RxAADQb5DHAQBAv0EeBwAA/QZ5HAAA9BvkcQAA0G+QxwEAQL9BHgcAAP0GeRwAAPQb5HEAANBvkMcBAEC/QR4HAAD9BnkcAAD0G+RxAADQb5DHAQBAv0EeBwAA/QZ5HAAA9BvkcQAA0G+QxwEAQL9BHgcAAP0GeRwAAPQb5HEAANBvkMcBAEC/QR4HAAD9BnkcAAD0G+RxAADQb5DHAQBAv0EeBwAA/QZ5HAAA9BvkcQAA0G+QxwEAQL9hOI5rOwYAVCk4OPjBgwfk4t27dx0cHExMTIhFAwOD48eP29raaik6AFSPru0AAFAxS0vLo0ePUtfcu3eP/LejoyMkcTDCwLgKGGmWL18+UBOTyQwKCtJgLABoAoyrgBFo4sSJ5eXl/f5uP3jwwNnZWfMhAaA+cD0ORqBVq1YZGBjIrMQwbPLkyZDEwcgDeRyMQB988EFPT4/MSgMDg48++kgr8QCgVjCuAkYmT0/Pn376qbe3l1yDYdiTJ09sbGy0GBUA6gDX42Bk+vDDDzEMIxdpNNrs2bMhiYMRCfI4GJkCAwOpixiGrVq1SlvBAKBWkMfByDRq1Kh58+aRdzsxDPPz89NuSACoCeRxMGKtXLmSuP1jYGDwxz/+0czMTNsRAaAWkMfBiOXv789kMhFCOI6vXLlS2+EAoC6Qx8GIxePx/vSnPyGEmEzmwoULtR0OAOoCeRyMZCtWrEAI+fn58Xg8bccCgLro6Pxx6owxAADQEdnZ2UuXLtV2FLJ0t95haGioh4eHtqN4I9y6dSspKSk7O1vbgajFiRMnRCLRnTt3RvA5Ui1btgw+O2qybNkybYfQP929HtfNv3sjUk5OzrJly3TzN2H4Ojs72Wz2yD5HKvjsqI/O9i2Mj4MRjs1mazsEANQL8jgAAOg3yOMAAKDfII8DAIB+gzwOAAD6DfI4AAO6ePGikZHRt99+q+1ANKSwsDAsLOzMmTOOjo4YhmEY9uGHH1I38Pb25vP5BgYGEyZMuHv3rlaClEqlkZGRjo6OTCbTxsZm27ZtEomEbI2Pj3dxceFwODwez8XFJSIioq2tjWg6d+5cfHx83xeMjACQxwEY0JswT5H0+eefJycn79q1KyAg4NGjR05OTmZmZidOnLhw4QK5zZUrV3JzcxcuXFhWVubu7q6VOENDQxMSEuLi4pqamk6ePHns2LE1a9aQrTdu3Fi7dm1NTc2LFy9iYmLi4+OXLFlCNPn6+rLZ7Hnz5rW0tGglcvWBPA7AgHx8fFpbWzVQm0UikXh6eqr7KHLs378/KysrJyeHz+eTK5OTk2k0WnBwcGtrqxZjo3r06FF6evqqVatEIhGfz3/nnXdCQkK+/vrrX375hdiAyWRu3LjR3Nzc0NAwMDBw8eLFV69effbsGdG6adOmyZMnL1iwoLu7W3snoXqQxwHQvoyMjPr6em0d/eHDhxEREXv27JGZa+/p6RkaGlpXV7dt2zZtxSbjzp07vb29b7/9Nrlm/vz5CKHLly8Ti3l5edSzIP1nzMoAACAASURBVN4A9erVK3JNVFRUSUlJUlKShiLWCMjjAPTv5s2b9vb2GIYdOXIEIZSWlsbj8bhc7tmzZ99//32BQGBra3vq1Cli4+TkZDabbWFhsW7dOmtrazabTbwglGgNCQlhMplWVlbE4saNG3k8HoZhjY2NCKHQ0NCtW7dWVlZiGCYUChFCly5dEggEe/fu1cyZJicn4zju6+vbtyk2NtbZ2fnLL78sLCzs92dxHE9MTHzrrbdYLJaJicnixYvv379PNMnvMYRQT09PZGSkvb09h8OZNGmSIlUTaDQaQojD4ZBrxo0bhxAir8dlVFRUGBsbjxkzhlxjYmLi5eWVlJQ0ogbNcJ2EEMrOztZ2FG8K4vOj7SjUS7lzfPLkCUIoJSWFWAwPD0cIfffdd62trfX19XPmzOHxeF1dXURrcHAwj8crLy/v7OwsKyubPn06n8+vqakhWlesWGFpaUnuOSEhASHU0NBALAYEBDg5OZGt58+f5/P50dHRSpypEp8dR0dHV1dXmZVOTk6PHz/GcfzHH3+k0Whjx4599eoVjuMFBQWLFi0iN4uMjGQymV999VVLS8u9e/fc3d1HjRr1/PlzolV+j23bto3FYp0+fbq5uXnXrl00Gu3OnTvyQ7137x5CKCIiglxDjJD4+flRN+vq6qqtrU1JSWGxWF999ZXMTsLCwhBCxcXFCvfQf+lsXoLrcQCGxtPTUyAQmJubi0Sijo6OmpoasolOpxNXpq6urmlpae3t7ZmZmUocwsfHp62tLSIiQnVRD6ijo+Px48dOTk4DbeDh4bF58+aqqqqdO3fKNEkkksTERH9//5UrVxoZGbm5uaWnpzc2Nh49epS6Wb891tnZmZaW5ufnFxAQYGxsvHv3bgaDMWh3ubm5zZ8/PzU19dq1a52dnc+fP8/Ly8MwTCqVUjezs7OztbWNioo6cOBA3+JWxCV8aWnpYH2jNyCPA6Ak4mVDMhmENG3aNC6XSw4y6Kz6+nocx7lcrpxtYmNjx48fn5qaevPmTer6srKyV69eTZs2jVwzffp0JpNJDijJoPbYgwcPxGLxxIkTiSYOh2NlZaVId2VlZQUGBq5atcrU1HTWrFnffPMNjuMyL+178uRJfX39119/ffz48alTp8rceyBO9sWLF4MeS19AHgdAXVgsVkNDg7ajGERnZydCiMViydmGzWZnZmZiGLZ69WrqZG1iAp+hoSF1Y2Nj4/b29kGP29HRgRDavXs39pvq6mqxWDzoDxoZGaWnp9fW1orF4srKykOHDiGERo8eTd2GwWCYm5t7e3tnZWWVlZXFxcVRW4nhdeLERwbI4wCohVQqbWlpsbW11XYggyCS2qBPx3h4eGzZsqWioiImJoZcaWxsjBCSydoKnrW5uTlC6PDhw9Rx3lu3bg01/jt37iCE5s6d22+rUCg0MDAoKyujruzq6kL/e7NU30EeB0AtioqKcByfOXMmsUin0wcagdEuCwsLDMMUmSEeExPj4uJSXFxMrpk4caKhoeG//vUvcs1PP/3U1dX1u9/9btC92dnZsdnskpIS5cImHTt2zMHBwcvLCyHU1NS0fPlyamtFRUVPT4+dnR11JXGylpaWwzy07oA8DoDK9Pb2Njc3d3d337t3LzQ01N7ePigoiGgSCoUvX77Mz8+XSqUNDQ3V1dXUHzQ1NX369GlVVVV7e7tUKi0oKNDYvEMul+vo6FhbWzvolsToioGBAXXN1q1b8/LyTpw40dbWVlpaun79emtr6+DgYEX29vHHH586dSotLa2tra2np6e2tpZ4YEckEllaWg703P+MGTOqq6u7u7urqqq2bdtWWFiYkZFBjLzzeLwrV65cu3atra1NKpUWFxd/9NFHPB5vy5Yt1D0QJ+vm5jZokHpDK7NkBoV0dX7PiATzDvuVkpJCzPjmcrm+vr6pqanE/bFx48ZVVlYePXpUIBAghMaMGfPrr7/iOB4cHMxgMGxsbOh0ukAgWLx4cWVlJbm3pqamuXPnstlsBweHTz/9dPv27QghoVBITEy8e/fumDFjOBzO7Nmznz9/fvHiRT6fHxsbq8SZKvHZCQkJYTAYYrGYWMzLyyOmr4waNeqTTz6R2Xj79u3UeYe9vb0JCQnjxo1jMBgmJiZ+fn4PHjwgmgbtsdevX+/YscPe3p5Op5ubmwcEBJSVleE47ufnhxCKjIzsN9r33nvP2NiYTqebmJj4+PjITFX09fV1cHAwNDRksVhOTk4ikai0tFRmDz4+PjY2Nr29vUPqJVyH85KOfnp1tr9GJMjjKhEcHGxqaqrWQyhCic9ORUUFnU7vO89aW3p6eubMmZORkaGOnTc2NrLZ7IMHDyrxszqbl2BcBQCV0dNaekKhMDo6Ojo6mvr8urb09PTk5+e3t7eLRCJ17D8qKmrKlCkhISHq2Lm2jJA8vmbNGj6fj2HY8G+bqAS18ieByWRaWFi88847CQkJzc3N2g5QSa9fv960aZOVlRWXy3333XeJW2Tp6ekqPIScuqMIoejoaFdXV4FAwGKxhELhZ599pgupZwQICwsLDAwUiURaL4lVVFR05syZgoIC+VPalZOYmFhSUnLx4kUGg6HynWuTtr8Q9A8N/fsLUbdBiWdt1cfJycnIyAjHceL21/fffx8UFIRhmLW19aDPH2uS4mMOe/fudXZ2bm5u/utf/5qbm1tRUYEQ+uKLL1QYjI+Pz8GDB+vr69vb23NychgMxnvvvUe2enl5paamNjU1tbW1ZWdnMxiM+fPnK7JbdY+rhIWFEbfaxo4dm5ubq74DDUqJzw7p8uXLO3bsUG08uiM/Pz8uLq67u1vpPQynb9UK8rgakXmcKjc3l0ajWVhYtLS0aCWqvhTPcdOnT1++fDm5qJI8LhaLPTw8yEU/Pz+JREIuBgYGIoSePn1KLPr4+FA/h0uXLkUIkTVM5HgT7gEQdDbXjAA627cjZFwFIYRhmLZDUMiSJUuCgoLq6+tVOxyhGbW1tSr/QipTslV+3dHz589T572NGjUKIaTIQ4AAjGB6nMdxHE9ISBg/fjyLxTIyMiImcpH6LYk5aCHN69evz5gxg8vlCgQCNzc3YmR2oOqaShcXJeYUFxQUaCzU4bt69apQKHz27Nnx48cxDJN5FJuAD1zCFCF048YNV1dXIyMjNpvt5uZGFIzuW7JVRt+6o1R1dXUcDsfBwUFFZwmAftL2F4L+IQW+v4SHh2MYdujQoebmZrFYnJqaiijjKgOVxJRTSPPVq1cCgSA+Pl4ikTx//tzf358oKzrQrgYtLtrvuAqO40TOtbOz01io8ik+5mBpafnRRx+RizLjKvJLmObm5kZFRb18+bKpqWnmzJlmZmbEepmSrQT5dUcJHR0dfD4/JCREkchhXAUMn872rY7+Zg/aX2KxmMvlUu+AUcfHJRIJl8sViUTkxiwWa8OGDfhvyZEcgSWy/8OHD3Ec/89//oMQOn/+PPVAcnY1qIHyOI7jGIYZGxvrSKgqyeNisdjQ0JA8Oo7j//znPxFC/f6dI+oWEZX2+s3jxDPTZmZmf/nLX8hy1TLCw8OdnZ3b2toUiRzyOBg+ne1buqau+1Xs4cOHYrF43rx5/bYqXhKTWkjT0dHRwsJi5cqVmzZtCgoKGjt27JB2pbiOjg4cx4ln23Q8VMUNqYQpMcguZ7b1kydPWlpaiouLw8LCjh49eu3aNQsLC+oGeXl5OTk5V65cob5PclA5OTmKb6y/lKg2BfSbtv+Q9A8N9nfv4sWLCCHqE1/U6/H/9//+X98znTlzJt7nIvfYsWMIoV9++YVY/M9//vOnP/2JTqdjGLZs2TKxWCxnV4Ma6HqcKBzh7e2tI6Gq5Hr86tWrCKH09HTq9hYWFr///e+Jf58/f97Ly2vUqFFMJpO4Kf3s2TN8gOtx0q+//ooQ2rRpE3XlqVOnpk+fXldXp0jMBBXeKgBvMt28HtfX+5zElIbXr1/326p0ScwJEyZ8++23T58+3bFjR3Z29sGDB1VVXZPq0qVLCKH3339f90NVnPwSpjU1NX5+flZWVj/99FNra2t8fLyCu+1bdzQlJeXEiRPXrl2TKTmtCPV9kHQH0tVcMwIM9fdNY/Q1j0+cOJFGo12/fr3fVuVKYj59+rS8vBwhZG5uvm/fPnd39/LyclVV1yQ9f/788OHDtra2q1ev1vFQh0R+CdPS0lKpVLphwwZHR0c2mz3QJFH5dUdxHN+xY0dpaWl+fn6/E2YAeDPpax4nqqOdPn06IyOjra3t3r171FcCyimJKcfTp0/XrVt3//79rq6u4uLi6urqmTNnytmVIsVFcRx/9eoVUVmtoaEhOzt71qxZBgYG+fn5xPi4ZkLVAPklTO3t7RFChYWFnZ2dFRUV1EFzaslWJpMpp+5oeXn5gQMHjh07xmAwqDUPDh48qJlzBEBHafmLygCQAt8N29vb16xZY2ZmZmhoOHv27MjISISQra3tzz//jA9QElN+Ic2qqipPT08TExMDA4PRo0eHh4cTjw4OVF1TTnHRc+fOTZo0icvlMplMGo2GECImqMyYMSM6OrqpqYm6sQZClU+R8fGqqqqpU6cihOh0uru7++nTpw8dOkTMKuHxeP7+/rjcEqY4ju/YscPU1NTY2DgwMPDIkSMIIScnp5qaGpmSrXLqjg70YtyEhASVnOPIoMhnByhHZ/sWw3Vy0AfDsOzsbOKpa6BuOTk5y5Yt083fBFV5E86RAJ8d9dHZvtXXcRUAAAAEyOMAAKDfII8D8OYqLCwMCwujlsv/8MMPqRt4e3vz+XwDA4MJEyYM9MJMzejt7T18+LCnp6fMeqUL1p87dy4+Pl5PX/0hS8vj8wNAuno/YUR6E+4BvgnnSFD8sxMZGblw4UKysIGTk5OZmRnqU++hoKCA+kJOrfj1119nzZqFEJo8ebJM03AK1iclJXl5eTU3NysYhs7mJbgeB0A1JBJJ36tFre9qIPv378/KysrJyaEWNkhOTqbRaMHBwVp/KxDVzz//vHPnzvXr10+ZMqVvK5PJ3Lhxo7m5uaGhYWBg4OLFi69evUpOtzU0NCTem8rn85cuXern53fp0qUnT54QrZs2bZo8efKCBQu6u7s1dz5qAHkcANWQKaSuI7vq18OHDyMiIvbs2UMt9Y4Q8vT0DA0Nraur27Ztm/qOPlSTJ08+c+bMihUrWCxW39ZhFqyPiooqKSlJSkpSU/CaAXkcgP+DD1xCPSQkhMlkWllZEYsbN27k8XgYhjU2NqI+hdSTk5PZbLaFhcW6deusra3ZbLanpyf59NOQdoWGUel+IMnJyTiO+/r69m2KjY11dnb+8ssvCwsLh9pFgxbNV199fNJQC9abmJh4eXklJSXhej0nVbvDOgNBujoONSK9CWPHCp6j/BLqK1assLS0JDdOSEhACBGV3/E+Bb+Cg4N5PF55eXlnZ2dZWdn06dP5fD75Croh7WrQSvdUinx2HB0dXV1dZVY6OTk9fvwYx/Eff/yRRqONHTv21atXeJ/xcfldJKdoPq5sfXzS22+/3Xd8nDCcgvVhYWFIsVdC6mxegutxAP5LIpEkJib6+/uvXLnSyMjIzc0tPT29sbGRWvJhSOh0OnHd6urqmpaW1t7enpmZqcR+fHx82traIiIilAtDRkdHx+PHj52cnAbawMPDY/PmzVVVVTt37pRpUrCLPD09BQKBubm5SCTq6OioqalBCHV2dqalpfn5+QUEBBgbG+/evZvBYCjXIX3Z2dnZ2tpGRUUdOHBg2bJl/W4TFxdnbW0dGxsrs37cuHEIoYGeFtYLkMcB+K8hlVAfqmnTpnG5XI2Vg5eDeH0HUfVhILGxsePHj09NTb158yZ1/VC7iFo0X6318Z88eVJfX//1118fP3586tSpfe8uEAXrL1++3LdgPdEVL168UEkkWgF5HID/amlpQQjJVFI0NjaWKcarNBaL1dDQoJJdDUdnZycRjJxt2Gx2ZmYmhmGrV6+WSCTk+uF0UUdHB0Jo9+7dZIGz6upqVb0jm8FgmJube3t7Z2VllZWVES+cImVlZe3fv7+oqIh444oMDoeDfusWPQV5HID/kl9CfZikUqmqdjVMRNoa9PkXDw+PLVu2VFRUxMTEkCuH00WaqY+vRMH6rq4u9Fu36CnI4wD8l/wS6gghOp1ODBEooaioCMfxmTNnDn9Xw2RhYYFhmCIzxGNiYlxcXIqLi8k1g3aRHOqoj6+SgvVEVxDFO/UU5HEA/kt+CXWEkFAofPnyZX5+vlQqbWhoqK6upv44tZA6kaN7e3ubm5u7u7vv3bsXGhpqb28fFBSkxK4UqXSvOC6X6+joWFtbq0iHZGZmUudfD9pF8vc2UH18kUhkaWmpxHP/PB5v+AXria5wc3Mb6tF1iFZmyQwK6er8nhEJ5h2S5JdQb2pqmjt3LpvNdnBw+PTTT7dv344QEgqFxGxCmULqwcHBDAbDxsaGTqcLBILFixdXVlYqtys5le77UuSzExISwmAwxGIxsZiXl0dMXxk1atQnn3wis/H27dup8w7ldJH8ovn4wPXx/fz8EEKRkZH9Rnvr1q1Zs2ZZW1sTKcvKysrT0/P69etE6/AL1vv4+NjY2BAvexl+32qFjn56dba/RiTI4+pAPA6uySMSFPnsVFRU0On0geZZa15PT8+cOXOor03XmMbGRjabffDgQUU21tm8BOMqAKiLztbSEwqF0dHR0dHR5PPrWtTT05Ofn9/e3i4SiTR/9KioqClTpoSEhGj+0CoEeRyAN1FYWFhgYKBIJNJ6SayioqIzZ84UFBTIn9KuDomJiSUlJRcvXmQwGBo+tGpBHgdA9Xbt2pWZmdna2urg4HD69Glth9O/vXv3hoSE7Nu3T7thzJs37+TJk2S1GY05e/bs69evi4qKTExMNHxolaNrOwAARqC4uDiZR1F0k7e3t7e3t7aj0I5FixYtWrRI21GoBlyPAwCAfoM8DgAA+g3yOAAA6DfI4wAAoN909z7n4cOHc3NztR3FG4F4LjkwMFDbgajRm3COJPjsvGkwXCffZvSGfN6ABhQUFEydOlXz09rAiLRlyxYPDw9tRyFLR/M4AKqCYVh2dvbSpUu1HQgA6gLj4wAAoN8gjwMAgH6DPA4AAPoN8jgAAOg3yOMAAKDfII8DAIB+gzwOAAD6DfI4AADoN8jjAACg3yCPAwCAfoM8DgAA+g3yOAAA6DfI4wAAoN8gjwMAgH6DPA4AAPoN8jgAAOg3yOMAAKDfII8DAIB+gzwOAAD6DfI4AADoN8jjAACg3yCPAwCAfoM8DgAA+g3yOAAA6DfI4wAAoN8gjwMAgH6DPA4AAPoN8jgAAOg3yOMAAKDfII8DAIB+gzwOAAD6DfI4AADoN7q2AwBAxVpaWnAcp67p6Ohobm4mFw0NDRkMhsbjAkBdMJnfeAD03R/+8Ifvv/9+oFYDA4O6ujpLS0tNhgSAWsG4ChhpPvjgAwzD+m2i0Wi///3vIYmDEQbyOBhplixZQqf3P2CIYdiqVas0HA8A6gZ5HIw0JiYm3t7eBgYGfZtoNJqfn5/mQwJArSCPgxFo5cqVvb29MivpdLqPj4+RkZFWQgJAfSCPgxHI19eXxWLJrOzp6Vm5cqVW4gFArSCPgxGIy+X6+fnJTC7kcDgLFizQVkgAqA/kcTAyLV++XCqVkosMBmPJkiUcDkeLIQGgJpDHwcj0xz/+kToULpVKly9frsV4AFAfyONgZGIwGCKRiMlkEovGxsbz5s3TbkgAqAnkcTBiffDBB11dXQghBoOxcuXKgSaVA6Dv4Ll8MGL19vaOHj36xYsXCKGbN2/OmjVL2xEBoBZwPQ5GLBqN9uGHHyKErK2tPT09tR0OAOqitW+aOTk52jo0eHOMGjUKIfT222/n5uZqOxYw8nl6etra2mrhwLiWaOFUAQBAnbKzs7WSTrU5rqKtcwZDkp2djbT39374cnNzB91G389RcfC5Ux8t5lIYHwcj3JIlS7QdAgDqBXkcAAD0G+RxAADQb5DHAQBAv0EeBwAA/QZ5HAAA9BvkcQCUdPHiRSMjo2+//VbbgahLYWFhWFjYmTNnHB0dMQzDMIx4Ppbk7e3N5/MNDAwmTJhw9+5dbcWJEOrt7T18+HDfp3bj4+NdXFw4HA6Px3NxcYmIiGhrayNbo6OjXV1dBQIBi8USCoWfffbZq1eviKZz587Fx8f39PRo7hyGAfI4AErS7pRhdfv888+Tk5N37doVEBDw6NEjJycnMzOzEydOXLhwgdzmypUrubm5CxcuLCsrc3d311aoFRUVv//977ds2SIWi2Wabty4sXbt2pqamhcvXsTExMTHx1PnoV67du2TTz6pqqpqbGyMi4tLSkoKDAwkmnx9fdls9rx581paWjR3JsqCPA6Aknx8fFpbWxcuXKjuA0kkEg3Xh9m/f39WVlZOTg6fzydXJicn02i04ODg1tZWTQYj388//7xz587169dPmTKlbyuTydy4caO5ubmhoWFgYODixYuvXr367NkzotXQ0DA4ONjU1JTP5y9dutTPz+/SpUtPnjwhWjdt2jR58uQFCxZ0d3dr7nyUAnkcAF2XkZFRX1+vscM9fPgwIiJiz549bDabut7T0zM0NLSurm7btm0aC2ZQkydPPnPmzIoVK/q+kRUhlJeXRz0LGxsbhBA5eHL+/HkDAwOylajGQ72oj4qKKikpSUpKUlPwqgJ5HABl3Lx5097eHsOwI0eOIITS0tJ4PB6Xyz179uz7778vEAhsbW1PnTpFbJycnMxmsy0sLNatW2dtbc1msz09PX/66SeiNSQkhMlkWllZEYsbN27k8XgYhjU2NiKEQkNDt27dWllZiWGYUChECF26dEkgEOzdu1dNp5acnIzjuK+vb9+m2NhYZ2fnL7/8srCwsN+fxXE8MTHxrbfeYrFYJiYmixcvvn//PtEkv4sQQj09PZGRkfb29hwOZ9KkSUSxBNWqqKgwNjYeM2ZMv611dXUcDsfBwYFcY2Ji4uXllZSUpOtjaFqsRQB1HvTCm1B7RLlzJL6Ap6SkEIvh4eEIoe+++661tbW+vn7OnDk8Hq+rq4toDQ4O5vF45eXlnZ2dZWVl06dP5/P5NTU1ROuKFSssLS3JPSckJCCEGhoaiMWAgAAnJyey9fz583w+Pzo6WokzVeRz5+jo6OrqKrPSycnp8ePHOI7/+OOPNBpt7Nixr169wnG8oKBg0aJF5GaRkZFMJvOrr75qaWm5d++eu7v7qFGjnj9/TrTK76Jt27axWKzTp083Nzfv2rWLRqPduXNH8VN7++23J0+e3G9TV1dXbW1tSkoKi8X66quv+t2mo6ODz+eHhITIrA8LC0MIFRcXDxqAFnMaXI8DoEqenp4CgcDc3FwkEnV0dNTU1JBNdDqduFB1dXVNS0trb2/PzMxU4hA+Pj5tbW0RERGqi/r/dHR0PH782MnJaaANPDw8Nm/eXFVVtXPnTpkmiUSSmJjo7++/cuVKIyMjNze39PT0xsbGo0ePUjfrt4s6OzvT0tL8/PwCAgKMjY13797NYDCU65++7OzsbG1to6KiDhw4sGzZsn63iYuLs7a2jo2NlVk/btw4hFBpaalKIlETyOMAqAXxalCpVNpv67Rp07hcLjnmoDvq6+txHOdyuXK2iY2NHT9+fGpq6s2bN6nry8rKXr16NW3aNHLN9OnTmUwmOYIkg9pFDx48EIvFEydOJJo4HI6VlZWq+ufJkyf19fVff/318ePHp06d2vdmQ15eXk5OzuXLl6n3dQlEVxBvldJZkMcB0A4Wi9XQ0KDtKGR1dnYihPq9Z0his9mZmZkYhq1evVoikZDriSl6hoaG1I2NjY3b29sHPW5HRwdCaPfu3dhvqqur+84jVA6DwTA3N/f29s7KyiorK4uLi6O2ZmVl7d+/v6ioaOzYsX1/lsPhoN+6RWdBHgdAC6RSaUtLi3beHSMXkbYGff7Fw8Njy5YtFRUVMTEx5EpjY2OEkEzWVvA0zc3NEUKHDx+mDvveunVLiVOQQygUGhgYlJWVkWtSUlJOnDhx7dq10aNH9/sjxKu6iW7RWZDHAdCCoqIiHMdnzpxJLNLp9IFGYDTMwsICwzBFZojHxMS4uLgUFxeTayZOnGhoaPivf/2LXPPTTz91dXX97ne/G3RvdnZ2bDa7pKREubD71dTUtHz5cuqaioqKnp4eOzs7hBCO4zt27CgtLc3Pz5f5DkFFdIWlpaUKA1M5yOMAaEhvb29zc3N3d/e9e/dCQ0Pt7e2DgoKIJqFQ+PLly/z8fKlU2tDQUF1dTf1BU1PTp0+fVlVVtbe3S6XSgoIC9c075HK5jo6OtbW1g25JjK5Q51+z2eytW7fm5eWdOHGira2ttLR0/fr11tbWwcHBiuzt448/PnXqVFpaWltbW09PT21tLfHAjkgksrS0VOK5fx6Pd+XKlWvXrrW1tUml0uLi4o8++ojH423ZsgUhVF5efuDAgWPHjjEYDIzi4MGD1J0QXeHm5jbUo2uUVmbJ4DDvUH/AvMN+paSkEDO+uVyur69vamoqcUNs3LhxlZWVR48eFQgECKExY8b8+uuvOI4HBwczGAwbGxs6nS4QCBYvXlxZWUnurampae7cuWw228HB4dNPP92+fTtCSCgUEhMT7969O2bMGA6HM3v27OfPn1+8eJHP58fGxipxpop87kJCQhgMhlgsJhbz8vKI6SujRo365JNPZDbevn07dd5hb29vQkLCuHHjGAyGiYmJn5/fgwcPiKZBu+j169c7duywt7en0+nm5uYBAQFlZWU4jvv5+SGEIiMj+4321q1bs2bNsra2JhKalZWVp6fn9evXiVZfX18HBwdDQ0MWi+Xk5CQSiUpLS4mmgaagJCQkUPfv4+NjY2PT29urkr5VE8jjYBCQx1WCeP5brYdQhCKfu4qKCjqdPtA8a83rF73DQQAAIABJREFU6emZM2dORkaG5g/d2NjIZrMPHjyoyMZazGkwrgKAhuhL8TyhUBgdHR0dHU0+v65FPT09+fn57e3tIpFI80ePioqaMmVKSEiI5g89JHqTx9esWcPn8zEMU+2dkOEbqGCmHNRCoAQmk2lhYfHOO+8kJCQ0NzerL1q1ev369aZNm6ysrLhc7rvvvkvcMUtPT1fhIZQuQwqGJCwsLDAwUCQSab0kVlFR0ZkzZwoKCuRPaVeHxMTEkpKSixcvMhgMDR96yLTyLQBX6jsIUYpBkQdkNebXX3+dNWsWQmigB4LlcHJyMjIywnGcuP31/fffBwUFYRhmbW09pMeR1U3xMYe9e/c6Ozs3Nzf/9a9/zc3NraioQAh98cUXKgzGx8fn4MGD9fX17e3tOTk5DAbjvffeI1u9vLxSU1Obmpra2tqys7MZDMb8+fMV2a26x1XCwsKIZ17Gjh2bm5urvgMNakifu8uXL+/YsUOt8eis/Pz8uLi47u5uxX9EiZymKpDHlVdSUuLv73/ixIkpU6YMJ49T5ebm0mg0CwuLlpYWFYU5XIrnuOnTpy9fvpxcVEkeF4vFHh4e5KKfn59EIiEXiWrRT58+JRZ9fHyoH7ylS5cihMgaJnK8CfcACFrMNSOeFvtWb8ZVEEIYhmk7hP8hv2CmcpYsWRIUFFRfX6/a4QjNqK2tVfk3UJmSrcMsQwrAiKTTeRzH8YSEhPHjx7NYLCMjI2IyFqnfKpeD1sa8fv36jBkzuFyuQCBwc3MjRlfVUTBT6eKixJzigoICvThNwtWrV4VC4bNnz44fP45hWL9PVeADVzRFCN24ccPV1dXIyIjNZru5uV2+fBn1V7JVxlDLkAIwMmnlWwCu2HeQ8PBwDMMOHTrU3NwsFotTU1MRZVxloCqXcmpjvnr1SiAQxMfHSySS58+f+/v7E6VB1VEwc9Diov2Oq+A4TuRcOzs7HTlNxcccLC0tP/roI3JRZlxFfkXT3NzcqKioly9fNjU1zZw508zMjFgvU7KVMJwypMM8R32nyOcOKEeLfau7eVwsFnO5XOpdLOr4uEQi4XK5IpGI3JjFYm3YsAH/LcGRo6hE9n/48CGO4//5z38QQufPn6ceSM6uFCSn8LEcA+VxHMcxDDM2NpYfm8ZOUyV5XCwWGxoakkfHcfyf//wnQqjfv3NEGSOi8F6/eZx4SNrMzOwvf/kLWb1aRnh4uLOzc1tbmyKRQx4Hw6fFvqVr6rp/yB4+fCgWi+fNm9dvq+JVLqm1MR0dHS0sLFauXLlp06agoCCivJlaC2YqoaOjA8dx4lG3EXOaQ6poSgyyy5lt/eTJk5aWluLi4rCwsKNHj167ds3CwoK6AVGG9MqVK33LkMpBvmN3ZDt8+HBubq62owCqpLvj40RZA6IKWl/KVbnkcDjXrl2bPXv23r17HR0dRSKRRCJRa8FMJfz6668IIRcXFzSCTnPQiqYXLlx45513zM3NWSzWZ599Jn9vwylDCsDIo7vX48S0hNevX/fbSla5DA0NHdJuJ0yY8O233zY0NCQmJu7fv3/ChAnEc2JK7EpNLl26hBB6//330Qg6TfkVTWtqavz8/Pz9/f/2t7+NHj06JSVl0FRO6LcM6eXLl69duyangt1A3oSrVAzDNm/eTMzIBKqlxQl1uns9PnHiRBqNdv369X5blaty+fTp0/LycoSQubn5vn373N3dy8vL1VEwU2nPnz8/fPiwra3t6tWr0Qg6TfkVTUtLS6VS6YYNGxwdHdls9kCfB5WUIQVg5NHdPE4UPDt9+nRGRkZbW9u9e/eob/mTU+VSjqdPn65bt+7+/ftdXV3FxcXV1dUzZ85UbleDUqS4KI7jr169IkqpNTQ0ZGdnz5o1y8DAID8/nxgf1/3TVJD8iqb29vYIocLCws7OzoqKCuqgObVkK5PJHH4ZUgBGIK3cXcUVu7fb3t6+Zs0aMzMzQ0PD2bNnR0ZGIoRsbW1//vlnfIAql/JrY1ZVVXl6epqYmBgYGIwePTo8PJx4/G+ggpnyyS+YKae46Llz5yZNmsTlcplMJo1GQwgRE1RmzJgRHR3d1NRE3Vjrp6nIXI6qqqqpU6cihOh0uru7++nTpw8dOkTMKuHxeP7+/rjciqY4ju/YscPU1NTY2DgwMPDIkSMIIScnp5qaGpmSrcMvQ6r0OY4MinzugHK02LcYcXjNwzAsOzsbxul0X05OzrJly7T1e6IZb8I5EuBzpz5a7FvdHVcBAACgCMjj/bt//z42MK2UQgZAuwoLC8PCwqhVlz/88EPqBt7e3nw+38DAYMKECUq8hk1VpFJpXFycUChkMpnGxsYTJ06sqqrqu1lnZ6eLi8vu3buJxXPnzsXHx+tLjXgZkMf75+LiImc0KisrS9sBAqBRn3/+eXJy8q5duwICAh49euTk5GRmZnbixIkLFy6Q21y5ciU3N3fhwoVlZWXu7u7aCnXZsmX/+Mc/Tp48KRaLf/nlFycnp37L0IeHhz948IBc9PX1ZbPZ8+bNI5510C+QxwHQBIlEMqSXjWhmVwrav39/VlZWTk4O9fnY5ORkGo0WHBys9XdNUGVlZeXn5+fm5r799tt0Ot3a2vrs2bPkY8ykH3/8kShfQbVp06bJkycvWLCgu7tbU/GqBuRxADRBpgCvjuxKEQ8fPoyIiNizZw+1YjBCyNPTMzQ0tK6ubtu2bRoLZlBffPGFu7u7/NfbSySS7du3JyUl9W2KiooqKSnpt0mXQR4HQFH4wKV3Q0JCmEymlZUVsbhx40Yej4dhWGNjI+pTgDc5OZnNZltYWKxbt87a2prNZnt6epKz5oe0KzSMCskKSk5OxnHc19e3b1NsbKyzs/OXX35ZWFjY78/K6bFBay8rUWa5q6vr9u3bU6ZMkb9ZeHj4xo0b+635YWJi4uXllZSUpGeTl9Q/tbF/COax6ok3YW61gucov/TuihUrLC0tyY0TEhIQQkTFYLxP4cbg4GAej1deXt7Z2VlWVjZ9+nQ+n0++umhIuxq0QjKVEp87R0dHV1dXmZVOTk6PHz/GcfzHH3+k0Whjx4599eoVjuMFBQWLFi0iN5PfY3JqL+NKlVl+/PgxQmjKlCnvvPOOlZUVi8VycXE5cuQI8agd4ebNm76+vjiONzQ0IITCw8NldhIWFoaUeu+YFnMaXI8DoBCJRJKYmOjv779y5UojIyM3N7f09PTGxkbqY8ZDQqfTiQtVV1fXtLS09vb2zMxMJfbj4+PT1tYWERGhXBjydXR0PH782MnJaaANPDw8Nm/eXFVVtXPnTpkmBXvM09NTIBCYm5uLRKKOjo6amhqEUGdnZ1pamp+fX0BAgLGx8e7duxkMxqD9Q9zPNDc337t3b1lZ2YsXLxYvXvzJJ598/fXXZEihoaFpaWlydjJu3DiE0ECPlekmyOMAKGRIpXeHatq0aVwuV4vVkgdCVIGX/6762NjY8ePHp6am3rx5k7p+qD1Grb2sXJll4g2LEyZM8PT0NDU1NTIy2rNnj5GREfmXY9euXX/+85+J1wEOhDjZFy9eyD+WToE8DoBCBi29O0wsFov4pq9TOjs70W/5cSBsNjszMxPDsNWrV0skEnL9cHpMuTLLRJEM4kYCgclkjhkzprKyEiF08+bN0tLSNWvWyN8Jh8NBv524voA8DoBC5JfeHSapVKqqXakWkdQGfTrGw8Njy5YtFRUVMTEx5Mrh9BhZsZk6Cnzr1i35P2VoaDhu3Dii2Cepu7vbyMgIIZSRkfHdd9/RaDTiDwNxiL1792IYRq3E2dXVRZ64voA8DoBC5JfeRQjR6XRiTEAJRUVFOI7PnDlz+LtSLQsLCwzDFJkhHhMT4+LiUlxcTK4ZtMfkULrM8rJly4qLix89ekQsisXi6upqYhpiZmYm9a8C9T4ndfCHOFmiypu+gDwOgELkl95FCAmFwpcvX+bn50ul0oaGhurqauqPUwvwEjm6t7e3ubm5u7v73r17oaGh9vb2QUFBSuxKkQrJSuNyuY6OjsTLueQjRlcMDAyoa+T3mPy9DVRmWSQSWVpaDvTc/5YtW8aMGRMUFFRTU9PU1LRjxw6JRNL3HqwcxMnKn4GuczQ3NeZ/IZh3qCdg3iFJfundpqamuXPnstlsBweHTz/9dPv27QghoVBIzCaUKcAbHBzMYDBsbGzodLpAIFi8eHFlZaVyu5JTIbkvJT53ISEhDAZDLBYTi3l5ecT0lVGjRn3yyScyG2/fvp0671BOj8mvvYwPXGbZz88PIRQZGTlQwE+ePPnggw9MTExYLNaMGTMKCgr63WygeYc+Pj42NjbUqYoK0mJOgzwOBgF5XB2Cg4NNTU01eUSCEp+7iooKOp3+1VdfqSmkoerp6ZkzZ05GRoY6dt7Y2Mhmsw8ePKjEz2oxp8G4CgDaoS+l9YRCYXR0dHR0dL/VpjSsp6cnPz+/vb1dTTVHo6KipkyZEhISoo6dqw/kcQDAIMLCwgIDA0UikdZLYhUVFZ05c6agoED+lHblJCYmlpSUXLx4kcFgqHznagV5HABN27VrV2ZmZmtrq4ODw+nTp7UdjkL27t0bEhKyb98+7YYxb968kydPksVnVOjs2bOvX78uKioyMTFR+c7Vja7tAAB448TFxcXFxWk7iiHz9vb29vbWdhTqsmjRokWLFmk7CiXB9TgAAOg3yOMAAKDfII8DAIB+gzwOAAD6DfI4AADoOa08fYTr1zuTAABAAdp6nlNr8w4VedseAMO3bNmy0NBQDw8PbQcCRj5PT0+tHBeDS2MwsmEYlp2dvXTpUm0HAoC6wPg4AADoN8jjAACg3yCPAwCAfoM8DgAA+g3yOAAA6DfI4wAAoN8gjwMAgH6DPA4AAPoN8jgAAOg3yOMAAKDfII8DAIB+gzwOAAD6DfI4AADoN8jjAACg3yCPAwCAfoM8DgAA+g3yOAAA6DfI4wAAoN8gjwMAgH6DPA4AAPoN8jgAAOg3yOMAAKDfII8DAIB+gzwOAAD6DfI4AADoN8jjAACg3yCPAwCAfoM8DgAA+g3yOAAA6DfI4wAAoN8gjwMAgH6DPA4AAPqNru0AAFCxU6dOtbe3U9cUFha2tLSQi35+fubm5hqPCwB1wXAc13YMAKhSUFDQ8ePHGQwGsUj8hmMYhhDq6ekxNDSsr69nsVjaDBEAlYJxFTDSfPDBBwgh6W+6u7u7u7v/P3t3HtfEmT8O/BnIHUK4D7mUhKMoqKy0QGWp65a1sqKASLRaqS+7eLSIVxHxQK6KWGRRqKt12bZaEdSCVVGLSldXbd0VKsXWIgiIF7ccCXLN7495Ob98OUIIuSb9vP8yM5NnPs+QfJw888xniH/r6+uHhYVBEgc6Bs7Hga7p6+uztLRsaWkZdu3ly5f/9Kc/qTkkAFQKzseBrqHRaIsXLybHVaSZmZn5+/urPyQAVAryONBBixcv7u3tHbSQTqcvW7ZMX19fIyEBoDowrgJ0EI7j9vb29fX1g5b/+OOPXl5eGgkJANWB83GggzAMW7p06aChFTs7uxkzZmgqJABUB/I40E2DhlbodHpERAQx+xAAHQPjKkBnubq63r9/n3z5888/T548WYPxAKAicD4OdNayZcvIoRU3NzdI4kBXQR4HOmvp0qV9fX0IITqdvnz5ck2HA4CqwLgK0GUzZsz43//+h2FYTU2Nvb29psMBQCXgfBzosvfeew8h9MYbb0ASBzpMrfUOb968mZ6ers49gt+57u5uDMNevnwZFham6VjA74iPj8+GDRvUtju1no8/evTo5MmT6twjUJtbt27dunVL01EMxmKxLC0tbW1tldKadvZR6err6+F7Oh63bt26efOmOveogfrj+fn56t8pUDXihFcL/7gPHjwQCoVKaUpr+6hceXl54eHhOt9N1VH/jz8YHwc6TllJHACtBXkcAACoDfI4AABQG+RxAACgNsjjAABAbZDHAVCt8+fP8/n8b7/9VtOBqEpxcXFsbOypU6ccHR0xDMMwbNmyZdIbBAQE8Hg8fX39yZMn37lzR1Nx9vb2pqSkCIVCBoNhZGQ0ZcqUmpqaoZt1d3e7urpu27aNeHnmzJnU1NT+/n61xjpGkMcBUC3dLn2xc+fOzMzMrVu3hoaGVldXCwQCU1PTo0ePnjt3jtzm0qVL+fn58+bNq6io8PT01FSo4eHhX3755bFjx8Ri8S+//CIQCDo7O4duFhcXJ10mMygoiMVizZ49u62tTY3Bjg3kcQBUKzAw8MWLF/PmzVP1jiQSia+vr6r3Im337t25ubl5eXk8Ho9cmJmZqaenFxkZ+eLFC3UGI1tubm5BQUF+fv4bb7xBo9Gsra0LCwunTJkyaLMbN278/PPPgxauW7du6tSpc+fOJcquaSHI4wDoiCNHjjQ0NKhtdw8ePNi+ffuuXbtYLJb0cl9f3+jo6MePH2/atEltwYzqs88+8/T0dHd3l7GNRCLZvHlzRkbG0FXx8fFlZWXDrtIGkMcBUKHr16/b29tjGHbgwAGEUHZ2NpfL5XA4hYWF77zzjqGhoa2t7fHjx4mNMzMzWSyWhYXFqlWrrK2tWSyWr6/vDz/8QKyNiopiMBhWVlbEy7Vr13K5XAzDmpqaEELR0dEbN26sqqrCMIy49enChQuGhobJyckq6lpmZiaO40FBQUNXJSUlOTs7f/7558XFxcO+F8fx9PT01157jclkGhsbL1iw4NdffyVWyT5ECKH+/v4dO3bY29uz2WwPD48TJ06MGmpPT8+tW7emTZsme7O4uLi1a9eam5sPXWVsbOzv75+RkaGdo2SQxwFQoZkzZ964cYN8uWbNmvXr10skEh6Pd+LEiaqqKkdHxw8++IB4BF1UVFRERIRYLF63bl1NTc2dO3f6+vrefvvtR48eIYQyMzMXLVpENpWVlbVr1y7yZUZGxrx58wQCAY7jDx48QAgRl+YGBgZU1LVz5865uLhwOJyhq9hs9r/+9S89Pb0PPvigq6tr6Abx8fGxsbFxcXENDQ3//ve/Hz165Ofn9/z5czTaIUIIbdmyZc+ePfv27Xv69Om8efOWLFny3//+V3aoT5486enp+d///jdr1iziP8jXXnstKytLOin/5z//qaqqWrJkyUiNTJ8+/fHjxz/99JM8B0fNII8DoAG+vr6Ghobm5uYikairq6uuro5cRaPRiBNVNze37Ozsjo6OnJwcBXYRGBjY3t6+fft25UX9/3V1dT18+FAgEIy0gY+Pz/r162tqarZs2TJolUQiSU9PDwkJWbp0KZ/Pd3d3P3jwYFNT06FDh6Q3G/YQdXd3Z2dnBwcHh4aGGhkZbdu2jU6nj3p8iOuZ5ubmycnJFRUVz58/X7BgwYcffvj111+TIUVHR2dnZ8toxMnJCSFUXl4ue18aAXkcAE1iMBgIIelHQkubMWMGh8Mhxxy0R0NDA47jw56Mk5KSklxcXLKysq5fvy69vKKiorOzc8aMGeQSLy8vBoNBjiANIn2I7t+/LxaLyeuTbDbbyspq1OPDZDIRQpMnT/b19TUxMeHz+bt27eLz+eT/HFu3bv3b3/5mY2MjoxGis8SPBm0DeRwArcZkMhsbGzUdxWDd3d3oVX4cCYvFysnJwTBsxYoVEomEXE5M4DMwMJDe2MjIqKOjY9T9EqM027Ztw16pra0Vi8Wy32VtbY0QIi4kEBgMhoODQ1VVFULo+vXr5eXlK1eulN0Im81GrzqubSCPA6C9ent729ralFU/XYmIpDbq3THE4xQqKysTExPJhUZGRgihQVlbzm4SFyH37duHSxm12LeBgYGTk9O9e/ekF/b19fH5fITQkSNHLl++rKenR/zHQOwiOTkZwzDpkfeenh6y49oG8jgA2qukpATHcW9vb+IljUYbaQRGzSwsLDAMk2eGeGJioqura2lpKblkypQpBgYG0inyhx9+6Onp+cMf/jBqa3Z2diwWq6ysbKwBh4eHl5aWVldXEy/FYnFtbS0xDTEnJ0f6fwXi109cXByO49KDP0RnLS0tx7prNYA8DoB2GRgYaG1t7evru3v3bnR0tL29fUREBLFKKBS2tLQUFBT09vY2NjbW1tZKv9HExOTJkyc1NTUdHR29vb1FRUWqm3fI4XAcHR3r6+tH3ZIYXdHX15desnHjxtOnTx89erS9vb28vHz16tXW1taRkZHytPb+++8fP348Ozu7vb29v7+/vr7+6dOnCCGRSGRpaTnSff8bNmxwcHCIiIioq6trbm6OiYmRSCRDr8HKQHRW9gx0TYE8DoAKHThwwMvLCyEUExMzf/787Ozsffv2IYQ8PDyqq6sPHz68ceNGhNCcOXMqKyuJt3R3d7u7u7PZbD8/P2dn56tXr5LD0GvWrJk1a9bixYtdXFwSExOJ3/g+Pj7ExMTVq1dbWFi4ubnNnTu3paVF1V0LDAysqKggB76/+eYboVBYVVXl5eX10UcfSW/p7e096GGVO3fuTElJSUhIMDMz8/f3nzhxYklJCZfLRQiNeogyMjLWr1+fmppqampqbW0dHR3d2tqKEOrp6WloaCgsLBw2WmNj42vXrtna2k6bNs3GxubHH388d+7cqDPKpd2+fdvGxsbDw0P+t6gPrkbEjH117hGozcKFCxcuXKjpKFRLDX2MjIw0MTFR6S5GJef3tLKykkajffXVV2oISR79/f1+fn5HjhxRReNNTU0sFmvv3r3ybKz+7wKcjwOgXbS8tB5JKBQmJCQkJCQMW21Kzfr7+wsKCjo6OkQikSraj4+PnzZtWlRUlCoaHz/I4wAABcXGxoaFhYlEIo2XxCopKTl16lRRUZHsKe2KSU9PLysrO3/+PJ1OV3rjSqHteXzlypU8Hg/DMAWuUGsP6dLMBAaDYWFh8dZbb6WlpRGje78fL1++XLdunZWVFYfD+fOf/0zMfDh48KASd5Gamurq6spms7lcrqur6/bt29vb28m1CQkJbm5uhoaGTCZTKBR+/PHH2nBGiRDaunVrTk7OixcvJk2adPLkSU2HI5fk5OSoqKhPPvlEs2HMnj372LFjZPEZJSosLHz58mVJSYmxsbHSG1cadQ7iKDY+TpTIKS0tVUVI6iQQCPh8Po7jxISEq1evRkREYBhmbW19+/ZtTUc3XvKPCSYnJzs7O7e2tv7jH//Iz88nLl599tlnSgwmMDBw7969DQ0NHR0deXl5dDr97bffJtf6+/tnZWU1Nze3t7efOHGCTqfPmTNHnmZ/D9cAcLiONW4wPk4lCpd7xjDMyMjorbfeysnJycvLe/78OVGiWukRjpOK6lkXFBTMmDHDyMjob3/728KFC5XS5qBQGQwGUbjOwMAgLCxswYIF3333HTE7DSFkYGBAXE7k8XiLFi0KDg6+cOECMeUDACqiQB7HMEzTIQxPKeWeFy5cGBER0dDQoNyBBaVQUT3r+vp6pY8zDgr19OnT0kWxiboZ5ODJ2bNnpaczm5mZIYRGvbcbAK2ljXkcx/G0tDQXFxcmk8nn8zdv3kyu2rNnD4fD4fF4DQ0NGzdutLGxuX//Pj5yLWPZBZ2RzDrIaiv3TNzlUVRUpOUdHL/vvvtOKBQ+ffr0iy++wDBsUIWNUQNGCF27ds3NzY3P57NYLHd394sXL8oTamVlpZGRkYODw7BRPX78mM1mT5o0SSl9BEAD1DmII+e4W1xcHIZhn376aWtrq1gszsrKQlLj43FxcQihdevW7d+/PyQk5JdfftmxYweDwfjqq6/a2tru3r3r6elpZmb27NkzYvvIyEgul3vv3r3u7u6KigovLy8ej1dXV0eslf3ed99919LSkgwsLS0NIdTY2Ei8DA0NJco9E86ePcvj8RISEkbqFzk+PghxCc7Ozk7LOyib/GOClpaWy5cvJ18OGh+XHXB+fn58fHxLS0tzc7O3t7epqamMUHt6eurr6/fv389kMkea5tzV1cXj8aKiopTbR0qD8fFxUv/nROvyuFgs5nA40lelBl3nJNKcRCIhtzcwMBCJROT2P/74I0KIzKeRkZHS2fP27dsIoV27dsnzXiWmOXzkPI7jODFiTukOKiWPjxqwtJSUFPSqgOqwoRKlMExNTf/+97/39PQMG0xcXJyzs3N7e7s8kUMeB/JQ/+eEpq7zfnk9ePBALBbPnj1bzu3HWstYuqDzWN+rIl1dXTiOGxoaDrtWBzoovzEFTAyyy7hr5tGjR21tbaWlpbGxsYcOHbpy5YqFhYX0BqdPn87Ly7t06ZL0Y4JlO3nypNZesFGu30k3VURZF/DlpHV5nChGM+wj8oalQC1jsqDzeOogK9Fvv/2GEHJ1dR12rQ50UH6jBnzu3Lm0tLSKior29vZRK//R6XRzc/OAgIBJkyY5OzunpKRIPyc3Nzc3PT29pKRkwoQJ8kfo7e29fv16+benops3b2ZkZMjz3EswLKI+jDppXR4nphm8fPlSzu3HWstYuqDzeOogK9GFCxcQQu+8886wa3Wgg/KTHXBdXV1wcHBISMg///nPCRMm7N+//+OPP5anWaFQqK+vX1FRQS7Zv3//xYsXr1y5Muy1VhlsbW2lH5KpqzIyMn4P3VSR/Px8Ne9R6+arTJkyRU9P7/vvv5d/+zHVMpYu6Dzqe9VQ7vnZs2f79u2ztbVdsWLFsBtQvYNjIjvg8vLy3t7eNWvWODo6sliskX74Nzc3D3pabmVlZX9/v52dHUIIx/GYmJjy8vKCgoKxJnEAtJPW5XFzc/PQ0NCTJ08eOXKkvb397t27g56+Oog8tYxHKug86nuVXu4Zx/HOzs6BgQEcxxsbG0+cOPHmm2/q6+sXFBSMND6uPR2U0S9lkR2wvb09Qqi4uLi7u7uyslJ60Fw6VAaDcenSpStXrhBjL6WlpcuXL+dyuUTp1Hv37u3Zs+fw4cN0Ol26UsLevXvV0EEAVEKdF1XlvA7e0dGxcuVKU1NTAwODmTNn7tixAyFka2sxMk2sAAAgAElEQVT7008/paamEjWX7ezsyJlkAwMDaWlpTk5OdDrd2Ng4ODiYmHNNiIyMpNPpNjY2NBrN0NBwwYIFVVVV5FrZ721ubp41axaLxZo0adJHH31EzGQXCoXErL47d+44ODiw2eyZM2c+e/bs/PnzPB4vKSlpaI/OnDnj4eHB4XAYDIaenh56dUvn66+/npCQ0NzcTG6pzR2U/VeT5xp9TU3N9OnTEUI0Gs3T0/PkyZOffvopMauEy+WGhISMGnBMTIyJiYmRkVFYWNiBAwcQQgKBoK6ublCoQUFBkyZNMjAwYDKZAoFAJBKVl5cTLYz0vPO0tDTZwcvZRx0A81XGSf2fEwzHcXX8d4EQQigvLy88PFyde0QIrVq1Kj8/v7m5WZ07VSct6WBYWBjSxMigOv0e+og09D3VJer/nGjduIoqUKWgs8J0voMAABl+F3kcAKA6xcXFsbGx0sWZly1bJr1BQEAAj8fT19efPHnySM/PVIPe3t6UlBShUMhgMIyMjKZMmVJTUzN0s+7ubldX123bthEvz5w5k5qaquWnSjqex6lY0HlMdL6DQMvt3LkzMzNz69atoaGh1dXVAoHA1NT06NGj586dI7e5dOlSfn7+vHnzKioqPD09NRVqeHj4l19+eezYMbFY/MsvvwgEgmHrzsfFxd2/f598GRQUxGKxZs+eTdzcoJ10PI+npKS8fPkSx/GHDx+q+Q4r9dD5Dv7eKLFWsIrKDkvbvXt3bm5uXl6e9A2xmZmZenp6kZGRWlWKOTc3t6CgID8//4033qDRaNbW1oWFhVOmTBm02Y0bN37++edBC9etWzd16tS5c+f29fWpK96x0fE8DgC1KLFWsIrKDpMePHiwffv2Xbt2SZcIRgj5+vpGR0c/fvx406ZNqtv7WH322Weenp7u7u4ytpFIJJs3b5a+6ZcUHx9fVlY27CptAHkcACXDlVQrWHZRYrXVVR5JZmYmjuNBQUFDVyUlJTk7O3/++efFxcVjPUTZ2dlcLpfD4RQWFr7zzjuGhoa2trZEsTxCf3//jh077O3t2Wy2h4eHPPUDenp6bt26NW3aNNmbxcXFEY8fGbrK2NjY398/IyNDS6fxqHOSI8xL1WG/h7nVcvZRibWCZRclVm5dZZKc31NHR0c3N7dBCwUCwcOHD3Ecv3Hjhp6e3sSJEzs7O3EcLyoqmj9/PrmZ7ENElPy8fPnyixcvGhoa/Pz8uFwuWbFy06ZNTCbz5MmTra2tW7du1dPTG/WxiA8fPkQITZs27a233rKysmIyma6urgcOHCDuyCNcv349KCgIx3GiNlFcXNygRmJjY5F8D5iE57oBQG0SiSQ9PT0kJGTp0qV8Pt/d3f3gwYNNTU2yb0uWgUajEeetbm5u2dnZHR0dOTk5CrQTGBjY3t6+fft2xcIYpKur6+HDhwKBYKQNfHx81q9fX1NTs2XLlkGr5DxEvr6+hoaG5ubmIpGoq6urrq4OIdTd3Z2dnR0cHBwaGmpkZLRt2zY6nT7qASGuZ5qbmycnJ1dUVDx//nzBggUffvjh119/TYYUHR2dnZ0toxEnJyeE0Ej3kWkW5HEAlEmltYKlixJrFlH2ncPhyNgmKSnJxcUlKyvr+vXr0svHeogYDAZCiKgMcf/+fbFYTF6fZLPZVlZWox4QJpOJEJo8ebKvr6+JiQmfz9+1axefzyf/59i6devf/vY34vl/IyE6+/z5c9n70gjI4wAok6prBZNFiTWru7sbvcqPI2GxWDk5ORiGrVixQiKRkMvHc4i6uroQQtu2bSML49TW1o76bFVra2uEEHHlgMBgMBwcHKqqqhBC169fLy8vX7lypexGiIIZRMe1DeRxAJRJpbWCpYsSaxaR1Ea9O8bHx2fDhg2VlZWJiYnkwvEcIuIi5L59+6RHh2/evCn7XQYGBk5OTvfu3ZNe2NfXx+fzEUJHjhy5fPmynp4e8R8DsYvk5GQMw6RLb/b09JAd1zaQxwFQJpXWCpYuSjzOpsbJwsICwzB5ZognJia6urqWlpaSS8ZailmanZ0di8UqKysba8Dh4eGlpaXV1dXES7FYXFtbS0xDzMnJkf5fQfo6p/TgD9FZoqybtoE8DoAyKb1W8EhFicfalDx1leXH4XAcHR2Jp3eNekBycnL09fWll4xaillGa++///7x48ezs7Pb29v7+/vr6+ufPn2KEBKJRJaWliPd979hwwYHB4eIiIi6urrm5uaYmBiJRDL0GqwMRGdlz0DXFMjjACjZzp07U1JSEhISzMzM/P39J06cWFJSwuVyibVr1qyZNWvW4sWLXVxcEhMTid/pPj4+jx49QgitXr3awsLCzc1t7ty5LS0tCKHu7m53d3c2m+3n5+fs7Hz16lVyVHqsTSlXYGBgRUUFOfD9zTffCIXCqqoqLy+vjz76SHpLb29vovi7PIcoOzubeC6ah4dHdXX14cOHN27ciBCaM2cO8UjujIyM9evXp6ammpqaWltbR0dHt7a2IoR6enoaGhoKCwuHjdbY2PjatWu2trbTpk2zsbH58ccfz507N+qMcmm3b9+2sbHx8PCQ/y3qo74pjjB/XKfB/HFViIyMNDExUececbm/p5WVlTQajaySr3H9/f1+fn5HjhxRReNNTU0sFmvv3r3ybAzzxwEA/4fWVtoTCoUJCQkJCQnDVptSs/7+/oKCgo6ODpFIpIr24+Pjp02bFhUVpYrGxw/yOABAQbGxsWFhYSKRSOMlsUpKSk6dOlVUVCR7Srti0tPTy8rKzp8/T6fTld64UkAeB0BLUaIocXJyclRU1CeffKLZMGbPnn3s2DGy2owSFRYWvnz5sqSkxNjYWOmNKwtN0wEAAIaXkpKSkpKi6ShGFxAQEBAQoOkoVGX+/Pnz58/XdBSjgPNxAACgNsjjAABAbZDHAQCA2iCPAwAAtWngOmdeXp76dwpUjbhrWbf/uL+HPiKEiLJTOt9N1amvr1d3LTN13nQkzxOYAACA6tR8PyeGa+fj5gBQEgzDTpw4sWjRIk0HAoCqwPg4AABQG+RxAACgNsjjAABAbZDHAQCA2iCPAwAAtUEeBwAAaoM8DgAA1AZ5HAAAqA3yOAAAUBvkcQAAoDbI4wAAQG2QxwEAgNogjwMAALVBHgcAAGqDPA4AANQGeRwAAKgN8jgAAFAb5HEAAKA2yOMAAEBtkMcBAIDaII8DAAC1QR4HAABqgzwOAADUBnkcAACoDfI4AABQG+RxAACgNsjjAABAbZDHAQCA2iCPAwAAtUEeBwAAaoM8DgAA1AZ5HAAAqA3yOAAAUBuG47imYwBAmSIjI+/fv0++vHPnzqRJk4yNjYmX+vr6X3zxha2trYaiA0D5aJoOAAAls7S0PHTokPSSu3fvkv92dHSEJA50DIyrAF2zZMmSkVYxGIyIiAg1xgKAOsC4CtBBU6ZMuXfv3rCf7fv37zs7O6s/JABUB87HgQ5677339PX1By3EMGzq1KmQxIHugTwOdNDixYv7+/sHLdTX11++fLlG4gFApWBcBegmX1/fH374YWBggFyCYdijR49sbGw0GBUAqgDn40A3LVu2DMMw8qWent7MmTMhiQOdBHkc6KawsDDplxiGvffee5oKBgCVgjwOdJOZmdns2bPJq50YhgUHB2s2JABUBPI40FlLly4lLv/o6+v/5S9/MTU11XREAKgE5HGgs0JCQhgMBkIIx/GlS5dqOhwAVAXyONBZXC73r3/9K0KIwWDMmzdP0+EAoCqQx4Eue/fddxFCwcHBXC5X07EAoDK4Fjhx4oSmDwMAAIzZwoULNZ0+cRzHtajeIWRzSti3bx9CaP369ZoORF5Hjx4ViUQ02hg+6pTro2Ju3ryZkZEB3zuFEZ8TbaBFeXzRokWaDgGMLj8/H1HqjxUUFMRiscb0Fsr1UWEZGRm/h26qCPE50QYwPg503FiTOACUA3kcAACoDfI4AABQG+RxAACgNsjjAABAbZDHAVCO8+fP8/n8b7/9VtOBqEpxcXFsbOypU6ccHR0xDMMwbNmyZdIbBAQE8Hg8fX39yZMn37lzR1Nx9vb2pqSkCIVCBoNhZGQ0ZcqUmpqaoZt1d3e7urpu27aNeHnmzJnU1NShjx+hBMjjACgHrtOPZNm5c2dmZubWrVtDQ0Orq6sFAoGpqenRo0fPnTtHbnPp0qX8/Px58+ZVVFR4enpqKtTw8PAvv/zy2LFjYrH4l19+EQgEnZ2dQzeLi4u7f/8++ZKYnzp79uy2tjY1BqsckMcBUI7AwMAXL16ooZCLRCLx9fVV9V6k7d69Ozc3Ny8vj8fjkQszMzP19PQiIyNfvHihzmBky83NLSgoyM/Pf+ONN2g0mrW1dWFh4ZQpUwZtduPGjZ9//nnQwnXr1k2dOnXu3Ll9fX3qilc5II8DQDFHjhxpaGhQ2+4ePHiwffv2Xbt2DZqJ7+vrGx0d/fjx402bNqktmFF99tlnnp6e7u7uMraRSCSbN2/OyMgYuio+Pr6srGzYVdoM8jgASnD9+nV7e3sMww4cOIAQys7O5nK5HA6nsLDwnXfeMTQ0tLW1PX78OLFxZmYmi8WysLBYtWqVtbU1i8UiniZKrI2KimIwGFZWVsTLtWvXcrlcDMOampoQQtHR0Rs3bqyqqsIwTCgUIoQuXLhgaGiYnJysoq5lZmbiOB4UFDR0VVJSkrOz8+eff15cXDzse3EcT09Pf+2115hMprGx8YIFC3799VdilexDhBDq7+/fsWOHvb09m8328PCQp35AT0/PrVu3pk2bJnuzuLi4tWvXmpubD11lbGzs7++fkZFBrVEyyOMAKMHMmTNv3LhBvlyzZs369eslEgmPxztx4kRVVZWjo+MHH3zQ29uLEIqKioqIiBCLxevWraupqblz505fX9/bb7/96NEjhFBmZqb0vfJZWVm7du0iX2ZkZMybN08gEOA4/uDBA4QQcWlO+onSynXu3DkXFxcOhzN0FZvN/te//qWnp/fBBx90dXUN3SA+Pj42NjYuLq6hoeHf//73o0eP/Pz8nj9/jkY7RAihLVu27NmzZ9++fU+fPp03b96SJUv++9//yg71yZMnPT09//vf/2bNmkX8B/naa69lZWVJJ+X//Oc/VVVVS5YsGamR6dOnP378+KeffpLn4GgJyOMAqJCvr6+hoaG5ublIJOrq6qqrqyNX0Wg04kTVzc0tOzu7o6MjJydHgV0EBga2t7dv375deVH/f11dXQ8fPhQIBCNt4OPjs379+pqami1btgxaJZFI0tPTQ0JCli5dyufz3d3dDx482NTUdOjQIenNhj1E3d3d2dnZwcHBoaGhRkZG27Zto9Ppox4f4nqmubl5cnJyRUXF8+fPFyxY8OGHH3799ddkSNHR0dnZ2TIacXJyQgiVl5fL3pdWgTwOgDoQTyYiTzYHmTFjBofDIccctEdDQwOO48OejJOSkpJcXFyysrKuX78uvbyioqKzs3PGjBnkEi8vLwaDQY4gDSJ9iO7fvy8Wi8nrk2w228rKatTjw2QyEUKTJ0/29fU1MTHh8/m7du3i8/nk/xxbt27929/+ZmNjI6MRorPEjwaqgDwOgFZgMpmNjY2ajmKw7u5u9Co/joTFYuXk5GAYtmLFColEQi4nJvAZGBhIb2xkZNTR0THqfolRmm3btmGv1NbWisVi2e+ytrZGCBEXEggMBsPBwaGqqgohdP369fLy8pUrV8puhM1mo1cdpwrI4wBoXm9vb1tbm62traYDGYxIaqPeHePj47Nhw4bKysrExERyoZGREUJoUNaWs5vERch9+/ZJPy3h5s2bst9lYGDg5OR079496YV9fX18Ph8hdOTIkcuXL+vp6RH/MRC7SE5OxjBMeuS9p6eH7DhVQB4HQPNKSkpwHPf29iZe0mi0kUZg1MzCwgLDMHlmiCcmJrq6upaWlpJLpkyZYmBgIJ0if/jhh56enj/84Q+jtmZnZ8discrKysYacHh4eGlpaXV1NfFSLBbX1tYS0xBzcnKk/1cgfv3ExcXhOC49+EN01tLScqy71iDI4wBoxsDAQGtra19f3927d6Ojo+3t7SMiIohVQqGwpaWloKCgt7e3sbGxtrZW+o0mJiZPnjypqanp6Ojo7e0tKipS3bxDDofj6OhYX18/6pbE6Iq+vr70ko0bN54+ffro0aPt7e3l5eWrV6+2traOjIyUp7X333//+PHj2dnZ7e3t/f399fX1T58+RQiJRCJLS8uR7vvfsGGDg4NDREREXV1dc3NzTEyMRCIZeg1WBqKzsmegaxvI4wAowYEDB7y8vBBCMTEx8+fPz87OJh765eHhUV1dffjw4Y0bNyKE5syZU1lZSbylu7vb3d2dzWb7+fk5OztfvXqVHIZes2bNrFmzFi9e7OLikpiYSPzG9/HxISYmrl692sLCws3Nbe7cuS0tLaruWmBgYEVFBTnw/c033wiFwqqqKi8vr48++kh6S29v7w0bNkgv2blzZ0pKSkJCgpmZmb+//8SJE0tKSohnXo96iDIyMtavX5+ammpqamptbR0dHd3a2ooQ6unpaWhoKCwsHDZaY2Pja9eu2draTps2zcbG5scffzx37tyoM8ql3b5928bGxsPDQ/63aJ46HgI6GmKGv6ajAHJZuHChljxbVnXU0MfIyEgTExOV7mJUcn7vKisraTTaV199pYaQ5NHf3+/n53fkyBFVNN7U1MRisfbu3SvPxtrzXYDzcQA0gyql9YRCYUJCQkJCwrDVptSsv7+/oKCgo6NDJBKpov34+Php06ZFRUWponHVoWoeX7lyJY/HwzBMgSshKpKQkODm5mZoaMhkMoVC4ccffyzn5166ECiBwWBYWFi89dZbaWlpxG9JKnr58uW6deusrKw4HM6f//xn4orZwYMHlbiL1NRUV1dXNpvN5XJdXV23b9/e3t5OrlX4LwIGiY2NDQsLE4lEGi+JVVJScurUqaKiItlT2hWTnp5eVlZ2/vx5Op2u9MZVS9M/CHBc0XEVohRDaWmpKkJSgL+/f1ZWVnNzc3t7+4kTJ+h0+pw5c+R/u0Ag4PP5OI4Tl7+uXr0aERGBYZi1tfXt27dVFvWYyf9bMjk52dnZubW19R//+Ed+fj4x6PnZZ58pMZjAwMC9e/c2NDR0dHTk5eXR6fS3336bXKvwX0TVv5djY2OJe14mTpyYn5+vuh3JNtbv3cWLF2NiYlQXj2YVFBSkpKT09fXJ/xbtGVeBPK40gYGB0h8CokRGXV2dnG8n87i0/Px8PT09CwuLtrY2pQU6PvJ/dr28vJYsWUK+VEoeF4vFPj4+5Mvg4GCJREK+DAsLQwg9efKEeKnwX0R7vp8qBdelxkl7PidUHVdBCGEYpukQ/o+zZ89KT7oyMzNDCI16B5psCxcujIiIaGhoUO5whHrU19cr/ffpoJKtp0+fli6mStxvTQ6eqOIvAoAWolIex3E8LS3NxcWFyWTy+fzNmzdLrx22yuWotTG///77119/ncPhGBoauru7E6OrChTMHOrx48dsNnvSpEnES4WLixJziouKirSzm8P67rvvhELh06dPv/jiCwzDBt2ZTcBHrmiKELp27Zqbmxufz2exWO7u7hcvXkTDlWwdpLKy0sjIyMHBYdioBv1FANAdmv5BgONy/76Li4vDMOzTTz9tbW0Vi8VZWVlIalxl06ZNTCbz5MmTra2tW7du1dPTI4aV4+LiEEKXL19+8eJFQ0ODn58fl8vt6enBcbyzs9PQ0DA1NVUikTx79iwkJKSxsVFGU/Lr6uri8XhRUVHkkrNnz/J4vISEhJHeMuy4Co7jRM61s7PTkm7K/1vS0tJy+fLl5MtB4yo7duxgMBhfffVVW1vb3bt3PT09zczMnj17RqzNz8+Pj49vaWlpbm729vY2NTUlloeGhhIlW6X19PTU19fv37+fyWSOND1u6F9EKX2kNBhXGSft+ZxoxV9Rns+TWCzmcDjSV7Gkx8clEgmHwxGJROTGTCZzzZo1+KsER46iEtn/wYMHOI4TD3Y6e/as9I5kNCW/uLg4Z2fn9vZ2+d8yUh7HcRzDMCMjIy3pplLyuFgsNjAwIPeO4/iPP/6IEBr2/7mUlBT0qvDesHmcuIXa1NT073//O/Ff11Bj+otoz/dTpSCPj5P2fE5o6j37V9yDBw/EYvHs2bOHXSt/lUvp2piOjo4WFhZLly5dt25dRETExIkTx9TUSE6fPp2Xl3fp0iXphxkqrKurC8dxQ0PDMcWmhm6Ox5gqmhKD7DJmWz969Kitra20tDQ2NvbQoUNXrlyxsLCQ3kCBv0h9fX1eXp6cG1MUUXZK57upOvX19dpS2kzT/5HguHznBefPn0cISd/EJX0+/p///Gdo17y9vfEhJ6qHDx9GCP3yyy/Ey59//vmvf/0rjUbDMCw8PFwsFstoSh7Hjx/38vJ6/PjxWA/CSOfjRB2JgIAALemmUs7Hv/vuO4TQwYMHpbe3sLD44x//SPz77Nmz/v7+ZmZmDAaDuKD99OlTfITzcdJvv/2GEFq3bp30QgX+IgsXLlTs2wR+b7TkfJwy1zmJaQkvX74cdq1iVS4RQpMnT/7222+fPHkSExNz4sSJvXv3KtwUQmj//v1Hjx69cuXKhAkTxtA3mS5cuIAQeuedd5DWdHP8ZFc0raurCw4OtrKy+uGHH168eJGamipns0KhUF9fv6Kiglyi8F9ES76fKgXjKuOkPf/fUyaPT5kyRU9P7/vvvx92rWJVLp88eUKUKjY3N//kk088PT3v3bunWFM4jsfExJSXlxcUFAw7PUMxz54927dvn62t7YoVK5AWdFNZZFc0LS8v7+3tXbNmjaOjI4vFGmmCaXNz86CnLFZWVvb399vZ2SGV/UUA0EKUyePm5uahoaEnT548cuRIe3v73bt3pZ/yJ6PKpQxPnjxZtWrVr7/+2tPTU1paWltb6+3trVhT9+7d27Nnz+HDh+l0uvQd9nv37iU2kKe4KI7jnZ2dAwMDOI43NjaeOHHizTff1NfXLygoIMbHNd5NZZFd0dTe3h4hVFxc3N3dXVlZKT1oLl2ylcFgXLp06cqVK+3t7b29vaWlpcuXL+dyuUTJvVH/IgDoDs3+MCHI+fuuo6Nj5cqVpqamBgYGM2fO3LFjB0LI1tb2p59+wnH85cuXMTEx9vb2NBqNSPoVFRVZWVlEHQYnJ6eqqqpDhw4RCdHBweG3336rqanx9fU1NjbW19efMGFCXFwccfvfsE3Jjm2kp7KmpaURG5w/f57H4yUlJQ1975kzZzw8PDgcDoPB0NPTQwgRE1Ref/31hISE5uZm6Y01201cvvHxmpqa6dOnI4RoNJqnp+fJkyc//fRTYlYJl8sNCQnBcXxgYCAtLc3JyYlOpxsbGwcHB9+/f59sISYmxsTExMjIKCws7MCBAwghgUBQV1d3584dBwcHNps9c+bMZ8+eBQUFTZo0ycDAgMlkCgQCkUhUXl4u519knH3UATCuMk7a8znBcBxX+v8NY5WXlxceHq4NkYBREfe+5+fnazoQFfo99BHB927ctOdzQplxFQAAAMOCPC6XX3/9FRuZikohAwCAPCCPy8XV1VXG4FRubq6mAwRA5YqLi2NjY6XL5S9btkx6g4CAAB6Pp6+vP3ny5JGen6lqCpekP3PmTGpqKlUe7jEI5HEAwOh27tyZmZm5devW0NDQ6upqgUBgamp69OjRc+fOkdtcunQpPz9/3rx5FRUVnp6eGonz2rVrH3zwQV1d3fPnzxMTE1NTU6VneV+5cuXDDz+sqalpampKSUnJyMggxrgRQkFBQSwWa/bs2W1tbRqJfDwgjwOgARKJxNfXV9uaGsnu3btzc3Pz8vKkCxtkZmbq6elFRkZq/CFB0hgMxtq1a83NzQ0MDMLCwhYsWPDdd9+RE2oNDAyIJ6PyeLxFixYFBwdfuHCBeHo1QmjdunVTp06dO3duX1+f5nqgCMjjAGjAoELqWtLUsB48eLB9+/Zdu3ZJl3pHCPn6+kZHRz9+/HjTpk2q2/tYjbMkfXx8fFlZWUZGhprCVRLI4wAoCB+5hHpUVBSDwbCysiJerl27lsvlYhjW1NSEhhRSz8zMZLFYFhYWq1atsra2ZrFYvr6+5N1PY2oKjaPS/UgyMzNxHA8KChq6KikpydnZ+fPPPy8uLh7rIRq1aL5S6uOPtSS9sbGxv79/RkYGxaZjqn6K+ujgfgQK0Z57H1RHzj7KLqH+7rvvWlpakhunpaUhhIjK7/iQgl+RkZFcLvfevXvd3d0VFRVeXl48Ho98BN2Ymhq10j1Jzu+do6Ojm5vboIUCgeDhw4c4jt+4cUNPT2/ixImdnZ04jhcVFc2fP5/cTPYhklE0Hx/fYwDGU5I+NjYWyffASO35LsD5OACKkEgk6enpISEhS5cu5fP57u7uBw8ebGpqki4XMSY0Go04b3Vzc8vOzu7o6MjJyVGgncDAwPb29u3btysWxiBdXV0PHz4UCAQjbeDj47N+/fqampotW7YMWiXnIfL19TU0NDQ3NxeJRF1dXXV1dQih7u7u7Ozs4ODg0NBQIyOjbdu20el0+Q+InZ2dra1tfHz8nj17wsPDh90mJSXF2to6KSlp0HInJyeE0Ej3A2snyOMAKGJMJdTHasaMGRwOR23l4GUgHt9BVH0YSVJSkouLS1ZW1vXr16WXj/UQSRfNH2d9/EePHjU0NHz99ddffPHF9OnTh14/IErSX7x4cWhJeqKzz58/l3Nf2gDyOACKIGanDaqkaGRkNKgYr8KYTGZjY6NSmhqP7u5uIhgZ27BYrJycHAzDVqxYIZFIyOXjOURdXV0IoW3btpF329XW1sr/jGw6nW5ubh4QEJCbm1tRUUE8UoqUm5u7e/fukpIS4pkqg7DZbPSq41QBeRwARcguoT5Ovb29ympqnIikNurdMT4+Phs2bKisrFCwqNYAACAASURBVExMTCQXjucQKas+vgIl6Xt6etCrjlMF5HEAFCG7hDpCiEajEUMECigpKcFx3Nvbe/xNjZOFhQWGYfLMEE9MTHR1dS0tLSWXjHqIZFCsPr5SStITnSXKc1IF5HEAFCG7hDpCSCgUtrS0FBQU9Pb2NjY21tbWSr9dupA6kaMHBgZaW1v7+vru3r0bHR1tb28fERGhQFPyVLqXH4fDcXR0rK+vl+eA5OTkSM/OHvUQyW5tpPr4IpHI0tJy2Pv+uVzu+EvSE511d3cfNUjtAXkcAAXt3LkzJSUlISHBzMzM399/4sSJJSUlXC6XWLtmzZpZs2YtXrzYxcUlMTGR+J3u4+ND3D24evVqCwsLNze3uXPntrS0IIS6u7vd3d3ZbLafn5+zs/PVq1fJUemxNqVcgYGBFRUV5MD3N998IxQKq6qqvLy8PvroI+ktvb29iYwpzyHKzs7et28fQsjDw6O6uvrw4cMbN25ECM2ZM4d4lGtGRsb69etTU1NNTU2tra2jo6NbW1sRQj09PQ0NDYWFhUNDZbFYb7755sqVK21sbHg8XlhY2MSJE2/dukVcL8XlmxJ++/ZtGxsbDw+PsR8qzdHEZMfBYP44hWjPnFnVUX8fiZvF1blHXO7vXWVlJY1GG2kWtvr19/f7+flJP3JdiZqamlgs1t69e+XZWHu+C3A+DoBW0NpKe0KhMCEhISEhgby7XYP6+/sLCgo6OjpUVCw6Pj5+2rRpUVFRqmhcdSCPAwBGERsbGxYWJhKJNF4Sq6Sk5NSpU0VFRbKntCsmPT29rKzs/PnzdDpd6Y2rFORxADRs69atOTk5L168mDRp0smTJzUdzvCSk5OjoqI++eQTzYYxe/bsY8eOkdVmlKiwsPDly5clJSXGxsZKb1zVaJoOAIDfu5SUlEE3qmingICAgIAATUehKvPnz58/f76mo1AQnI8DAAC1QR4HAABqgzwOAADUBnkcAACoTYuuc5IPPAXa7NatW0jX/1i/hz6iVzeg63w3VefWrVtkDRzNwnAteHzRzZs309PTNR0F0E1FRUXTp09XxUw1AIhCj5qOQjvyOACqg2HYiRMnFi1apOlAAFAVGB8HAABqgzwOAADUBnkcAACoDfI4AABQG+RxAACgNsjjAABAbZDHAQCA2iCPAwAAtUEeBwAAaoM8DgAA1AZ5HAAAqA3yOAAAUBvkcQAAoDbI4wAAQG2QxwEAgNogjwMAALVBHgcAAGqDPA4AANQGeRwAAKgN8jgAAFAb5HEAAKA2yOMAAEBtkMcBAIDaII8DAAC1QR4HAABqgzwOAADUBnkcAACoDfI4AABQG+RxAACgNsjjAABAbZDHAQCA2iCPAwAAtdE0HQAAStbW1objuPSSrq6u1tZW8qWBgQGdTld7XACoCjboEw8A1f3pT3+6evXqSGv19fUfP35saWmpzpAAUCkYVwG6ZvHixRiGDbtKT0/vj3/8IyRxoGMgjwNds3DhQhpt+AFDDMPee+89NccDgKpBHge6xtjYOCAgQF9ff+gqPT294OBg9YcEgEpBHgc6aOnSpQMDA4MW0mi0wMBAPp+vkZAAUB3I40AHBQUFMZnMQQv7+/uXLl2qkXgAUCnI40AHcTic4ODgQZML2Wz23LlzNRUSAKoDeRzopiVLlvT29pIv6XT6woUL2Wy2BkMCQEUgjwPd9Je//EV6KLy3t3fJkiUajAcA1YE8DnQTnU4XiUQMBoN4aWRkNHv2bM2GBICKQB4HOmvx4sU9PT0IITqdvnTp0pEmlQNAdXBfPtBZAwMDEyZMeP78OULo+vXrb775pqYjAkAl4Hwc6Cw9Pb1ly5YhhKytrX19fTUdDgCqohW/NOvr62/cuKHpKIAOMjMzQwi98cYb+fn5mo4F6CA7OzsfHx9NR4EQrgVOnDih6cMAAABjtnDhQk2nTxzHca04HyfgMFJPBWFhYQghCp3enjx5cuHChWN6C+X6qJi8vLzw8HD43imM+JxoAxgfBzpurEkcAMqBPA4AANQGeRwAAKgN8jgAAFAb5HEAAKA2yOMAAEBtkMcBUI7z58/z+fxvv/1W04GoSnFxcWxs7KlTpxwdHTEMwzCMuF2WFBAQwOPx9PX1J0+efOfOHY0EmZqa6urqymazuVyuq6vr9u3b29vbybUJCQlubm6GhoZMJlMoFH788cednZ3EqjNnzqSmpvb392sk7HGCPA6Acuj2ROydO3dmZmZu3bo1NDS0urpaIBCYmpoePXr03Llz5DaXLl3Kz8+fN29eRUWFp6enRuK8du3aBx98UFdX9/z588TExNTUVOmJp1euXPnwww9ramqamppSUlIyMjLIOeBBQUEsFmv27NltbW0aiXw8II8DoByBgYEvXryYN2+eqnckkUjUXC5m9+7dubm5eXl5PB6PXJiZmamnpxcZGfnixQt1BiMbg8FYu3atubm5gYFBWFjYggULvvvuu6dPnxJrDQwMIiMjTUxMeDzeokWLgoODL1y48OjRI2LtunXrpk6dOnfu3L6+Ps31QBGQxwGgmCNHjjQ0NKhtdw8ePNi+ffuuXbtYLJb0cl9f3+jo6MePH2/atEltwYzq9OnT0nHa2NgghMjBk7Nnz+rr65NrifI7YrGYXBIfH19WVpaRkaGmcJUE8jgASnD9+nV7e3sMww4cOIAQys7O5nK5HA6nsLDwnXfeMTQ0tLW1PX78OLFxZmYmi8WysLBYtWqVtbU1i8Xy9fX94YcfiLVRUVEMBsPKyop4uXbtWi6Xi2FYU1MTQig6Onrjxo1VVVUYhgmFQoTQhQsXDA0Nk5OTVdS1zMxMHMeDgoKGrkpKSnJ2dv7888+Li4uHfS+O4+np6a+99hqTyTQ2Nl6wYMGvv/5KrJJ9iBBC/f39O3bssLe3Z7PZHh4eilVhqqysNDIycnBwGHbt48eP2Wz2pEmTyCXGxsb+/v4ZGRkUGyXTZHGXV4i/kKajAHJZuHChltQGUh3F+kj8PN+/fz/xMi4uDiF0+fLlFy9eNDQ0+Pn5cbncnp4eYm1kZCSXy7137153d3dFRYWXlxePx6urqyPWvvvuu5aWlmTLaWlpCKHGxkbiZWhoqEAgINeePXuWx+MlJCSMNWA5v3eOjo5ubm6DFgoEgocPH+I4fuPGDT09vYkTJ3Z2duI4XlRUNH/+fHKzHTt2MBiMr776qq2t7e7du56enmZmZs+ePSPWyj5EmzZtYjKZJ0+ebG1t3bp1q56e3u3bt+XsWk9PT319/f79+5lM5ldffTXsNl1dXTweLyoqatDy2NhYhFBpaemoe9Ge7wKcjwOgQr6+voaGhubm5iKRqKurq66ujlxFo9GIE1U3N7fs7OyOjo6cnBwFdhEYGNje3r59+3blRf3/dXV1PXz4UCAQjLSBj4/P+vXra2pqtmzZMmiVRCJJT08PCQlZunQpn893d3c/ePBgU1PToUOHpDcb9hB1d3dnZ2cHBweHhoYaGRlt27aNTqfLf3zs7OxsbW3j4+P37NkTHh4+7DYpKSnW1tZJSUmDljs5OSGEysvL5dyXNoA8DoA6EE8K7e3tHXbtjBkzOBwOOeagPRoaGnAc53A4MrZJSkpycXHJysq6fv269PKKiorOzs4ZM2aQS7y8vBgMBjmCNIj0Ibp//75YLJ4yZQqxis1mW1lZyX98Hj161NDQ8PXXX3/xxRfTp08fejnh9OnTeXl5Fy9elL5ySyA6SzxGiiogjwOgFZhMZmNjo6ajGKy7uxshxGQyZWzDYrFycnIwDFuxYoVEIiGXExP4DAwMpDc2MjLq6OgYdb9dXV0IoW3btmGv1NbWSl+QlI1Op5ubmwcEBOTm5lZUVKSkpEivzc3N3b17d0lJycSJE4e+l81mo1cdpwrI4wBoXm9vb1tbm62traYDGYxIaqPeHePj47Nhw4bKysrExERyoZGREUJoUNaWs5vm5uYIoX379kmPAt+8eXOs8QuFQn19/YqKCnLJ/v37jx49euXKlQkTJgz7FuLZ3ETHqQLyOACaV1JSguO4t7c38ZJGo400AqNmFhYWGIbJM0M8MTHR1dW1tLSUXDJlyhQDA4P//ve/5JIffvihp6fnD3/4w6it2dnZsVissrKyMUXb3Ny8ZMkS6SWVlZX9/f12dnYIIRzHY2JiysvLCwoKBv1KkEZ01tLScky71izI4wBoxsDAQGtra19f3927d6Ojo+3t7SMiIohVQqGwpaWloKCgt7e3sbGxtrZW+o0mJiZPnjypqanp6Ojo7e0tKipS3bxDDofj6OhYX18/6pbE6Ir07GwWi7Vx48bTp08fPXq0vb29vLx89erV1tbWkZGR8rT2/vvvHz9+PDs7u729vb+/v76+nridRyQSWVpaDnvfP5fLvXTp0pUrV9rb23t7e0tLS5cvX87lcjds2IAQunfv3p49ew4fPkyn0zEpe/fulW6E6Ky7u/uoQWoPyOMAKMGBAwe8vLwQQjExMfPnz8/Ozt63bx9CyMPDo7q6+vDhwxs3bkQIzZkzp7KyknhLd3e3u7s7m8328/Nzdna+evUqOQy9Zs2aWbNmLV682MXFJTExkfiN7+PjQ0xtXL16tYWFhZub29y5c1taWlTdtcDAwIqKCnLg+5tvvhEKhVVVVV5eXh999JH0lt7e3kTGJO3cuTMlJSUhIcHMzMzf33/ixIklJSVcLhchNOohysjIWL9+fWpqqqmpqbW1dXR0dGtrK0Kop6enoaGhsLBwaKgsFuvNN99cuXKljY0Nj8cLCwubOHHirVu3iOuluHxTwm/fvm1jY+Ph4TH2Q6U5mpjsOBjMH6cQ7Zkzqzpq6CNxd7hKdzEqOb93lZWVNBptpFnY6tff3+/n53fkyBFVNN7U1MRisfbu3SvPxtrzXYDzcQA0gyql9YRCYUJCQkJCAnl3uwb19/cXFBR0dHSIRCJVtB8fHz9t2rSoqChVNK46VM3jK1eu5PF4GIaN9UqI6sgumCmDdCFQAoPBsLCweOutt9LS0ojfklT08uXLdevWWVlZcTicP//5z8QVs4MHDypxFwoXKQVjEhsbGxYWJhKJNF4Sq6Sk5NSpU0VFRbKntCsmPT29rKzs/PnzdDpd6Y2rlqZ/EOC4ouMqRCkGeW6fVY/AwMC9e/c2NDR0dHTk5eXR6fS3335b/rcLBAI+n4/jOHH56+rVqxERERiGWVtby387shrI/1syOTnZ2dm5tbX1H//4R35+PjHo+dlnnykxGNnH3N/fPysrq7m5ub29/cSJE3Q6fc6cOfI0q+rfy7GxscQ9LxMnTszPz1fdjmQb6/fu4sWLMTExqotHswoKClJSUvr6+uR/i/aMq0AeV5rg4GCJREK+JOoaP3nyRM63k3lcWn5+vp6enoWFRVtbm9ICHR/5P7teXl5LliwhXyolj4vFYh8fH/Kl7GMeGBgo/bVctGgRQoisYSKD9nw/VQquS42T9nxOqDqughDCMEzTIfwfsgtmKmbhwoURERENDQ3KHY5Qj/r6eqX/Ph1UsnWcRUoB0A1UyuM4jqelpbm4uDCZTD6fv3nzZum1w1a5HLU25vfff//6669zOBxDQ0N3d3didFUVBTMVLi5KzCkuKirSzm4O67vvvhMKhU+fPv3iiy8wDBv2ngt85IqmCKFr1665ubnx+XwWi+Xu7n7x4kU0XMnWQcZapBQAHaHpHwQ4Lvfvu7i4OAzDPv3009bWVrFYnJWVhaTGVUaqcimjNmZnZ6ehoWFqaqpEInn27FlISAhRGlQVBTNHLS467LgKjuNEzrWzs9OSbsr/W9LS0nL58uXky0HjKrIrmubn58fHx7e0tDQ3N3t7e5uamhLLB5VsJYynSOk4+0hpMK4yTtrzOdGKv6I8nyexWMzhcKSvYkmPj0skEg6HIxKJyI2ZTOaaNWvwVwmOHEUlsv+DBw9wHP/5558RQmfPnpXekYym5EHczmtqavr3v/+drKQsj5HyOI7jGIYZGRlpSTeVksfFYrGBgQG5dxzHf/zxR4TQsP/PEUWOiMJ7w+ZxeY55XFycs7Nze3u7PJFrz/dTpSCPj5P2fE5oaj35H4cHDx6IxeLZs2cPu1b+KpfStTEdHR0tLCyWLl26bt26iIgIovjZ+AtmtrW1lZaWxsbGHjp06MqVKxYWFmPr6v/V1dWF47ihoaFWdXOcxlTRlBhklzHbetRjThQpvXTp0tAipSO5desW+QReXUXcgK7z3VSdW7dukSVxNIsy4+PEZ46ogjaUYlUu2Wz2lStXZs6cmZyc7OjoKBKJJBKJSgtmKuC3335DCLm6uiJt6uY4jVrR9Ny5c2+99Za5uTmTyfz4449ltzaeIqUA6ADKnI8T0xJevnw57FqyymV0dPSYmp08efK3337b2NiYnp6+e/fuyZMnE/eJKdDUIEMLZirmwoULCKF33nkHaWU3FSO7omldXV1wcHBISMg///nPCRMm7N+/f9RUThi2SOnFixevXLkio77dsLy9vfPz88f0FsrJy8sLDw/X+W6qjvb8lKHM+fiUKVP09PS+//77YdcqVuXyyZMn9+7dQwiZm5t/8sknnp6e9+7dU0XBTIU9e/Zs3759tra2K1asQFrQTWWRXdG0vLy8t7d3zZo1jo6OLBZrpAmmSilSCoAOoEweNzc3Dw0NPXny5JEjR9rb2+/evSv9lD8ZVS5lePLkyapVq3799deenp7S0tLa2lpvb2/FmpJdMBMhJE9xURzHOzs7BwYGcBxvbGw8ceLEm2++qa+vX1BQQIyPa7ybyiK7oqm9vT1CqLi4uLu7u7KyUnrQXLpkK4PBGH+RUgB0gWYvsxLkvG7e0dGxcuVKU1NTAwODmTNn7tixAyFka2v7008/4Tj+8uXLmJgYe3t7Go1GJP2KioqsrCyiDoOTk1NVVdWhQ4eIhOjg4PDbb7/V1NT4+voaGxvr6+tPmDAhLi6OuP1v2KZGDS8oKGjSpEkGBgZMJlMgEIhEovLycnLt+fPneTxeUlLS0DeeOXPGw8ODw+EwGAw9PT2EEDFB5fXXX09ISGhubpbeWOPdlOcafU1NzfTp0xFCNBrN09Pz5MmTn376KTGrhMvlhoSE4Dg+MDCQlpbm5OREp9ONjY2Dg4Pv379PthATE2NiYmJkZBQWFnbgwAGEkEAgqKuru3PnjoODA5vNnjlz5rNnz2Qc85Gek5uWlqaUPuoAmK8yTtrzOcFw+WryqhQxTqcNkYBREWOCuj2o+nvoI4Lv3bhpz+eEMuMqAAAAhgV5XC6//vorNjIVlUIGQKsUFxfHxsZKl1letmyZ9AYBAQE8Hk9fX3/y5MnDPndNbQYGBvbt2+fr6zt0VW9vb0pKilAoZDAYRkZGU6ZMqampQQidOXMmNTWVKkXhB4E8LhdXV1cZg1O5ubmaDhAA1dq5c2dmZubWrVtDQ0Orq6sFAoGpqenRo0fPnTtHbnPp0qX8/Px58+ZVVFR4enpqKtTKyso//vGPGzZsGPZ+iPDw8C+//PLYsWNisfiXX34RCAREYbWgoCAWizV79mzi5gZqgTwOgAZIJJJhzxY129RIdu/enZubm5eXJ31DbGZmpp6eXmRkpMYfLiHtp59+2rJly+rVq6dNmzZ0bW5ubkFBQX5+/htvvEGj0aytrQsLC8m7mtetWzd16tS5c+f29fWpN+rxgjwOgAYMKsCrJU0N68GDB9u3b9+1a5d0iWCEkK+vb3R09OPHjzdt2qS6vY/V1KlTT5069e6775IPrZb22WefeXp6uru7j/T2+Pj4srKyjIwMVcaofJDHAVAQPnLp3aioKAaDYWVlRbxcu3Ytl8vFMKypqQkNKcCbmZnJYrEsLCxWrVplbW3NYrF8fX3JWfNjagqNo0LySDIzM3EcDwoKGroqKSnJ2dn5888/Ly4uHushGrXYstLrKvf09Ny6dWvY83SSsbGxv79/RkYGxabxqH5q4+hgHiuFaM+cWdWRs4+yS+++++67lpaW5MZpaWkIIaJiMD6kcGNkZCSXy7137153d3dFRYWXlxePxyMfXTSmpkatkEyS83vn6Ojo5uY2aKFAIHj48CGO4zdu3NDT05s4cWJnZyeO40VFRfPnzyc3k32IZBRbxsdXPhrH8TfeeGPq1KnSSx4+fIgQmjZt2ltvvWVlZcVkMl1dXQ8cOEDceUeKjY1F8j1oTHu+C3A+DoAiJBJJenp6SEjI0qVL+Xy+u7v7wYMHm5qapG8zHhMajUact7q5uWVnZ3d0dOTk5CjQTmBgYHt7+/bt2xULY5Curq6HDx8KBIKRNvDx8Vm/fn1NTc2WLVsGrZLzEPn6+hoaGpqbm4tEoq6urrq6OoRQd3d3dnZ2cHBwaGiokZHRtm3b6HS6YgeERFzPNDc3T05OrqioeP78+YIFCz788MOvv/5aejMnJyeE0Ej3kWknyOMAKGJMpXfHasaMGRwOR21lhGUgyr7Lfjh9UlKSi4tLVlbW9evXpZeP9RBJF1tWRV1lYsR88uTJvr6+JiYmfD5/165dfD5/0P8rRGefP38+nn2pGeRxABQxaundcWIymY2NjUppajy6u7vRqww4EhaLlZOTg2HYihUrJBIJuXw8h0gVdZWtra0RQsR1BQKDwXBwcKiqqpLejM1mo1cdpwrI4wAoQnbp3XHq7e1VVlPjRCS1Ue+O8fHx2bBhQ2VlZWJiIrlwPIeILNEsPQp88+ZNBbpAMjAwcHJyImp/kvr6+vh8vvSSnp4e9KrjVAF5HABFyC69ixCi0WjEEIECSkpKcBwnnzUznqbGycLCAsMweWaIJyYmurq6lpaWkktGPUQyqKiucnh4eGlpaXV1NfFSLBbX1tYOmoZIdJYo60YVkMcBUITs0rsIIaFQ2NLSUlBQ0Nvb29jYWFtbK/126QK8RI4eGBhobW3t6+u7e/dudHS0vb19RESEAk3JUyFZfhwOx9HRkXga16gHJCcnR19fX3qJ7EMku7WR6iqLRCJLS0vF7vvfsGGDg4NDREREXV1dc3NzTEyMRCIZdIWW6KyMOebaSDPTZP4vmHdIIdoz10p15Oyj7NK7zc3Ns2bNYrFYkyZN+uijjzZv3owQEgqFxGzCQQV4IyMj6XS6jY0NjUYzNDRcsGBBVVWVYk3JqJA8iJzfu6ioKDqdLhaLiZenT58mpq+YmZl9+OGHgzbevHmz9LxDGYdIdrFlfOS6ysHBwQihHTt2DBvtzZs333zzTWIoHCFkZWXl6+v7/fffkxs8evRo8eLFxsbGTCbz9ddfLyoqGtRCYGCgjY3NoMmIw9Ke74JWZE/I4xSiPZ9d1VF/HyMjI01MTNS5R1zu711lZSWNRvvqq6/UEJI8+vv7/fz8jhw5oorGm5qaWCzW3r175dlYe74LMK4CgFbQ2kp7QqEwISEhISGBmH+tWf39/QUFBR0dHSoqMhofHz9t2rSoqChVNK46kMcBAKOIjY0NCwsTiUQaL4lVUlJy6tSpoqIi2VPaFZOenl5WVnb+/Hk6na70xlUK8jgAGrZ169acnJwXL15MmjTp5MmTmg5neMnJyVFRUZ988olmw5g9e/axY8fIajNKVFhY+PLly5KSEmNjY6U3rmo0TQcAwO9dSkpKSkqKpqMYXUBAQEBAgKajUJX58+fPnz9f01EoCM7HAQCA2iCPAwAAtUEeBwAAaoM8DgAA1AZ5HAAAqE2L5qtgGKbpEIC8fg9/rN9DH9HvppsqsnDhQk2HgBBCGK4Fj6Grr6+/ceOGpqMAuik8PDw6OtrHx0fTgQAdZGdnpw0fLa3I4wCoDoZhJ06cWLRokaYDAUBVYHwcAACoDfI4AABQG+RxAACgNsjjAABAbZDHAQCA2iCPAwAAtUEeBwAAaoM8DgAA1AZ5HAAAqA3yOAAAUBvkcQAAoDbI4wAAQG2QxwEAgNogjwMAALVBHgcAAGqDPA4AANQGeRwAAKgN8jgAAFAb5HEAAKA2yOMAAEBtkMcBAIDaII8DAAC1QR4HAABqgzwOAADUBnkcAACoDfI4AABQG+RxAACgNsjjAABAbZDHAQCA2iCPAwAAtUEeBwAAaoM8DgAA1EbTdAAAKNnx48c7OjqklxQXF7e1tZEvg4ODzc3N1R4XAKqC4Tiu6RgAUKaIiIgvvviCTqcTL4lPOIZhCKH+/n4DA4OGhgYmk6nJEAFQKhhXAbpm8eLFCKHeV/r6+vr6+oh/6+vrh4WFQRIHOgbOx4Gu6evrs7S0bGlpGXbt5cuX//SnP6k5JABUCs7Hga6h0WiLFy8mx1WkmZmZ+fv7qz8kAFQK8jjQQYsXL+7t7R208P+1d+dhTV1pA8DPhewQAiIgiiAQloKoZdBCKmN9mPK08ogCommLI+PTFreyqZVFFNmUYoGBQh1bhnbUKQI6waqopRRneMTWqaAURwsomxuL7An7/f64YyZfgCRkIQl9f//l3pOT91zIy+Xcc99LJpO3bt2qq6urlpAAUB2YVwFzEI7jlpaWbW1tYtt/+umnlStXqiUkAFQHzsfBHIRhWFBQkNjUyuLFi93c3NQVEgCqA3kczE1iUytkMjk4OJhYfQjAHAPzKmDOcnR0fPDggfDlL7/84uzsrMZ4AFAROB8Hc9bWrVuFUytOTk6QxMFcBXkczFlBQUFjY2MIITKZvG3bNnWHA4CqwLwKmMvc3Nx+/vlnDMOamposLS3VHQ4AKgHn42Au++Mf/4gQeu211yCJgzlMI+odVlVVpaenqzsKMAcNDQ1hGDY8PBwYGKjuWMAc5OHhERkZqe4oNON8vLW1tbi4WN1RAJncvHnz5s2b6o5CVjQazczMzMLCYkbv0q4xyq2trQ2+d4q4efNmVVWVuqNASEPOxwlFRUXqDgFIR5zYatEPq6Ghgc1mz+gtWjdG+RQWFm7ZsmXOD1N1NOefPI04HwdAdWaaxAHQOpDHAQBAu0EeBwAA7QZ5HAAAtBvkcQAA0G6QxwFQMBo3qAAAFu5JREFUjsuXL7NYrG+//VbdgahKWVlZdHT0uXPnbGxsMAzDMGzr1q2iDby9vZlMpq6urrOz8+3bt9UVJ0JoYmIiIyODw+FM3jU6OpqSksJmsykUiqGh4dKlS5uamhBCFy5cSE1NHR8fn+1YlQHyOADKMbdLXBw+fDgrKysmJiYgIODhw4e2trbGxsanT5++dOmSsM21a9eKiorWr19fV1fn6uqqrlDr6+t///vfR0ZG8vn8yXu3bNnyt7/97cyZM3w+/z//+Y+tre3AwABCyNfXl0ajeXl59fT0zHrIioI8DoBy+Pj49Pb2rl+/XtUfJBAIpjzTVJ1jx44VFBQUFhYymUzhxqysLB0dnZCQkN7e3tkMRrI7d+5ERUXt3LlzxYoVk/cWFBTweLyioqLXXnuNRCKZm5uXlJQsXbqU2BsWFrZ8+fJ169YR5dW0CORxALRMXl5ee3v7rH1cQ0NDXFzckSNHaDSa6HYOhxMeHv748eN9+/bNWjBSLV++/Ny5c++99x6VSp289/PPP3d1dXVxcZnu7fHx8TU1NZmZmaqMUfkgjwOgBJWVlZaWlhiGffbZZwih3NxcPT09BoNRUlLy9ttvGxgYWFhYfPPNN0TjrKwsGo1mamq6Y8cOc3NzGo3G4XB+/PFHYm9oaCiFQlmwYAHxcvfu3Xp6ehiGdXZ2IoTCw8P37t3b2NiIYRhxi9OVK1cMDAySk5NVNLSsrCwcx319fSfvSkpKsre3//LLL8vKyqZ8L47j6enpr7zyCpVKNTIy2rhx4/3794ldkg8RQmh8fPzQoUOWlpZ0On3ZsmVnz55VcCAjIyM3b96c8jxdyMjIaM2aNZmZmdo1SwZ5HAAlWL169Y0bN4Qvd+3aFRERIRAImEzm2bNnGxsbbWxsPvjgA+JRc6GhocHBwXw+PywsrKmp6fbt22NjY2+++WZraytCKCsra/PmzcKucnJyjhw5InyZmZm5fv16W1tbHMcbGhoQQsSluYmJCRUN7dKlSw4ODgwGY/IuOp3+1Vdf6ejofPDBB4ODg5MbxMfHR0dHx8bGtre3//Of/2xtbfX09Hz+/DmSdogQQlFRUZ988klGRsbTp0/Xr1//7rvv/vvf/1ZkIE+ePBkZGfn555/Xrl1L/Pl85ZVXcnJyxFL2q6+++vjx4zt37ijyWbMM8jgAKsThcAwMDExMTLhc7uDgYEtLi3AXiUQiTlSdnJxyc3P7+/vz8/Pl+AgfH5++vr64uDjlRf0/g4ODjx49srW1na6Bh4dHREREU1NTVFSU2C6BQJCenu7v7x8UFMRisVxcXE6cONHZ2Xny5EnRZlMeoqGhodzcXD8/v4CAAENDw4MHD5LJZPmOjxBxPdPExCQ5Obmuru758+cbN27cs2fP3//+d9FmdnZ2CKHa2lpFPmuWQR4HYDZQKBSEkOijn0W5ubkxGAzhnIPmaG9vx3F8ypNxoaSkJAcHh5ycnMrKStHtdXV1AwMDbm5uwi0rV66kUCjCGSQxoofowYMHfD5feAWSTqcvWLBAweNDzJg7OztzOJx58+axWKwjR46wWCyxvyvEYIl/GrQF5HEANAKVSu3o6FB3FOKGhobQyww4HRqNlp+fj2HY9u3bBQKBcDuxgE9fX1+0saGhYX9/v9TPJWZpDh48iL3U3Nw85TpC2ZmbmyOEiMsMBAqFYmVl1djYKNqMTqejlwPXFpDHAVC/0dHRnp6emdZJnwVEUpN6dwzxOIX6+vrExEThRkNDQ4SQWNaWcZgmJiYIoYyMDFyEgsW+9fX17ezs7t27J7pxbGyMxWKJbhkZGUEvB64tII8DoH4VFRU4jru7uxMvSSTSdDMws8zU1BTDMFlWiCcmJjo6OlZXVwu3LF26VF9fX/Ti5I8//jgyMvK73/1Oam+LFy+m0Wg1NTXyhT2dLVu2VFdXP3z4kHjJ5/Obm5vFliESgzUzM1PuR6sU5HEA1GNiYqK7u3tsbOzu3bvh4eGWlpbBwcHELjab/eLFCx6PNzo62tHR0dzcLPrGefPmPXnypKmpqb+/f3R0tLS0VHXrDhkMho2NTVtbm9SWxOyKrq6u6Ja9e/eeP3/+9OnTfX19tbW1O3fuNDc3DwkJkaW3P/3pT998801ubm5fX9/4+HhbW9vTp08RQlwu18zMTL77/iMjI62srIKDg1taWrq6ug4cOCAQCMSu0BKDlbDGXBPhGoBYGaruKIBMNm3atGnTJnVHoVpyjDE7O5tY8c1gMHx9fXNycojLZXZ2do2NjSdPnjQwMEAIWVlZ/frrrziOh4SEkMnkRYsWkUgkAwODjRs3NjY2Cnvr6upau3YtjUaztrb+6KOP9u/fjxBis9ktLS04jt++fdvKyopOp69evfrZs2eXL19mMplJSUkzHaaM37vQ0FAymczn84mX58+fJ5avzJ8/f8+ePWKN9+/fv2HDBuHLiYmJtLQ0Ozs7MplsZGTk5+f34MEDYpfUQzQ8PHzgwAFLS0sSiWRiYhIQEFBXV4fjuJ+fH0Lo0KFDU0ZbVVX1+uuvE1PhCKEFCxZwOJzr168LG7S2tr7zzjtGRkZUKnXVqlWlpaViPfj4+CxatGhiYkLqkdGc74JGZE/I41pEc353VWcWxhgSEjJv3jyVfoRUMn7v6uvrSSTSqVOnZiEkWYyPj3t6eubl5ami887OThqNdvz4cVkaa853AeZVAFAPbSmtx2azExISEhISiPXX6jU+Ps7j8fr7+7lcrir6j4+PX7FiRWhoqCo6Vx3I4wAAKaKjowMDA7lcrtpLYlVUVJw7d660tFTyknb5pKen19TUXL58mUwmK71zldLWPP7+++8zmUwMw5R+RVsphoaGHB0dDx48KEtj0YLOBAqFYmpq+sYbb6SlpXV3d6s6WhUZHh4OCwtbsGABg8H4wx/+QKx8OHHihBI/IjU11dHRkU6n6+npOTo6xsXF9fX1CfcmJCQ4OTkZGBhQqVQ2m/3xxx9rwhklQigmJiY/P7+3t9fa2rq4uFjd4cgkOTk5NDT06NGj6g3Dy8vrzJkzwuIzSlRSUjI8PFxRUWFkZKT0zlVO3RM7OC7v/DhRUqe6uloVISkoMjISIRQbGyv7W2xtbVksFo7jxDKGH374ITg4GMMwc3PzW7duqSzSGZN9TjA5Odne3r67u/svf/lLUVFRfX09Qujzzz9XYjA+Pj7Hjx9vb2/v7+8vLCwkk8lvvvmmcO+aNWtycnK6urr6+vrOnj1LJpPfeustWbrVnHlPlYLrUgrSnN8TbT0f12Q3btz45Zdf5H47hmGGhoZvvPFGfn5+YWHh8+fPicLWSoxwdvB4PDc3N0NDww8//HDTpk1K6VOs9DaFQtm9e7eJiYm+vn5gYODGjRu/++47YnUaQkhfX5+4nMhkMjdv3uzn53flyhWiFhUAc4kW53EMw9QdwhQEAsH+/fuVVb9406ZNwcHB7e3typ2OmB1tbW1Kn2cUK719/vx50aLYixYtQi/LISGELl68KLqcef78+QghBe/tBkADaVMex3E8LS3NwcGBSqWyWCxiUa3QlNWKpdY4vn79+qpVqxgMhoGBgYuLCzG7qkjh49jYWOIMUWy73EWiiXtDSktLNWqYkn333XdsNvvp06dff/01hmFiFTYI+PSVqRFC//rXv5ycnFgsFo1Gc3FxuXr1Kpqq9LaY+vp6Q0NDKyurKaN6/PgxnU63trZW0igB0BjqntjBcZnn6WJjYzEM+/TTT7u7u/l8fk5ODhKZH9+3bx+VSi0uLu7u7o6JidHR0SGmlWNjYxFC33//fW9vb3t7u6enp56e3sjICI7jAwMDBgYGqampAoHg2bNn/v7+HR0dErqSqrKy0tfXF8dxouCR6Pz4xYsXmUxmQkLCdO8Vzo+LIXLu4sWLNWSYss8JmpmZbdu2TfhSbH780KFDFArl1KlTPT09d+/edXV1nT9//rNnz4i9RUVF8fHxL1686Orqcnd3NzY2JrYHBAQQpbdFjYyMtLW1ZWdnU6nU6ZY5Dw4OMpnM0NBQWSLXnHlPlYL5cQVpzu+JRvwUZfl94vP5DAZD9CqW6HVOgUDAYDC4XK6wMZVK3bVrF/4ywQkEAmIXkf0bGhpwHCdmsS9evCj6QRK6khqhm5tbW1sbPlUel2q6PI7jODFjriHDVEoe5/P5+vr6wk/Hcfynn35CCE35dy4lJQW9LKA6ZR4nSmEYGxv/+c9/Jv50TRYbG2tvb9/X1ydL5Jrz/VQpyOMK0pzfE9JsnvsroqGhgc/ne3l5TblX9mrFojWObWxsTE1Ng4KCwsLCgoODlyxZMqOuxMTExHz44YfEFK0SDQ4O4jhO3LKsCcNUihlVpiYm2SXcNdPa2trT01NdXR0dHX3y5Mny8nJTU1PRBufPny8sLLx27ZroY4IlKy4u1swLMEr3GxmmiijrAr6CtCaPE8VrJs87E4TVikWXbAtrLEyHTqeXl5dHRUUlJycnJCRs3rw5Pz9fvq4qKytra2vT09NlG80M/PrrrwghR0dHpAHDVBaplakvXbqUlpZWV1fX19cntfIfmUw2MTHx9va2tra2t7dPSUkRvc5cUFCQnp5eUVGxcOFC2SN0d3ePiIiQvb02qqqqyszMVOJ1kd+ajIwMdYfwX1qTx4llCcPDw1PuFVYrDg8Pn1G3zs7O3377bUdHR3p6+rFjx5ydnYn7fWfaVV5e3vfff6+j8/+uGycnJycnJ9+6dUv0xHOmrly5ghB6++23kQYMU1kkV6ZuaWnx8/Pz9/f/61//unDhwuzs7I8//liWbtlstq6ubl1dnXBLdnb21atXy8vLp7zWKoGFhYXoQzLnqszMzN/CMFWkqKhI3SH8l9asV1m6dKmOjs7169en3CtfteInT54QReVNTEyOHj3q6up67949+brKz88Xna4SnR9XJIk/e/YsIyPDwsJi+/btSAOGqSySK1PX1taOjo7u2rXLxsaGRqNN949/V1fXu+++K7qlvr5+fHx88eLFCCEcxw8cOFBbW8vj8WaaxAHQLlqTx4nClcXFxXl5eX19fXfv3hV9qp6EasUSPHnyZMeOHffv3x8ZGamurm5ubnZ3d5evK6lkKRKN4/jAwABRMLOjo+Ps2bOvv/66rq4uj8cj5sc1f5gyklyZ2tLSEiFUVlY2NDRUX18vOmkuWnqbQqFcu3atvLycmHuprq7etm2bnp4ecTPtvXv3Pvnkky+++IJMJovWPDh+/PjsjBGA2TP7l1Ynk/G6eX9///vvv29sbKyvr7969epDhw4hhCwsLO7cuYNPU61Yco3jpqYmDodjZGSkq6u7cOHC2NjYsbGx6bqa0Ygmr1eRUCT6woULy5YtYzAYFAqFmJkhFqisWrUqISGhq6tLtLHahynLNfqmpqZXX30VIUQikVxdXYuLiz/99FNiVYmenp6/vz8usTI1juMHDhyYN2+eoaFhYGDgZ599hhCytbVtaWkRK73t6+trbW2tr69PpVJtbW25XG5tbS3Rw3TPO09LS1PKGOcAWK+iIM35PcFwHJ+FvxaSFRYWbtmyRRMiAVIFBgYiTZoZVIXfwhgRfO8Upjm/J1ozrwIAAGBKkMdlcv/+fWx6KippD4AmKysri46OFq26vHXrVtEG3t7eTCZTV1fX2dlZvsdpKk5C7eILFy6kpqZqy9M8JNOadYfq5ejoCP9+AiB0+PDh6urqM2fOMJnMgIAANpvd09Nz+vRpLpfr4+NDtLl27dqVK1dOnDjB4/HUFWd5efmePXu4XC6ZTC4tLQ0KCqqtrSWqFfn6+j569MjLy4vH4xELYbUXnI8DoAZiBXg1pCsZHTt2rKCgoLCwUPT+2KysLB0dnZCQEI2qsSy5dnFYWNjy5cvXrVs3Njam3jgVBHkcADUQK8CrIV3JoqGhIS4u7siRI6IVgxFCHA4nPDz88ePH+/btm7VgpJJauzg+Pr6mpkZZhabVBfI4AHLCpy+9GxoaSqFQhI8f2717t56eHoZhnZ2daFIB3qysLBqNZmpqumPHDnNzcxqNxuFwhKvmZ9QVUqBCsoyysrJwHPf19Z28Kykpyd7e/ssvvywrK5vyvRKOmNTay0opszy5drGRkdGaNWsyMzO1e+JUfUse/wfWsWoRzVkzqzoyjlFy6d333nvPzMxM2DgtLQ0hRFQMxicVbgwJCdHT07t3797Q0FBdXd3KlSuZTGZLS4scXUmtkCwk3/fOxsbGyclJbKOtre2jR49wHL9x44aOjs6SJUsGBgZwHC8tLd2wYYOwmeQjJqH2Mq5ANWmh6WoXR0dHI7meEKk53wU4HwdAHgKBID093d/fPygoiMViubi4nDhxorOzU/Q24xkhkUjEiaqTk1Nubm5/f39+fr4c/fj4+PT19cXFxckXhmSDg4OPHj2ytbWdroGHh0dERERTU1NUVJTYLhmPGIfDMTAwMDEx4XK5g4ODLS0tCKGhoaHc3Fw/P7+AgABDQ8ODBw+SyeSZHp+UlBRzc/OkpCSx7XZ2dgih6W4c0wqQxwGQx4xK786Um5sbg8GYtTLCsiOqwBM3D08nKSnJwcEhJyensrJSdPtMj5ho7WXFyywTtYuvXr06uXYxMZznz5/L3pumgTwOgDyklt5VEJVKJao7aJShoSGEEJVKldCGRqPl5+djGLZ9+3aBQCDcrsgRE5ZZFt600dzcLPujVgsKCo4dO1ZRUUEU3xdDp9PRy6FpKcjjAMhDculdBY2OjiqrK+UiUp7Ue2c8PDwiIyPr6+sTExOFGxU5YsKKzaKTwlVVVbLEnJ2dffr06fLy8ukK0I+MjKCXQ9NSkMcBkIfk0rsIIRKJJPUJGNOpqKjAcdzd3V3xrpTL1NQUwzBZVognJiY6OjpWV1cLt0g9YhLIV2YZl612MTEcoo6bloI8DoA8JJfeRQix2ewXL17weLzR0dGOjo7m5mbRt4sW4CVy9MTERHd399jY2N27d8PDwy0tLYODg+XoSpYKyXJjMBg2NjbEw7kkI2ZXRNduSz1iknubrswyl8s1MzOb8r5/GWsXE8NxcXGRGobGgjwOgJwOHz6ckpKSkJAwf/78NWvWLFmypKKiQk9Pj9i7a9eutWvXvvPOOw4ODomJicS/7R4eHsTNhDt37jQ1NXVyclq3bt2LFy8QQkNDQy4uLnQ63dPT097e/ocffhBOQ8+0K5Xy8fGpq6sTTnz/4x//YLPZjY2NK1eu/Oijj0Rburu7E7XghSQcsdzcXOIxacuWLXv48OEXX3yxd+9ehNBbb71FPKE7MzMzIiIiNTXV2NjY3Nw8PDy8u7sbITQyMtLe3l5SUjI5VFy2JeG3bt1atGjRsmXL5DgamkIdix3FwfpxLaI5a2ZVZ/bHSNw7PpufiMv7vauvryeRSKdOnVJFSHIYHx/39PTMy8uT7+2dnZ00Gu348eNyvFdzvgtwPg6ARtCWwntsNjshISEhIUFYOFCNxsfHeTxef3+/3DVH4+PjV6xYERoaqtzAZhnkcQDAzERHRwcGBnK5XLWXxKqoqDh37lxpaankJe3TSU9Pr6mpuXz5MplMVnpsswnyOABqFhMTk5+f39vba21tXVxcrO5wZJKcnBwaGnr06FH1huHl5XXmzBlh8ZkZKSkpGR4erqioMDIyUnpgswzqjwOgZikpKSkpKeqOYsa8vb29vb3VHYX8NmzYsGHDBnVHoRxwPg4AANoN8jgAAGg3yOMAAKDdII8DAIB206DrnIWFheoOAUhH3MQ8t39Yv4UxIoSIOlNzfpiq09bWpim1zNR9IxKOv7yvDAAAtIuG3M+J4Vr9VDoAAPjNg/lxAADQbpDHAQBAu0EeBwAA7QZ5HAAAtNv/AewpmNfhLYkxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Vb8b-qSIQ4",
        "outputId": "572d47ab-7223-41d8-b916-1b437005f0ec"
      },
      "source": [
        "#Check Trainable Parameters\n",
        "# Note: All the models are similarly structured\n",
        "\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 132)               12408     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8512      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 23,562\n",
            "Trainable params: 23,562\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyx2tehuTPfe"
      },
      "source": [
        "#Set checkpoints, metrics, loss, and optimizer functions for the model\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model1.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model2.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model3.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model4.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model5.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "\n",
        "model6.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model7.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model8.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model9.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model10.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "\n",
        "model11.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model12.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model13.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model14.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model15.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "\n",
        "model16.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model17.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model18.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model19.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model20.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "\n",
        "model21.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model22.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model23.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model24.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model25.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "\n",
        "model26.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model27.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model28.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model29.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])\n",
        "model30.compile(optimizer='adam',loss='mse',metrics=['acc', precision, recall, f1_metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhwA7DthFS72",
        "outputId": "241f5962-4c32-42bc-cc8d-032e322508dc"
      },
      "source": [
        "#Run the model\n",
        "\n",
        "print(\"Model 1 Fitting\")\n",
        "history1 = model1.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 2 Fitting\")\n",
        "history2 = model2.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 3 Fitting\")\n",
        "history3 = model3.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 4 Fitting\")\n",
        "history4 = model4.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 5 Fitting\")\n",
        "history5 = model5.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "\n",
        "print(\"Model 6 Fitting\")\n",
        "history6 = model6.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 7 Fitting\")\n",
        "history7 = model7.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 8 Fitting\")\n",
        "history8 = model8.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 9 Fitting\")\n",
        "history9 = model9.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 10 Fitting\")\n",
        "history10 = model10.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "\n",
        "print(\"Model 11 Fitting\")\n",
        "history11 = model11.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 12 Fitting\")\n",
        "history12 = model12.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 13 Fitting\")\n",
        "history13 = model13.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 14 Fitting\")\n",
        "history14 = model14.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 15 Fitting\")\n",
        "history15 = model15.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "\n",
        "print(\"Model 16 Fitting\")\n",
        "history16 = model16.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 17 Fitting\")\n",
        "history17 = model17.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 18 Fitting\")\n",
        "history18 = model18.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 19 Fitting\")\n",
        "history19 = model19.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 20 Fitting\")\n",
        "history20 = model20.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "\n",
        "print(\"Model 21 Fitting\")\n",
        "history21 = model21.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 22 Fitting\")\n",
        "history22 = model22.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 23 Fitting\")\n",
        "history23 = model23.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 24 Fitting\")\n",
        "history24 = model24.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 25 Fitting\")\n",
        "history25 = model25.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "\n",
        "print(\"Model 26 Fitting\")\n",
        "history26 = model26.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 27 Fitting\")\n",
        "history27 = model27.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 28 Fitting\")\n",
        "history28 = model28.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 29 Fitting\")\n",
        "history29 = model29.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])\n",
        "print(\"Model 30 Fitting\")\n",
        "history30 = model30.fit(features_train, labels_train, epochs=50, validation_data=(features_validation, labels_validation), callbacks=[tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model 1 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.8086 - acc: 0.4972 - precision: 0.1191 - recall: 1.2342 - f1_metric: 0.2132 - val_loss: 0.9305 - val_acc: 0.8874 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8510 - acc: 0.5071 - precision: 0.1203 - recall: 1.2802 - f1_metric: 0.2156 - val_loss: 0.9305 - val_acc: 0.8917 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6803 - acc: 0.4952 - precision: 0.1145 - recall: 1.2292 - f1_metric: 0.2056 - val_loss: 0.9305 - val_acc: 0.8140 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7327 - acc: 0.4776 - precision: 0.1174 - recall: 1.2383 - f1_metric: 0.2103 - val_loss: 0.9305 - val_acc: 0.5991 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8586 - acc: 0.4735 - precision: 0.1223 - recall: 1.2644 - f1_metric: 0.2192 - val_loss: 0.9305 - val_acc: 0.8654 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9148 - acc: 0.4940 - precision: 0.1122 - recall: 1.2628 - f1_metric: 0.2020 - val_loss: 0.9305 - val_acc: 0.8152 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7270 - acc: 0.4755 - precision: 0.1174 - recall: 1.2800 - f1_metric: 0.2113 - val_loss: 0.9305 - val_acc: 0.7044 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0289 - acc: 0.4790 - precision: 0.1178 - recall: 1.2206 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.8470 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0851 - acc: 0.4886 - precision: 0.1244 - recall: 1.2398 - f1_metric: 0.2214 - val_loss: 0.9305 - val_acc: 0.9002 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1298 - acc: 0.4964 - precision: 0.1263 - recall: 1.2948 - f1_metric: 0.2259 - val_loss: 0.9305 - val_acc: 0.8800 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6936 - acc: 0.4949 - precision: 0.1256 - recall: 1.2217 - f1_metric: 0.2233 - val_loss: 0.9305 - val_acc: 0.8862 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8739 - acc: 0.5209 - precision: 0.1183 - recall: 1.1892 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.8213 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8974 - acc: 0.4942 - precision: 0.1187 - recall: 1.2162 - f1_metric: 0.2117 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7713 - acc: 0.4970 - precision: 0.1250 - recall: 1.2753 - f1_metric: 0.2234 - val_loss: 0.9305 - val_acc: 0.8703 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8984 - acc: 0.5014 - precision: 0.1188 - recall: 1.2541 - f1_metric: 0.2125 - val_loss: 0.9305 - val_acc: 0.8843 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7965 - acc: 0.4804 - precision: 0.1181 - recall: 1.2342 - f1_metric: 0.2108 - val_loss: 0.9305 - val_acc: 0.8262 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9988 - acc: 0.4810 - precision: 0.1114 - recall: 1.2188 - f1_metric: 0.1995 - val_loss: 0.9305 - val_acc: 0.0820 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7134 - acc: 0.4646 - precision: 0.1165 - recall: 1.2911 - f1_metric: 0.2100 - val_loss: 0.9305 - val_acc: 0.8733 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2891 - acc: 0.4741 - precision: 0.1210 - recall: 1.2519 - f1_metric: 0.2166 - val_loss: 0.9305 - val_acc: 0.8752 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6550 - acc: 0.4936 - precision: 0.1101 - recall: 1.2426 - f1_metric: 0.1982 - val_loss: 0.9305 - val_acc: 0.8446 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1481 - acc: 0.4860 - precision: 0.1193 - recall: 1.2533 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.8807 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7256 - acc: 0.4912 - precision: 0.1207 - recall: 1.2187 - f1_metric: 0.2156 - val_loss: 0.9305 - val_acc: 0.8354 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1814 - acc: 0.4762 - precision: 0.1268 - recall: 1.2358 - f1_metric: 0.2262 - val_loss: 0.9305 - val_acc: 0.8237 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6338 - acc: 0.4834 - precision: 0.1291 - recall: 1.2175 - f1_metric: 0.2293 - val_loss: 0.9305 - val_acc: 0.8323 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7635 - acc: 0.4646 - precision: 0.1142 - recall: 1.2538 - f1_metric: 0.2052 - val_loss: 0.9305 - val_acc: 0.8213 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7550 - acc: 0.4975 - precision: 0.1190 - recall: 1.2811 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.8390 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0293 - acc: 0.4988 - precision: 0.1203 - recall: 1.2403 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.8146 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6962 - acc: 0.4818 - precision: 0.1146 - recall: 1.1808 - f1_metric: 0.2046 - val_loss: 0.9305 - val_acc: 0.7968 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8395 - acc: 0.5100 - precision: 0.1141 - recall: 1.2297 - f1_metric: 0.2048 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7528 - acc: 0.4978 - precision: 0.1212 - recall: 1.2726 - f1_metric: 0.2163 - val_loss: 0.9305 - val_acc: 0.8745 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8942 - acc: 0.4879 - precision: 0.1168 - recall: 1.2717 - f1_metric: 0.2099 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7714 - acc: 0.5074 - precision: 0.1128 - recall: 1.2247 - f1_metric: 0.2036 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9333 - acc: 0.4792 - precision: 0.1282 - recall: 1.2701 - f1_metric: 0.2280 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9057 - acc: 0.4955 - precision: 0.1168 - recall: 1.2259 - f1_metric: 0.2095 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.4304 - acc: 0.5152 - precision: 0.1120 - recall: 1.2300 - f1_metric: 0.2015 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6887 - acc: 0.4787 - precision: 0.1164 - recall: 1.2322 - f1_metric: 0.2097 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9967 - acc: 0.4821 - precision: 0.1189 - recall: 1.2177 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7625 - acc: 0.4751 - precision: 0.1169 - recall: 1.2349 - f1_metric: 0.2086 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.4985 - acc: 0.5091 - precision: 0.1140 - recall: 1.2051 - f1_metric: 0.2048 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0885 - acc: 0.4877 - precision: 0.1286 - recall: 1.2390 - f1_metric: 0.2280 - val_loss: 0.9305 - val_acc: 0.8262 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7520 - acc: 0.4851 - precision: 0.1220 - recall: 1.2384 - f1_metric: 0.2183 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5850 - acc: 0.4969 - precision: 0.1146 - recall: 1.2528 - f1_metric: 0.2056 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6573 - acc: 0.4933 - precision: 0.1208 - recall: 1.2057 - f1_metric: 0.2156 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9340 - acc: 0.4738 - precision: 0.1150 - recall: 1.2434 - f1_metric: 0.2059 - val_loss: 0.9305 - val_acc: 0.9009 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8763 - acc: 0.5003 - precision: 0.1194 - recall: 1.2585 - f1_metric: 0.2142 - val_loss: 0.9305 - val_acc: 0.8990 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9573 - acc: 0.5096 - precision: 0.1160 - recall: 1.1723 - f1_metric: 0.2068 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8660 - acc: 0.4830 - precision: 0.1198 - recall: 1.2593 - f1_metric: 0.2139 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6083 - acc: 0.5009 - precision: 0.1161 - recall: 1.2021 - f1_metric: 0.2082 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8979 - acc: 0.5016 - precision: 0.1176 - recall: 1.2793 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8515 - acc: 0.4932 - precision: 0.1151 - recall: 1.2124 - f1_metric: 0.2063 - val_loss: 0.9305 - val_acc: 0.8929 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 2 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 4ms/step - loss: 0.7759 - acc: 0.4696 - precision: 0.1183 - recall: 1.2442 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.2576 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7641 - acc: 0.4651 - precision: 0.1160 - recall: 1.2001 - f1_metric: 0.2075 - val_loss: 0.9305 - val_acc: 0.0875 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0837 - acc: 0.4750 - precision: 0.1150 - recall: 1.2049 - f1_metric: 0.2057 - val_loss: 0.9305 - val_acc: 0.1591 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6985 - acc: 0.4499 - precision: 0.1178 - recall: 1.2547 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.0734 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1221 - acc: 0.4522 - precision: 0.1156 - recall: 1.2478 - f1_metric: 0.2073 - val_loss: 0.9305 - val_acc: 0.2589 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7061 - acc: 0.4494 - precision: 0.1167 - recall: 1.2411 - f1_metric: 0.2089 - val_loss: 0.9305 - val_acc: 0.8182 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0891 - acc: 0.4750 - precision: 0.1220 - recall: 1.2000 - f1_metric: 0.2177 - val_loss: 0.9305 - val_acc: 0.8629 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7243 - acc: 0.4753 - precision: 0.1235 - recall: 1.3082 - f1_metric: 0.2210 - val_loss: 0.9305 - val_acc: 0.8305 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7606 - acc: 0.4723 - precision: 0.1158 - recall: 1.1943 - f1_metric: 0.2071 - val_loss: 0.9305 - val_acc: 0.4786 - val_precision: 0.1313 - val_recall: 1.2943 - val_f1_metric: 0.2310\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8124 - acc: 0.4797 - precision: 0.1133 - recall: 1.1839 - f1_metric: 0.2029 - val_loss: 0.9305 - val_acc: 0.1028 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9029 - acc: 0.4611 - precision: 0.1218 - recall: 1.1765 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.8690 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7886 - acc: 0.4876 - precision: 0.1166 - recall: 1.2197 - f1_metric: 0.2084 - val_loss: 0.9305 - val_acc: 0.8715 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7769 - acc: 0.4916 - precision: 0.1273 - recall: 1.2571 - f1_metric: 0.2265 - val_loss: 0.9305 - val_acc: 0.8874 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7751 - acc: 0.4876 - precision: 0.1127 - recall: 1.2543 - f1_metric: 0.2029 - val_loss: 0.9305 - val_acc: 0.4914 - val_precision: 0.1315 - val_recall: 1.2943 - val_f1_metric: 0.2313\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6150 - acc: 0.4832 - precision: 0.1159 - recall: 1.1655 - f1_metric: 0.2069 - val_loss: 0.9305 - val_acc: 0.6273 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0685 - acc: 0.4660 - precision: 0.1185 - recall: 1.2321 - f1_metric: 0.2125 - val_loss: 0.9305 - val_acc: 0.1677 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7746 - acc: 0.4963 - precision: 0.1170 - recall: 1.2593 - f1_metric: 0.2100 - val_loss: 0.9305 - val_acc: 0.8727 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8553 - acc: 0.4994 - precision: 0.1149 - recall: 1.2201 - f1_metric: 0.2067 - val_loss: 0.9305 - val_acc: 0.8929 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5622 - acc: 0.4863 - precision: 0.1181 - recall: 1.2154 - f1_metric: 0.2112 - val_loss: 0.9305 - val_acc: 0.2882 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7651 - acc: 0.4945 - precision: 0.1247 - recall: 1.2349 - f1_metric: 0.2228 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8885 - acc: 0.4875 - precision: 0.1162 - recall: 1.2711 - f1_metric: 0.2093 - val_loss: 0.9305 - val_acc: 0.8684 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6681 - acc: 0.4803 - precision: 0.1197 - recall: 1.2791 - f1_metric: 0.2141 - val_loss: 0.9305 - val_acc: 0.8623 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1372 - acc: 0.4678 - precision: 0.1284 - recall: 1.3042 - f1_metric: 0.2277 - val_loss: 0.9305 - val_acc: 0.8341 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9275 - acc: 0.4680 - precision: 0.1209 - recall: 1.2640 - f1_metric: 0.2171 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2857 - acc: 0.4828 - precision: 0.1216 - recall: 1.1811 - f1_metric: 0.2167 - val_loss: 0.9305 - val_acc: 0.8378 - val_precision: 0.1311 - val_recall: 1.2975 - val_f1_metric: 0.2306\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0884 - acc: 0.5143 - precision: 0.1186 - recall: 1.1678 - f1_metric: 0.2107 - val_loss: 0.9305 - val_acc: 0.8660 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7760 - acc: 0.4960 - precision: 0.1174 - recall: 1.2980 - f1_metric: 0.2109 - val_loss: 0.9305 - val_acc: 0.0936 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8330 - acc: 0.4821 - precision: 0.1248 - recall: 1.3149 - f1_metric: 0.2235 - val_loss: 0.9305 - val_acc: 0.8531 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0262 - acc: 0.4709 - precision: 0.1113 - recall: 1.1895 - f1_metric: 0.1989 - val_loss: 0.9305 - val_acc: 0.7638 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7159 - acc: 0.4736 - precision: 0.1283 - recall: 1.2432 - f1_metric: 0.2276 - val_loss: 0.9305 - val_acc: 0.8966 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7924 - acc: 0.5043 - precision: 0.1184 - recall: 1.2343 - f1_metric: 0.2118 - val_loss: 0.9305 - val_acc: 0.0991 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7312 - acc: 0.4733 - precision: 0.1211 - recall: 1.2045 - f1_metric: 0.2155 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7513 - acc: 0.5130 - precision: 0.1245 - recall: 1.2520 - f1_metric: 0.2223 - val_loss: 0.9305 - val_acc: 0.8990 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7861 - acc: 0.4730 - precision: 0.1287 - recall: 1.2616 - f1_metric: 0.2295 - val_loss: 0.9305 - val_acc: 0.8666 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7849 - acc: 0.4919 - precision: 0.1210 - recall: 1.2765 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.8923 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0284 - acc: 0.5014 - precision: 0.1214 - recall: 1.2258 - f1_metric: 0.2176 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6516 - acc: 0.4857 - precision: 0.1199 - recall: 1.1880 - f1_metric: 0.2136 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7135 - acc: 0.4926 - precision: 0.1243 - recall: 1.2563 - f1_metric: 0.2225 - val_loss: 0.9305 - val_acc: 0.8984 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0489 - acc: 0.4859 - precision: 0.1155 - recall: 1.2292 - f1_metric: 0.2064 - val_loss: 0.9305 - val_acc: 0.9002 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6232 - acc: 0.4864 - precision: 0.1170 - recall: 1.1469 - f1_metric: 0.2078 - val_loss: 0.9305 - val_acc: 0.8819 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9344 - acc: 0.4808 - precision: 0.1120 - recall: 1.1832 - f1_metric: 0.1997 - val_loss: 0.9305 - val_acc: 0.8862 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8233 - acc: 0.4680 - precision: 0.1207 - recall: 1.2582 - f1_metric: 0.2167 - val_loss: 0.9305 - val_acc: 0.8323 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6497 - acc: 0.4755 - precision: 0.1195 - recall: 1.2420 - f1_metric: 0.2142 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7333 - acc: 0.4944 - precision: 0.1219 - recall: 1.2139 - f1_metric: 0.2175 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2108 - acc: 0.5038 - precision: 0.1283 - recall: 1.2424 - f1_metric: 0.2282 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9207 - acc: 0.4863 - precision: 0.1194 - recall: 1.2829 - f1_metric: 0.2142 - val_loss: 0.9305 - val_acc: 0.8727 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8162 - acc: 0.4919 - precision: 0.1191 - recall: 1.2375 - f1_metric: 0.2131 - val_loss: 0.9305 - val_acc: 0.8807 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9261 - acc: 0.4908 - precision: 0.1197 - recall: 1.3031 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.0930 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7027 - acc: 0.4855 - precision: 0.1208 - recall: 1.2646 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0109 - acc: 0.4893 - precision: 0.1135 - recall: 1.1752 - f1_metric: 0.2032 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 3 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.7665 - acc: 0.4845 - precision: 0.1135 - recall: 1.1789 - f1_metric: 0.2025 - val_loss: 0.9305 - val_acc: 0.0820 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7475 - acc: 0.4509 - precision: 0.1266 - recall: 1.2260 - f1_metric: 0.2256 - val_loss: 0.9305 - val_acc: 0.8384 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7290 - acc: 0.4801 - precision: 0.1164 - recall: 1.2451 - f1_metric: 0.2087 - val_loss: 0.9305 - val_acc: 0.0783 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7641 - acc: 0.4545 - precision: 0.1215 - recall: 1.2425 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.0783 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0174 - acc: 0.4577 - precision: 0.1190 - recall: 1.2571 - f1_metric: 0.2130 - val_loss: 0.9305 - val_acc: 0.1016 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1172 - acc: 0.4772 - precision: 0.1225 - recall: 1.2337 - f1_metric: 0.2172 - val_loss: 0.9305 - val_acc: 0.1108 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6891 - acc: 0.4634 - precision: 0.1214 - recall: 1.2382 - f1_metric: 0.2172 - val_loss: 0.9305 - val_acc: 0.0918 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.3876 - acc: 0.4470 - precision: 0.1181 - recall: 1.3220 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.1389 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0328 - acc: 0.4793 - precision: 0.1185 - recall: 1.2479 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.1200 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8412 - acc: 0.4834 - precision: 0.1316 - recall: 1.2947 - f1_metric: 0.2346 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8181 - acc: 0.4731 - precision: 0.1126 - recall: 1.2642 - f1_metric: 0.2024 - val_loss: 0.9305 - val_acc: 0.0796 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8630 - acc: 0.4749 - precision: 0.1148 - recall: 1.2578 - f1_metric: 0.2069 - val_loss: 0.9305 - val_acc: 0.0722 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8847 - acc: 0.4881 - precision: 0.1184 - recall: 1.2499 - f1_metric: 0.2123 - val_loss: 0.9305 - val_acc: 0.1506 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7671 - acc: 0.4904 - precision: 0.1201 - recall: 1.2684 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.2307 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9641 - acc: 0.5153 - precision: 0.1258 - recall: 1.1926 - f1_metric: 0.2232 - val_loss: 0.9305 - val_acc: 0.0998 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6564 - acc: 0.4945 - precision: 0.1119 - recall: 1.2049 - f1_metric: 0.2005 - val_loss: 0.9305 - val_acc: 0.7699 - val_precision: 0.1326 - val_recall: 1.3040 - val_f1_metric: 0.2331\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7554 - acc: 0.4971 - precision: 0.1206 - recall: 1.1967 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.3262 - val_precision: 0.1332 - val_recall: 1.2895 - val_f1_metric: 0.2340\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7281 - acc: 0.5061 - precision: 0.1285 - recall: 1.3196 - f1_metric: 0.2283 - val_loss: 0.9305 - val_acc: 0.3329 - val_precision: 0.1316 - val_recall: 1.2975 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9278 - acc: 0.4963 - precision: 0.1239 - recall: 1.1980 - f1_metric: 0.2198 - val_loss: 0.9305 - val_acc: 0.1561 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7995 - acc: 0.5003 - precision: 0.1098 - recall: 1.2010 - f1_metric: 0.1974 - val_loss: 0.9305 - val_acc: 0.1812 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9024 - acc: 0.4915 - precision: 0.1249 - recall: 1.1910 - f1_metric: 0.2215 - val_loss: 0.9305 - val_acc: 0.8874 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7410 - acc: 0.5164 - precision: 0.1188 - recall: 1.2969 - f1_metric: 0.2130 - val_loss: 0.9305 - val_acc: 0.1475 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7333 - acc: 0.4864 - precision: 0.1210 - recall: 1.2508 - f1_metric: 0.2172 - val_loss: 0.9305 - val_acc: 0.0863 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9194 - acc: 0.4747 - precision: 0.1258 - recall: 1.2937 - f1_metric: 0.2252 - val_loss: 0.9305 - val_acc: 0.8960 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0683 - acc: 0.5096 - precision: 0.1171 - recall: 1.2123 - f1_metric: 0.2095 - val_loss: 0.9305 - val_acc: 0.2197 - val_precision: 0.1321 - val_recall: 1.3040 - val_f1_metric: 0.2324\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9620 - acc: 0.4995 - precision: 0.1222 - recall: 1.2308 - f1_metric: 0.2179 - val_loss: 0.9305 - val_acc: 0.6255 - val_precision: 0.1315 - val_recall: 1.3012 - val_f1_metric: 0.2314\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8678 - acc: 0.4891 - precision: 0.1249 - recall: 1.2949 - f1_metric: 0.2235 - val_loss: 0.9305 - val_acc: 0.1548 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8929 - acc: 0.4960 - precision: 0.1241 - recall: 1.2771 - f1_metric: 0.2210 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7651 - acc: 0.4947 - precision: 0.1222 - recall: 1.2313 - f1_metric: 0.2180 - val_loss: 0.9305 - val_acc: 0.8984 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6964 - acc: 0.4978 - precision: 0.1081 - recall: 1.1723 - f1_metric: 0.1937 - val_loss: 0.9305 - val_acc: 0.0734 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7714 - acc: 0.4741 - precision: 0.1186 - recall: 1.2698 - f1_metric: 0.2123 - val_loss: 0.9305 - val_acc: 0.8764 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7453 - acc: 0.4969 - precision: 0.1161 - recall: 1.2368 - f1_metric: 0.2079 - val_loss: 0.9305 - val_acc: 0.8966 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8826 - acc: 0.4989 - precision: 0.1234 - recall: 1.2731 - f1_metric: 0.2213 - val_loss: 0.9305 - val_acc: 0.8421 - val_precision: 0.1317 - val_recall: 1.2943 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0629 - acc: 0.4895 - precision: 0.1255 - recall: 1.2810 - f1_metric: 0.2252 - val_loss: 0.9305 - val_acc: 0.1132 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6575 - acc: 0.4912 - precision: 0.1141 - recall: 1.2307 - f1_metric: 0.2053 - val_loss: 0.9305 - val_acc: 0.0796 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6947 - acc: 0.4823 - precision: 0.1187 - recall: 1.2348 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0701 - acc: 0.4819 - precision: 0.1287 - recall: 1.3260 - f1_metric: 0.2303 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6994 - acc: 0.4858 - precision: 0.1157 - recall: 1.2266 - f1_metric: 0.2081 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6285 - acc: 0.4926 - precision: 0.1120 - recall: 1.2212 - f1_metric: 0.2012 - val_loss: 0.9305 - val_acc: 0.8800 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0454 - acc: 0.5018 - precision: 0.1201 - recall: 1.1998 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.8733 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1702 - acc: 0.4908 - precision: 0.1149 - recall: 1.2401 - f1_metric: 0.2063 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7955 - acc: 0.4822 - precision: 0.1218 - recall: 1.2204 - f1_metric: 0.2179 - val_loss: 0.9305 - val_acc: 0.0734 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9510 - acc: 0.4598 - precision: 0.1240 - recall: 1.2415 - f1_metric: 0.2217 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7398 - acc: 0.4805 - precision: 0.1109 - recall: 1.2347 - f1_metric: 0.1993 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0794 - acc: 0.4693 - precision: 0.1283 - recall: 1.2572 - f1_metric: 0.2295 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8249 - acc: 0.4877 - precision: 0.1178 - recall: 1.2238 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8340 - acc: 0.4979 - precision: 0.1257 - recall: 1.2422 - f1_metric: 0.2248 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8740 - acc: 0.4906 - precision: 0.1213 - recall: 1.2724 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.8109 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6461 - acc: 0.4973 - precision: 0.1114 - recall: 1.1960 - f1_metric: 0.2001 - val_loss: 0.9305 - val_acc: 0.8911 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1664 - acc: 0.4895 - precision: 0.1200 - recall: 1.2692 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.1077 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2324\n",
            "Model 4 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.7158 - acc: 0.4663 - precision: 0.1242 - recall: 1.2399 - f1_metric: 0.2219 - val_loss: 0.9305 - val_acc: 0.3672 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7771 - acc: 0.4852 - precision: 0.1190 - recall: 1.2348 - f1_metric: 0.2131 - val_loss: 0.9305 - val_acc: 0.4706 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6882 - acc: 0.4922 - precision: 0.1211 - recall: 1.2216 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9948 - acc: 0.5071 - precision: 0.1185 - recall: 1.2021 - f1_metric: 0.2118 - val_loss: 0.9305 - val_acc: 0.8929 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6085 - acc: 0.4996 - precision: 0.1189 - recall: 1.1897 - f1_metric: 0.2125 - val_loss: 0.9305 - val_acc: 0.8947 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8960 - acc: 0.4907 - precision: 0.1170 - recall: 1.2155 - f1_metric: 0.2101 - val_loss: 0.9305 - val_acc: 0.8874 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8209 - acc: 0.4974 - precision: 0.1166 - recall: 1.2114 - f1_metric: 0.2087 - val_loss: 0.9305 - val_acc: 0.8819 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6466 - acc: 0.5087 - precision: 0.1095 - recall: 1.1760 - f1_metric: 0.1967 - val_loss: 0.9305 - val_acc: 0.8911 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9934 - acc: 0.4878 - precision: 0.1241 - recall: 1.2531 - f1_metric: 0.2214 - val_loss: 0.9305 - val_acc: 0.8905 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9507 - acc: 0.5174 - precision: 0.1175 - recall: 1.2270 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.8709 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6986 - acc: 0.5037 - precision: 0.1205 - recall: 1.1813 - f1_metric: 0.2131 - val_loss: 0.9305 - val_acc: 0.8678 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2159 - acc: 0.4826 - precision: 0.1224 - recall: 1.2880 - f1_metric: 0.2196 - val_loss: 0.9305 - val_acc: 0.8727 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9935 - acc: 0.4810 - precision: 0.1186 - recall: 1.2273 - f1_metric: 0.2124 - val_loss: 0.9305 - val_acc: 0.8721 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9227 - acc: 0.4952 - precision: 0.1240 - recall: 1.2419 - f1_metric: 0.2213 - val_loss: 0.9305 - val_acc: 0.8611 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8773 - acc: 0.4883 - precision: 0.1147 - recall: 1.2287 - f1_metric: 0.2052 - val_loss: 0.9305 - val_acc: 0.8709 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0734 - acc: 0.4885 - precision: 0.1223 - recall: 1.2977 - f1_metric: 0.2194 - val_loss: 0.9305 - val_acc: 0.8586 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0532 - acc: 0.4761 - precision: 0.1257 - recall: 1.2766 - f1_metric: 0.2247 - val_loss: 0.9305 - val_acc: 0.8550 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7007 - acc: 0.4830 - precision: 0.1148 - recall: 1.1851 - f1_metric: 0.2051 - val_loss: 0.9305 - val_acc: 0.8868 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8683 - acc: 0.4769 - precision: 0.1160 - recall: 1.2278 - f1_metric: 0.2066 - val_loss: 0.9305 - val_acc: 0.8849 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7735 - acc: 0.4933 - precision: 0.1197 - recall: 1.2126 - f1_metric: 0.2139 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9025 - acc: 0.4804 - precision: 0.1291 - recall: 1.2275 - f1_metric: 0.2287 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8725 - acc: 0.5082 - precision: 0.1139 - recall: 1.1976 - f1_metric: 0.2042 - val_loss: 0.9305 - val_acc: 0.8929 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6421 - acc: 0.4636 - precision: 0.1177 - recall: 1.2464 - f1_metric: 0.2103 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6302 - acc: 0.4968 - precision: 0.1179 - recall: 1.2327 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1996 - acc: 0.4930 - precision: 0.1251 - recall: 1.2522 - f1_metric: 0.2224 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9787 - acc: 0.4883 - precision: 0.1136 - recall: 1.2526 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1801 - acc: 0.4868 - precision: 0.1147 - recall: 1.2777 - f1_metric: 0.2067 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7782 - acc: 0.5055 - precision: 0.1067 - recall: 1.2002 - f1_metric: 0.1915 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7214 - acc: 0.5091 - precision: 0.1153 - recall: 1.2367 - f1_metric: 0.2073 - val_loss: 0.9305 - val_acc: 0.8788 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8594 - acc: 0.4679 - precision: 0.1188 - recall: 1.2367 - f1_metric: 0.2128 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8526 - acc: 0.4921 - precision: 0.1149 - recall: 1.2314 - f1_metric: 0.2060 - val_loss: 0.9305 - val_acc: 0.8127 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6801 - acc: 0.4808 - precision: 0.1126 - recall: 1.2097 - f1_metric: 0.2020 - val_loss: 0.9305 - val_acc: 0.8647 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9752 - acc: 0.4937 - precision: 0.1132 - recall: 1.2272 - f1_metric: 0.2034 - val_loss: 0.9305 - val_acc: 0.8274 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7604 - acc: 0.4997 - precision: 0.1135 - recall: 1.2017 - f1_metric: 0.2029 - val_loss: 0.9305 - val_acc: 0.8586 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9623 - acc: 0.4786 - precision: 0.1141 - recall: 1.2155 - f1_metric: 0.2048 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8291 - acc: 0.4920 - precision: 0.1201 - recall: 1.2634 - f1_metric: 0.2151 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9961 - acc: 0.5088 - precision: 0.1210 - recall: 1.2084 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.4211 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6912 - acc: 0.4776 - precision: 0.1106 - recall: 1.1737 - f1_metric: 0.1985 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7702 - acc: 0.4813 - precision: 0.1178 - recall: 1.2278 - f1_metric: 0.2115 - val_loss: 0.9305 - val_acc: 0.0685 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9465 - acc: 0.4834 - precision: 0.1158 - recall: 1.1598 - f1_metric: 0.2059 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9291 - acc: 0.4489 - precision: 0.1136 - recall: 1.2467 - f1_metric: 0.2050 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8259 - acc: 0.4611 - precision: 0.1215 - recall: 1.2373 - f1_metric: 0.2179 - val_loss: 0.9305 - val_acc: 0.8825 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9568 - acc: 0.4901 - precision: 0.1259 - recall: 1.2484 - f1_metric: 0.2238 - val_loss: 0.9305 - val_acc: 0.8935 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8893 - acc: 0.5181 - precision: 0.1277 - recall: 1.2607 - f1_metric: 0.2279 - val_loss: 0.9305 - val_acc: 0.0747 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8725 - acc: 0.4727 - precision: 0.1245 - recall: 1.2610 - f1_metric: 0.2228 - val_loss: 0.9305 - val_acc: 0.0741 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6256 - acc: 0.4907 - precision: 0.1212 - recall: 1.2206 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7148 - acc: 0.5111 - precision: 0.1162 - recall: 1.2458 - f1_metric: 0.2088 - val_loss: 0.9305 - val_acc: 0.8953 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1294 - acc: 0.4788 - precision: 0.1161 - recall: 1.2297 - f1_metric: 0.2090 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7466 - acc: 0.4767 - precision: 0.1192 - recall: 1.2240 - f1_metric: 0.2133 - val_loss: 0.9305 - val_acc: 0.0741 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1101 - acc: 0.4849 - precision: 0.1223 - recall: 1.2823 - f1_metric: 0.2178 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 5 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.7963 - acc: 0.4537 - precision: 0.1215 - recall: 1.2367 - f1_metric: 0.2174 - val_loss: 0.9305 - val_acc: 0.0796 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2210 - acc: 0.4599 - precision: 0.1276 - recall: 1.2612 - f1_metric: 0.2270 - val_loss: 0.9305 - val_acc: 0.1169 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0242 - acc: 0.4804 - precision: 0.1175 - recall: 1.2929 - f1_metric: 0.2118 - val_loss: 0.9305 - val_acc: 0.1279 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7337 - acc: 0.4859 - precision: 0.1147 - recall: 1.2239 - f1_metric: 0.2051 - val_loss: 0.9305 - val_acc: 0.0820 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0622 - acc: 0.5017 - precision: 0.1132 - recall: 1.2065 - f1_metric: 0.2031 - val_loss: 0.9305 - val_acc: 0.0771 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6527 - acc: 0.4987 - precision: 0.1068 - recall: 1.2304 - f1_metric: 0.1926 - val_loss: 0.9305 - val_acc: 0.0826 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6719 - acc: 0.4879 - precision: 0.1131 - recall: 1.2541 - f1_metric: 0.2035 - val_loss: 0.9305 - val_acc: 0.0832 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7702 - acc: 0.5090 - precision: 0.1136 - recall: 1.2012 - f1_metric: 0.2036 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8027 - acc: 0.4919 - precision: 0.1235 - recall: 1.2295 - f1_metric: 0.2195 - val_loss: 0.9305 - val_acc: 0.0875 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8425 - acc: 0.5029 - precision: 0.1223 - recall: 1.2367 - f1_metric: 0.2176 - val_loss: 0.9305 - val_acc: 0.0777 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8262 - acc: 0.4957 - precision: 0.1171 - recall: 1.2416 - f1_metric: 0.2101 - val_loss: 0.9305 - val_acc: 0.0851 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6445 - acc: 0.5166 - precision: 0.1236 - recall: 1.2761 - f1_metric: 0.2212 - val_loss: 0.9305 - val_acc: 0.1040 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.3383 - acc: 0.5156 - precision: 0.1202 - recall: 1.2829 - f1_metric: 0.2153 - val_loss: 0.9305 - val_acc: 0.1542 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0920 - acc: 0.4959 - precision: 0.1214 - recall: 1.2157 - f1_metric: 0.2166 - val_loss: 0.9305 - val_acc: 0.0863 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8471 - acc: 0.4784 - precision: 0.1169 - recall: 1.1920 - f1_metric: 0.2096 - val_loss: 0.9305 - val_acc: 0.0759 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9052 - acc: 0.4800 - precision: 0.1194 - recall: 1.2094 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9691 - acc: 0.5050 - precision: 0.1138 - recall: 1.2244 - f1_metric: 0.2037 - val_loss: 0.9305 - val_acc: 0.1750 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6910 - acc: 0.4866 - precision: 0.1187 - recall: 1.2403 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.1420 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0537 - acc: 0.4929 - precision: 0.1225 - recall: 1.2876 - f1_metric: 0.2192 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7081 - acc: 0.5101 - precision: 0.1144 - recall: 1.2445 - f1_metric: 0.2058 - val_loss: 0.9305 - val_acc: 0.1995 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7433 - acc: 0.4939 - precision: 0.1209 - recall: 1.2166 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.7246 - val_precision: 0.1326 - val_recall: 1.3040 - val_f1_metric: 0.2333\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8151 - acc: 0.4833 - precision: 0.1211 - recall: 1.2529 - f1_metric: 0.2170 - val_loss: 0.9305 - val_acc: 0.0753 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6608 - acc: 0.4829 - precision: 0.1154 - recall: 1.1701 - f1_metric: 0.2061 - val_loss: 0.9305 - val_acc: 0.0685 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9917 - acc: 0.4698 - precision: 0.1174 - recall: 1.1971 - f1_metric: 0.2103 - val_loss: 0.9305 - val_acc: 0.0869 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7058 - acc: 0.4878 - precision: 0.1131 - recall: 1.2218 - f1_metric: 0.2033 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8831 - acc: 0.5008 - precision: 0.1170 - recall: 1.2463 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.0796 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7599 - acc: 0.4702 - precision: 0.1176 - recall: 1.2368 - f1_metric: 0.2100 - val_loss: 0.9305 - val_acc: 0.0808 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7744 - acc: 0.4781 - precision: 0.1206 - recall: 1.2617 - f1_metric: 0.2157 - val_loss: 0.9305 - val_acc: 0.2693 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0865 - acc: 0.5002 - precision: 0.1262 - recall: 1.2667 - f1_metric: 0.2251 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8986 - acc: 0.4946 - precision: 0.1244 - recall: 1.2737 - f1_metric: 0.2229 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7674 - acc: 0.4844 - precision: 0.1193 - recall: 1.2212 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.8617 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8616 - acc: 0.5054 - precision: 0.1320 - recall: 1.2522 - f1_metric: 0.2346 - val_loss: 0.9305 - val_acc: 0.8507 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0278 - acc: 0.4901 - precision: 0.1154 - recall: 1.2529 - f1_metric: 0.2073 - val_loss: 0.9305 - val_acc: 0.8996 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0214 - acc: 0.5098 - precision: 0.1153 - recall: 1.2785 - f1_metric: 0.2067 - val_loss: 0.9305 - val_acc: 0.1408 - val_precision: 0.1324 - val_recall: 1.3040 - val_f1_metric: 0.2328\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9710 - acc: 0.4766 - precision: 0.1251 - recall: 1.2696 - f1_metric: 0.2231 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7470 - acc: 0.4912 - precision: 0.1176 - recall: 1.2608 - f1_metric: 0.2109 - val_loss: 0.9305 - val_acc: 0.0894 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6958 - acc: 0.4833 - precision: 0.1127 - recall: 1.2590 - f1_metric: 0.2030 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8314 - acc: 0.4923 - precision: 0.1206 - recall: 1.2114 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0238 - acc: 0.4930 - precision: 0.1220 - recall: 1.2638 - f1_metric: 0.2193 - val_loss: 0.9305 - val_acc: 0.1083 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8982 - acc: 0.4898 - precision: 0.1144 - recall: 1.1912 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7716 - acc: 0.4761 - precision: 0.1220 - recall: 1.2058 - f1_metric: 0.2175 - val_loss: 0.9305 - val_acc: 0.1218 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8690 - acc: 0.4864 - precision: 0.1158 - recall: 1.2102 - f1_metric: 0.2084 - val_loss: 0.9305 - val_acc: 0.5080 - val_precision: 0.1324 - val_recall: 1.3040 - val_f1_metric: 0.2329\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7601 - acc: 0.4950 - precision: 0.1185 - recall: 1.2460 - f1_metric: 0.2131 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0253 - acc: 0.5102 - precision: 0.1244 - recall: 1.2680 - f1_metric: 0.2226 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7146 - acc: 0.4948 - precision: 0.1190 - recall: 1.2408 - f1_metric: 0.2139 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7299 - acc: 0.4730 - precision: 0.1287 - recall: 1.2843 - f1_metric: 0.2291 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7818 - acc: 0.4840 - precision: 0.1230 - recall: 1.2620 - f1_metric: 0.2205 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6252 - acc: 0.5003 - precision: 0.1169 - recall: 1.2095 - f1_metric: 0.2088 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1105 - acc: 0.4624 - precision: 0.1199 - recall: 1.2732 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.3089 - acc: 0.5013 - precision: 0.1117 - recall: 1.2168 - f1_metric: 0.2007 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 6 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.7550 - acc: 0.5152 - precision: 0.1243 - recall: 1.2776 - f1_metric: 0.2218 - val_loss: 0.9305 - val_acc: 0.7968 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7493 - acc: 0.5152 - precision: 0.1171 - recall: 1.1942 - f1_metric: 0.2093 - val_loss: 0.9305 - val_acc: 0.8782 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6369 - acc: 0.5087 - precision: 0.1178 - recall: 1.2478 - f1_metric: 0.2116 - val_loss: 0.9305 - val_acc: 0.1322 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7249 - acc: 0.4965 - precision: 0.1198 - recall: 1.2095 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7898 - acc: 0.5073 - precision: 0.1151 - recall: 1.1869 - f1_metric: 0.2062 - val_loss: 0.9305 - val_acc: 0.0949 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6110 - acc: 0.5084 - precision: 0.1117 - recall: 1.2686 - f1_metric: 0.2010 - val_loss: 0.9305 - val_acc: 0.8421 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6857 - acc: 0.5194 - precision: 0.1154 - recall: 1.2671 - f1_metric: 0.2072 - val_loss: 0.9305 - val_acc: 0.8703 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0257 - acc: 0.5049 - precision: 0.1231 - recall: 1.2693 - f1_metric: 0.2196 - val_loss: 0.9305 - val_acc: 0.2332 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7616 - acc: 0.4980 - precision: 0.1169 - recall: 1.1980 - f1_metric: 0.2097 - val_loss: 0.9305 - val_acc: 0.5918 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7849 - acc: 0.5174 - precision: 0.1186 - recall: 1.2717 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.6860 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6883 - acc: 0.5224 - precision: 0.1180 - recall: 1.2471 - f1_metric: 0.2118 - val_loss: 0.9305 - val_acc: 0.9002 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7755 - acc: 0.5249 - precision: 0.1161 - recall: 1.2314 - f1_metric: 0.2083 - val_loss: 0.9305 - val_acc: 0.8831 - val_precision: 0.1312 - val_recall: 1.2991 - val_f1_metric: 0.2308\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1735 - acc: 0.5059 - precision: 0.1128 - recall: 1.2972 - f1_metric: 0.2032 - val_loss: 0.9305 - val_acc: 0.6634 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9324 - acc: 0.5179 - precision: 0.1192 - recall: 1.2354 - f1_metric: 0.2132 - val_loss: 0.9305 - val_acc: 0.2118 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8730 - acc: 0.5050 - precision: 0.1211 - recall: 1.2120 - f1_metric: 0.2157 - val_loss: 0.9305 - val_acc: 0.1389 - val_precision: 0.1313 - val_recall: 1.3018 - val_f1_metric: 0.2310\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5359 - acc: 0.5031 - precision: 0.1092 - recall: 1.1805 - f1_metric: 0.1967 - val_loss: 0.9305 - val_acc: 0.8813 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7048 - acc: 0.5276 - precision: 0.1202 - recall: 1.2527 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.0685 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6993 - acc: 0.5086 - precision: 0.1197 - recall: 1.2627 - f1_metric: 0.2147 - val_loss: 0.9305 - val_acc: 0.8550 - val_precision: 0.1311 - val_recall: 1.2991 - val_f1_metric: 0.2307\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5968 - acc: 0.5028 - precision: 0.1138 - recall: 1.1850 - f1_metric: 0.2036 - val_loss: 0.9305 - val_acc: 0.5422 - val_precision: 0.1315 - val_recall: 1.2943 - val_f1_metric: 0.2312\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0406 - acc: 0.5091 - precision: 0.1173 - recall: 1.2917 - f1_metric: 0.2110 - val_loss: 0.9305 - val_acc: 0.8715 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6869 - acc: 0.5308 - precision: 0.1154 - recall: 1.2393 - f1_metric: 0.2078 - val_loss: 0.9305 - val_acc: 0.8868 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9313 - acc: 0.5133 - precision: 0.1195 - recall: 1.1861 - f1_metric: 0.2128 - val_loss: 0.9305 - val_acc: 0.8556 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9691 - acc: 0.5004 - precision: 0.1165 - recall: 1.2129 - f1_metric: 0.2086 - val_loss: 0.9305 - val_acc: 0.8752 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7255 - acc: 0.5147 - precision: 0.1131 - recall: 1.2270 - f1_metric: 0.2024 - val_loss: 0.9305 - val_acc: 0.8348 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6046 - acc: 0.5062 - precision: 0.1172 - recall: 1.2236 - f1_metric: 0.2100 - val_loss: 0.9305 - val_acc: 0.8911 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0973 - acc: 0.5161 - precision: 0.1235 - recall: 1.2678 - f1_metric: 0.2205 - val_loss: 0.9305 - val_acc: 0.7424 - val_precision: 0.1317 - val_recall: 1.2991 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7565 - acc: 0.4940 - precision: 0.1101 - recall: 1.2552 - f1_metric: 0.1988 - val_loss: 0.9305 - val_acc: 0.8335 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8323 - acc: 0.5014 - precision: 0.1192 - recall: 1.2134 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.8880 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7370 - acc: 0.4969 - precision: 0.1244 - recall: 1.2390 - f1_metric: 0.2213 - val_loss: 0.9305 - val_acc: 0.1720 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9309 - acc: 0.4861 - precision: 0.1211 - recall: 1.2477 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0084 - acc: 0.4911 - precision: 0.1214 - recall: 1.2380 - f1_metric: 0.2176 - val_loss: 0.9305 - val_acc: 0.8592 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9907 - acc: 0.4899 - precision: 0.1245 - recall: 1.2485 - f1_metric: 0.2220 - val_loss: 0.9305 - val_acc: 0.8715 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8327 - acc: 0.5005 - precision: 0.1163 - recall: 1.2305 - f1_metric: 0.2086 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6905 - acc: 0.5053 - precision: 0.1227 - recall: 1.2031 - f1_metric: 0.2181 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8478 - acc: 0.4692 - precision: 0.1161 - recall: 1.2151 - f1_metric: 0.2081 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7494 - acc: 0.4978 - precision: 0.1190 - recall: 1.2253 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9124 - acc: 0.5012 - precision: 0.1238 - recall: 1.2540 - f1_metric: 0.2220 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0254 - acc: 0.4868 - precision: 0.1208 - recall: 1.2415 - f1_metric: 0.2163 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8901 - acc: 0.4685 - precision: 0.1145 - recall: 1.2920 - f1_metric: 0.2059 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9265 - acc: 0.4869 - precision: 0.1151 - recall: 1.2394 - f1_metric: 0.2072 - val_loss: 0.9305 - val_acc: 0.7583 - val_precision: 0.1323 - val_recall: 1.3040 - val_f1_metric: 0.2328\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7803 - acc: 0.4752 - precision: 0.1201 - recall: 1.2685 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.8776 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1083 - acc: 0.5033 - precision: 0.1139 - recall: 1.1657 - f1_metric: 0.2025 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6803 - acc: 0.4881 - precision: 0.1196 - recall: 1.2394 - f1_metric: 0.2143 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0119 - acc: 0.4717 - precision: 0.1232 - recall: 1.2465 - f1_metric: 0.2195 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7589 - acc: 0.4772 - precision: 0.1169 - recall: 1.1871 - f1_metric: 0.2088 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7217 - acc: 0.4846 - precision: 0.1178 - recall: 1.2389 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.0955 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8473 - acc: 0.4678 - precision: 0.1256 - recall: 1.2728 - f1_metric: 0.2241 - val_loss: 0.9305 - val_acc: 0.8947 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6410 - acc: 0.5205 - precision: 0.1149 - recall: 1.2183 - f1_metric: 0.2066 - val_loss: 0.9305 - val_acc: 0.8935 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7849 - acc: 0.4846 - precision: 0.1172 - recall: 1.2058 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8038 - acc: 0.4757 - precision: 0.1196 - recall: 1.2674 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.8972 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 7 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.7605 - acc: 0.4772 - precision: 0.1139 - recall: 1.2326 - f1_metric: 0.2036 - val_loss: 0.9305 - val_acc: 0.0759 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6169 - acc: 0.4026 - precision: 0.1178 - recall: 1.2864 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6750 - acc: 0.5140 - precision: 0.1205 - recall: 1.2735 - f1_metric: 0.2162 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6098 - acc: 0.4986 - precision: 0.1187 - recall: 1.2116 - f1_metric: 0.2103 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9108 - acc: 0.5824 - precision: 0.1157 - recall: 1.3087 - f1_metric: 0.2078 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9690 - acc: 0.5367 - precision: 0.1217 - recall: 1.2530 - f1_metric: 0.2180 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6365 - acc: 0.4949 - precision: 0.1220 - recall: 1.2434 - f1_metric: 0.2190 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7255 - acc: 0.5058 - precision: 0.1241 - recall: 1.1787 - f1_metric: 0.2212 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7592 - acc: 0.4614 - precision: 0.1192 - recall: 1.2658 - f1_metric: 0.2146 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9185 - acc: 0.4839 - precision: 0.1178 - recall: 1.2094 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6409 - acc: 0.5409 - precision: 0.1170 - recall: 1.2519 - f1_metric: 0.2088 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 18461538.0000 - val_recall: 0.6079 - val_f1_metric: 1.2158\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7680 - acc: 0.4631 - precision: 0.1215 - recall: 1.2532 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8479 - acc: 0.5136 - precision: 0.1182 - recall: 1.2094 - f1_metric: 0.2108 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9257 - acc: 0.5225 - precision: 0.1232 - recall: 1.2087 - f1_metric: 0.2201 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0262 - acc: 0.5201 - precision: 0.1278 - recall: 1.2146 - f1_metric: 0.2238 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0692 - acc: 0.4648 - precision: 0.1147 - recall: 1.3027 - f1_metric: 0.2063 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9789 - acc: 0.4861 - precision: 0.1247 - recall: 1.2365 - f1_metric: 0.2229 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9816 - acc: 0.5187 - precision: 0.1184 - recall: 1.2450 - f1_metric: 0.2122 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7758 - acc: 0.5142 - precision: 0.1184 - recall: 1.2110 - f1_metric: 0.2110 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8433 - acc: 0.4688 - precision: 0.1229 - recall: 1.2164 - f1_metric: 0.2185 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8567 - acc: 0.4951 - precision: 0.1344 - recall: 1.1847 - f1_metric: 0.2274 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8874 - acc: 0.4998 - precision: 14149.1628 - recall: 1.2061 - f1_metric: 0.2108 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2989 - acc: 0.4889 - precision: 721514.9378 - recall: 1.1689 - f1_metric: 0.2652 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 18461538.0000 - val_recall: 0.6079 - val_f1_metric: 1.2158\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1482 - acc: 0.5961 - precision: 7803169.2573 - recall: 0.9092 - f1_metric: 0.5940 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5906 - acc: 0.5205 - precision: 25604.2147 - recall: 1.1545 - f1_metric: 0.2088 - val_loss: 0.9305 - val_acc: 0.2338 - val_precision: 0.1269 - val_recall: 1.2623 - val_f1_metric: 0.2262\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6663 - acc: 0.5012 - precision: 31361.9395 - recall: 1.2460 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9802 - acc: 0.4894 - precision: 0.1214 - recall: 1.2658 - f1_metric: 0.2172 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6694 - acc: 0.4923 - precision: 847697.7412 - recall: 1.2092 - f1_metric: 0.2508 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 18461538.0000 - val_recall: 0.6079 - val_f1_metric: 1.2158\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6943 - acc: 0.6848 - precision: 12785789.9129 - recall: 0.6880 - f1_metric: 0.9151 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 18461538.0000 - val_recall: 0.6079 - val_f1_metric: 1.2158\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6001 - acc: 0.6201 - precision: 5789092.7558 - recall: 0.7802 - f1_metric: 0.5387 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 18461538.0000 - val_recall: 0.6079 - val_f1_metric: 1.2158\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8063 - acc: 0.5479 - precision: 3513594.3604 - recall: 1.0630 - f1_metric: 0.4249 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0073 - acc: 0.4917 - precision: 173721.4880 - recall: 1.2213 - f1_metric: 0.2302 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9305 - acc: 0.4967 - precision: 0.1178 - recall: 1.2332 - f1_metric: 0.2110 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0541 - acc: 0.4993 - precision: 1255261.6042 - recall: 1.2342 - f1_metric: 0.2934 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8577 - acc: 0.5392 - precision: 1529950.0595 - recall: 1.1550 - f1_metric: 0.3066 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9966 - acc: 0.4811 - precision: 0.1318 - recall: 1.2555 - f1_metric: 0.2259 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1371 - val_recall: 1.2980 - val_f1_metric: 0.2399\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7222 - acc: 0.4598 - precision: 2846.3450 - recall: 1.2072 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6818 - acc: 0.5136 - precision: 6346964.5535 - recall: 1.0644 - f1_metric: 0.5773 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.4169 - acc: 0.5412 - precision: 5011881.2863 - recall: 1.0332 - f1_metric: 0.5444 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0363 - acc: 0.4948 - precision: 0.1152 - recall: 1.2754 - f1_metric: 0.2076 - val_loss: 0.9305 - val_acc: 0.1548 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9745 - acc: 0.4734 - precision: 0.1175 - recall: 1.2509 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7001 - acc: 0.5677 - precision: 4787959.0549 - recall: 0.9821 - f1_metric: 0.4693 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8319 - acc: 0.4962 - precision: 132259.1650 - recall: 1.2093 - f1_metric: 0.2163 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2609 - acc: 0.5236 - precision: 145873.4399 - recall: 1.2194 - f1_metric: 0.2201 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8757 - acc: 0.4969 - precision: 0.1267 - recall: 1.2372 - f1_metric: 0.2244 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9912 - acc: 0.4783 - precision: 0.1189 - recall: 1.2497 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7896 - acc: 0.4968 - precision: 0.1142 - recall: 1.2104 - f1_metric: 0.2058 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6175 - acc: 0.4826 - precision: 0.1489 - recall: 1.1796 - f1_metric: 0.2419 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6173 - acc: 0.5111 - precision: 1699456.5785 - recall: 1.1257 - f1_metric: 0.3186 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9322 - acc: 0.4963 - precision: 915168.5513 - recall: 1.2433 - f1_metric: 0.2999 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 8 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.8842 - acc: 0.4805 - precision: 0.1171 - recall: 1.2908 - f1_metric: 0.2100 - val_loss: 0.9305 - val_acc: 0.0838 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7382 - acc: 0.4768 - precision: 0.1203 - recall: 1.2964 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.6089 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9835 - acc: 0.4911 - precision: 0.1267 - recall: 1.2342 - f1_metric: 0.2261 - val_loss: 0.9305 - val_acc: 0.3244 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9474 - acc: 0.4910 - precision: 0.1171 - recall: 1.1765 - f1_metric: 0.2090 - val_loss: 0.9305 - val_acc: 0.0845 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7415 - acc: 0.5087 - precision: 0.1184 - recall: 1.2201 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.2319 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9722 - acc: 0.5158 - precision: 0.1211 - recall: 1.2582 - f1_metric: 0.2172 - val_loss: 0.9305 - val_acc: 0.1291 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9007 - acc: 0.4898 - precision: 0.1149 - recall: 1.2296 - f1_metric: 0.2063 - val_loss: 0.9305 - val_acc: 0.8825 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8252 - acc: 0.5118 - precision: 0.1194 - recall: 1.2453 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.8268 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8798 - acc: 0.5105 - precision: 0.1174 - recall: 1.2812 - f1_metric: 0.2098 - val_loss: 0.9305 - val_acc: 0.0808 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8928 - acc: 0.5213 - precision: 0.1209 - recall: 1.2483 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.3348 - val_precision: 0.1304 - val_recall: 1.2778 - val_f1_metric: 0.2291\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0803 - acc: 0.5184 - precision: 0.1177 - recall: 1.2669 - f1_metric: 0.2110 - val_loss: 0.9305 - val_acc: 0.8856 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0325 - acc: 0.5216 - precision: 0.1273 - recall: 1.1913 - f1_metric: 0.2257 - val_loss: 0.9305 - val_acc: 0.0783 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6141 - acc: 0.4964 - precision: 0.1232 - recall: 1.2690 - f1_metric: 0.2206 - val_loss: 0.9305 - val_acc: 0.2307 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6378 - acc: 0.5178 - precision: 0.1111 - recall: 1.2022 - f1_metric: 0.1996 - val_loss: 0.9305 - val_acc: 0.8366 - val_precision: 0.1310 - val_recall: 1.2943 - val_f1_metric: 0.2305\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8451 - acc: 0.5074 - precision: 0.1231 - recall: 1.3131 - f1_metric: 0.2216 - val_loss: 0.9305 - val_acc: 0.8121 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9901 - acc: 0.4962 - precision: 0.1204 - recall: 1.2198 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.8017 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2223 - acc: 0.5114 - precision: 0.1207 - recall: 1.2080 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.1187 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0903 - acc: 0.5023 - precision: 0.1173 - recall: 1.2566 - f1_metric: 0.2107 - val_loss: 0.9305 - val_acc: 0.8446 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8488 - acc: 0.4783 - precision: 0.1278 - recall: 1.2180 - f1_metric: 0.2266 - val_loss: 0.9305 - val_acc: 0.8874 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6536 - acc: 0.4898 - precision: 0.1120 - recall: 1.2581 - f1_metric: 0.2017 - val_loss: 0.9305 - val_acc: 0.7154 - val_precision: 0.1313 - val_recall: 1.3018 - val_f1_metric: 0.2311\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9516 - acc: 0.4767 - precision: 0.1209 - recall: 1.2137 - f1_metric: 0.2163 - val_loss: 0.9305 - val_acc: 0.8978 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0144 - acc: 0.4965 - precision: 0.1223 - recall: 1.2552 - f1_metric: 0.2189 - val_loss: 0.9305 - val_acc: 0.8892 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7391 - acc: 0.4977 - precision: 0.1235 - recall: 1.2697 - f1_metric: 0.2218 - val_loss: 0.9305 - val_acc: 0.8966 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9804 - acc: 0.4955 - precision: 0.1172 - recall: 1.2734 - f1_metric: 0.2113 - val_loss: 0.9305 - val_acc: 0.1187 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1439 - acc: 0.4759 - precision: 0.1212 - recall: 1.2077 - f1_metric: 0.2154 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8212 - acc: 0.5112 - precision: 0.1132 - recall: 1.1842 - f1_metric: 0.2020 - val_loss: 0.9305 - val_acc: 0.0875 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6477 - acc: 0.5020 - precision: 0.1098 - recall: 1.1645 - f1_metric: 0.1964 - val_loss: 0.9305 - val_acc: 0.1322 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8429 - acc: 0.4824 - precision: 0.1213 - recall: 1.2156 - f1_metric: 0.2166 - val_loss: 0.9305 - val_acc: 0.8996 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6347 - acc: 0.4995 - precision: 0.1181 - recall: 1.2516 - f1_metric: 0.2117 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7584 - acc: 0.4743 - precision: 0.1197 - recall: 1.2133 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.5875 - val_precision: 0.1337 - val_recall: 1.3040 - val_f1_metric: 0.2350\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9270 - acc: 0.4946 - precision: 0.1174 - recall: 1.2385 - f1_metric: 0.2103 - val_loss: 0.9305 - val_acc: 0.0728 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9859 - acc: 0.4924 - precision: 0.1253 - recall: 1.2367 - f1_metric: 0.2233 - val_loss: 0.9305 - val_acc: 0.2919 - val_precision: 0.1315 - val_recall: 1.2975 - val_f1_metric: 0.2312\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6890 - acc: 0.4811 - precision: 0.1221 - recall: 1.2308 - f1_metric: 0.2180 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9424 - acc: 0.4854 - precision: 0.1172 - recall: 1.2342 - f1_metric: 0.2098 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7430 - acc: 0.4962 - precision: 0.1145 - recall: 1.2545 - f1_metric: 0.2050 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7953 - acc: 0.4975 - precision: 0.1227 - recall: 1.2443 - f1_metric: 0.2188 - val_loss: 0.9305 - val_acc: 0.0704 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7644 - acc: 0.4792 - precision: 0.1164 - recall: 1.3002 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8258 - acc: 0.5058 - precision: 0.1213 - recall: 1.1859 - f1_metric: 0.2155 - val_loss: 0.9305 - val_acc: 0.2368 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8015 - acc: 0.4826 - precision: 0.1243 - recall: 1.2235 - f1_metric: 0.2207 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8389 - acc: 0.4901 - precision: 0.1208 - recall: 1.2546 - f1_metric: 0.2155 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9811 - acc: 0.4745 - precision: 0.1142 - recall: 1.2045 - f1_metric: 0.2044 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7520 - acc: 0.4894 - precision: 0.1179 - recall: 1.2002 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.8960 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7291 - acc: 0.5053 - precision: 0.1176 - recall: 1.2447 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9452 - acc: 0.4821 - precision: 0.1218 - recall: 1.2312 - f1_metric: 0.2167 - val_loss: 0.9305 - val_acc: 0.0722 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6430 - acc: 0.4866 - precision: 0.1148 - recall: 1.2453 - f1_metric: 0.2048 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6422 - acc: 0.4981 - precision: 0.1102 - recall: 1.2277 - f1_metric: 0.1991 - val_loss: 0.9305 - val_acc: 0.7931 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7530 - acc: 0.4926 - precision: 0.1246 - recall: 1.2094 - f1_metric: 0.2215 - val_loss: 0.9305 - val_acc: 0.8953 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0626 - acc: 0.4867 - precision: 0.1235 - recall: 1.2629 - f1_metric: 0.2210 - val_loss: 0.9305 - val_acc: 0.0783 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8634 - acc: 0.4846 - precision: 0.1105 - recall: 1.2108 - f1_metric: 0.1978 - val_loss: 0.9305 - val_acc: 0.8794 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7724 - acc: 0.5087 - precision: 0.1112 - recall: 1.1982 - f1_metric: 0.2003 - val_loss: 0.9305 - val_acc: 0.8703 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 9 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.7831 - acc: 0.4506 - precision: 0.1302 - recall: 1.2446 - f1_metric: 0.2317 - val_loss: 0.9305 - val_acc: 0.1328 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9498 - acc: 0.4647 - precision: 0.1125 - recall: 1.2601 - f1_metric: 0.2024 - val_loss: 0.9305 - val_acc: 0.0771 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8227 - acc: 0.4738 - precision: 0.1240 - recall: 1.2771 - f1_metric: 0.2211 - val_loss: 0.9305 - val_acc: 0.1548 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7694 - acc: 0.4604 - precision: 0.1176 - recall: 1.2262 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.1169 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8345 - acc: 0.4775 - precision: 0.1165 - recall: 1.1818 - f1_metric: 0.2089 - val_loss: 0.9305 - val_acc: 0.0832 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7289 - acc: 0.4690 - precision: 0.1226 - recall: 1.2488 - f1_metric: 0.2185 - val_loss: 0.9305 - val_acc: 0.0832 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8444 - acc: 0.4580 - precision: 0.1136 - recall: 1.2572 - f1_metric: 0.2037 - val_loss: 0.9305 - val_acc: 0.0789 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0011 - acc: 0.4702 - precision: 0.1164 - recall: 1.2642 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9554 - acc: 0.4889 - precision: 0.1225 - recall: 1.2606 - f1_metric: 0.2180 - val_loss: 0.9305 - val_acc: 0.8776 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0967 - acc: 0.4874 - precision: 0.1212 - recall: 1.2193 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.0924 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9188 - acc: 0.4749 - precision: 0.1164 - recall: 1.1869 - f1_metric: 0.2074 - val_loss: 0.9305 - val_acc: 0.1151 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0785 - acc: 0.5048 - precision: 0.1221 - recall: 1.3369 - f1_metric: 0.2199 - val_loss: 0.9305 - val_acc: 0.4878 - val_precision: 0.1321 - val_recall: 1.3040 - val_f1_metric: 0.2323\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2096 - acc: 0.4817 - precision: 0.1232 - recall: 1.2339 - f1_metric: 0.2196 - val_loss: 0.9305 - val_acc: 0.3439 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8362 - acc: 0.4976 - precision: 0.1153 - recall: 1.2134 - f1_metric: 0.2065 - val_loss: 0.9305 - val_acc: 0.5141 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7731 - acc: 0.4940 - precision: 0.1131 - recall: 1.2744 - f1_metric: 0.2040 - val_loss: 0.9305 - val_acc: 0.0881 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1688 - acc: 0.5261 - precision: 0.1214 - recall: 1.2348 - f1_metric: 0.2176 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8875 - acc: 0.4626 - precision: 0.1162 - recall: 1.2435 - f1_metric: 0.2087 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8794 - acc: 0.4936 - precision: 0.1218 - recall: 1.2362 - f1_metric: 0.2177 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7427 - acc: 0.4689 - precision: 0.1167 - recall: 1.2159 - f1_metric: 0.2090 - val_loss: 0.9305 - val_acc: 0.8990 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6506 - acc: 0.5026 - precision: 0.1137 - recall: 1.2486 - f1_metric: 0.2045 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0092 - acc: 0.4931 - precision: 0.1223 - recall: 1.2285 - f1_metric: 0.2184 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2119 - acc: 0.4718 - precision: 0.1211 - recall: 1.2791 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7050 - acc: 0.5005 - precision: 0.1138 - recall: 1.1904 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2504 - acc: 0.4845 - precision: 0.1291 - recall: 1.2647 - f1_metric: 0.2293 - val_loss: 0.9305 - val_acc: 0.8966 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8412 - acc: 0.5045 - precision: 0.1204 - recall: 1.2671 - f1_metric: 0.2142 - val_loss: 0.9305 - val_acc: 0.0771 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7414 - acc: 0.4957 - precision: 0.1176 - recall: 1.2799 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.8960 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8487 - acc: 0.4934 - precision: 0.1245 - recall: 1.2280 - f1_metric: 0.2212 - val_loss: 0.9305 - val_acc: 0.8299 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8945 - acc: 0.5281 - precision: 0.1272 - recall: 1.2862 - f1_metric: 0.2272 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7958 - acc: 0.4835 - precision: 0.1273 - recall: 1.2621 - f1_metric: 0.2264 - val_loss: 0.9305 - val_acc: 0.0685 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8225 - acc: 0.4945 - precision: 0.1216 - recall: 1.2240 - f1_metric: 0.2162 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8969 - acc: 0.4755 - precision: 0.1191 - recall: 1.2640 - f1_metric: 0.2141 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7173 - acc: 0.4784 - precision: 0.1196 - recall: 1.2361 - f1_metric: 0.2141 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3847 - acc: 0.4811 - precision: 0.1157 - recall: 1.2231 - f1_metric: 0.2066 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8131 - acc: 0.4955 - precision: 0.1208 - recall: 1.3159 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.0826 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7435 - acc: 0.4727 - precision: 0.1104 - recall: 1.2478 - f1_metric: 0.1989 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1851 - acc: 0.4963 - precision: 0.1226 - recall: 1.2676 - f1_metric: 0.2184 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1131 - acc: 0.4779 - precision: 0.1179 - recall: 1.3033 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.8788 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9071 - acc: 0.5058 - precision: 0.1197 - recall: 1.2340 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.8647 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7789 - acc: 0.4793 - precision: 0.1133 - recall: 1.2292 - f1_metric: 0.2033 - val_loss: 0.9305 - val_acc: 0.0838 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6903 - acc: 0.4957 - precision: 0.1187 - recall: 1.2541 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7581 - acc: 0.4786 - precision: 0.1174 - recall: 1.3046 - f1_metric: 0.2112 - val_loss: 0.9305 - val_acc: 0.8782 - val_precision: 0.1311 - val_recall: 1.3001 - val_f1_metric: 0.2307\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7417 - acc: 0.4968 - precision: 0.1179 - recall: 1.2429 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.8886 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5479 - acc: 0.4930 - precision: 0.1163 - recall: 1.2551 - f1_metric: 0.2092 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9971 - acc: 0.4673 - precision: 0.1127 - recall: 1.2154 - f1_metric: 0.2021 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8755 - acc: 0.4821 - precision: 0.1279 - recall: 1.2276 - f1_metric: 0.2279 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9026 - acc: 0.4793 - precision: 0.1177 - recall: 1.2270 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.8886 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7738 - acc: 0.4906 - precision: 0.1136 - recall: 1.2591 - f1_metric: 0.2041 - val_loss: 0.9305 - val_acc: 0.8770 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8975 - acc: 0.4846 - precision: 0.1160 - recall: 1.2269 - f1_metric: 0.2078 - val_loss: 0.9305 - val_acc: 0.9015 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7453 - acc: 0.4963 - precision: 0.1097 - recall: 1.2111 - f1_metric: 0.1974 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0112 - acc: 0.4731 - precision: 0.1232 - recall: 1.2859 - f1_metric: 0.2203 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 10 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.8347 - acc: 0.4989 - precision: 0.1110 - recall: 1.2024 - f1_metric: 0.1987 - val_loss: 0.9305 - val_acc: 0.1089 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8580 - acc: 0.4873 - precision: 0.1211 - recall: 1.3100 - f1_metric: 0.2180 - val_loss: 0.9305 - val_acc: 0.8776 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8636 - acc: 0.4686 - precision: 0.1188 - recall: 1.2019 - f1_metric: 0.2121 - val_loss: 0.9305 - val_acc: 0.8629 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1246 - acc: 0.4807 - precision: 0.1223 - recall: 1.2826 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.8482 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7754 - acc: 0.4906 - precision: 0.1140 - recall: 1.2200 - f1_metric: 0.2048 - val_loss: 0.9305 - val_acc: 0.8641 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7367 - acc: 0.4701 - precision: 0.1225 - recall: 1.2802 - f1_metric: 0.2198 - val_loss: 0.9305 - val_acc: 0.8341 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6564 - acc: 0.5118 - precision: 0.1182 - recall: 1.2451 - f1_metric: 0.2122 - val_loss: 0.9305 - val_acc: 0.7662 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6924 - acc: 0.5062 - precision: 0.1127 - recall: 1.2276 - f1_metric: 0.2026 - val_loss: 0.9305 - val_acc: 0.1805 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8853 - acc: 0.4882 - precision: 0.1264 - recall: 1.2426 - f1_metric: 0.2246 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6590 - acc: 0.4768 - precision: 0.1079 - recall: 1.1968 - f1_metric: 0.1940 - val_loss: 0.9305 - val_acc: 0.2472 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8625 - acc: 0.5016 - precision: 0.1184 - recall: 1.2387 - f1_metric: 0.2117 - val_loss: 0.9305 - val_acc: 0.1408 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7502 - acc: 0.4911 - precision: 0.1192 - recall: 1.2609 - f1_metric: 0.2139 - val_loss: 0.9305 - val_acc: 0.3237 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8864 - acc: 0.4900 - precision: 0.1158 - recall: 1.2552 - f1_metric: 0.2087 - val_loss: 0.9305 - val_acc: 0.1420 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6204 - acc: 0.5020 - precision: 0.1208 - recall: 1.2466 - f1_metric: 0.2167 - val_loss: 0.9305 - val_acc: 0.9009 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9876 - acc: 0.4895 - precision: 0.1204 - recall: 1.2099 - f1_metric: 0.2163 - val_loss: 0.9305 - val_acc: 0.8525 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0344 - acc: 0.5134 - precision: 0.1306 - recall: 1.3228 - f1_metric: 0.2330 - val_loss: 0.9305 - val_acc: 0.8758 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9275 - acc: 0.4943 - precision: 0.1155 - recall: 1.2219 - f1_metric: 0.2063 - val_loss: 0.9305 - val_acc: 0.0747 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7591 - acc: 0.4806 - precision: 0.1180 - recall: 1.2290 - f1_metric: 0.2113 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8159 - acc: 0.5043 - precision: 0.1162 - recall: 1.1807 - f1_metric: 0.2078 - val_loss: 0.9305 - val_acc: 0.8641 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8244 - acc: 0.5011 - precision: 0.1228 - recall: 1.2661 - f1_metric: 0.2194 - val_loss: 0.9305 - val_acc: 0.6628 - val_precision: 0.1308 - val_recall: 1.2911 - val_f1_metric: 0.2300\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7641 - acc: 0.5018 - precision: 0.1186 - recall: 1.1967 - f1_metric: 0.2115 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8080 - acc: 0.4741 - precision: 0.1140 - recall: 1.2554 - f1_metric: 0.2053 - val_loss: 0.9305 - val_acc: 0.8525 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9896 - acc: 0.4977 - precision: 0.1199 - recall: 1.2668 - f1_metric: 0.2150 - val_loss: 0.9305 - val_acc: 0.7417 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8813 - acc: 0.5019 - precision: 0.1186 - recall: 1.2451 - f1_metric: 0.2122 - val_loss: 0.9305 - val_acc: 0.0912 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7293 - acc: 0.4894 - precision: 0.1156 - recall: 1.2965 - f1_metric: 0.2069 - val_loss: 0.9305 - val_acc: 0.8880 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0127 - acc: 0.5100 - precision: 0.1247 - recall: 1.2550 - f1_metric: 0.2232 - val_loss: 0.9305 - val_acc: 0.1457 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8517 - acc: 0.4793 - precision: 0.1169 - recall: 1.2055 - f1_metric: 0.2100 - val_loss: 0.9305 - val_acc: 0.4890 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2541 - acc: 0.4954 - precision: 0.1162 - recall: 1.2128 - f1_metric: 0.2084 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7776 - acc: 0.4778 - precision: 0.1177 - recall: 1.2898 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9078 - acc: 0.4841 - precision: 0.1235 - recall: 1.2377 - f1_metric: 0.2192 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0232 - acc: 0.4831 - precision: 0.1219 - recall: 1.2341 - f1_metric: 0.2175 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9857 - acc: 0.4938 - precision: 0.1160 - recall: 1.2616 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7251 - acc: 0.5069 - precision: 0.1242 - recall: 1.2775 - f1_metric: 0.2205 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6602 - acc: 0.4604 - precision: 0.1231 - recall: 1.2351 - f1_metric: 0.2195 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6528 - acc: 0.4699 - precision: 0.1179 - recall: 1.2206 - f1_metric: 0.2109 - val_loss: 0.9305 - val_acc: 0.8782 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0957 - acc: 0.5011 - precision: 0.1144 - recall: 1.1993 - f1_metric: 0.2049 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8831 - acc: 0.4963 - precision: 0.1264 - recall: 1.2096 - f1_metric: 0.2250 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8756 - acc: 0.4964 - precision: 0.1171 - recall: 1.2437 - f1_metric: 0.2101 - val_loss: 0.9305 - val_acc: 0.0685 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1238 - acc: 0.4768 - precision: 0.1204 - recall: 1.2772 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.2681 - val_precision: 0.1313 - val_recall: 1.2991 - val_f1_metric: 0.2310\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0660 - acc: 0.4579 - precision: 0.1307 - recall: 1.2802 - f1_metric: 0.2327 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0038 - acc: 0.4729 - precision: 0.1145 - recall: 1.2590 - f1_metric: 0.2058 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8567 - acc: 0.4766 - precision: 0.1134 - recall: 1.2128 - f1_metric: 0.2032 - val_loss: 0.9305 - val_acc: 0.6432 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7451 - acc: 0.5047 - precision: 0.1279 - recall: 1.2384 - f1_metric: 0.2277 - val_loss: 0.9305 - val_acc: 0.8990 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0486 - acc: 0.4873 - precision: 0.1209 - recall: 1.2866 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.0808 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6800 - acc: 0.4711 - precision: 0.1242 - recall: 1.2521 - f1_metric: 0.2222 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0148 - acc: 0.4850 - precision: 0.1253 - recall: 1.2110 - f1_metric: 0.2235 - val_loss: 0.9305 - val_acc: 0.1524 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9841 - acc: 0.4729 - precision: 0.1201 - recall: 1.2641 - f1_metric: 0.2147 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6640 - acc: 0.4768 - precision: 0.1137 - recall: 1.2678 - f1_metric: 0.2046 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0855 - acc: 0.4831 - precision: 0.1193 - recall: 1.1994 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.1108 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0826 - acc: 0.4759 - precision: 0.1247 - recall: 1.2647 - f1_metric: 0.2225 - val_loss: 0.9305 - val_acc: 0.4400 - val_precision: 0.1314 - val_recall: 1.2927 - val_f1_metric: 0.2309\n",
            "Model 11 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.9858 - acc: 0.4869 - precision: 0.1173 - recall: 1.2491 - f1_metric: 0.2098 - val_loss: 0.9305 - val_acc: 0.7203 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0803 - acc: 0.4878 - precision: 0.1278 - recall: 1.2482 - f1_metric: 0.2284 - val_loss: 0.9305 - val_acc: 0.8409 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8797 - acc: 0.4821 - precision: 0.1191 - recall: 1.2486 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.8427 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8117 - acc: 0.4787 - precision: 0.1251 - recall: 1.1940 - f1_metric: 0.2223 - val_loss: 0.9305 - val_acc: 0.8360 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0538 - acc: 0.4745 - precision: 0.1271 - recall: 1.2515 - f1_metric: 0.2267 - val_loss: 0.9305 - val_acc: 0.8953 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9563 - acc: 0.4911 - precision: 0.1262 - recall: 1.2531 - f1_metric: 0.2248 - val_loss: 0.9305 - val_acc: 0.7901 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7354 - acc: 0.4834 - precision: 0.1208 - recall: 1.2450 - f1_metric: 0.2156 - val_loss: 0.9305 - val_acc: 0.8562 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1643 - acc: 0.4704 - precision: 0.1274 - recall: 1.2907 - f1_metric: 0.2275 - val_loss: 0.9305 - val_acc: 0.8091 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9817 - acc: 0.4886 - precision: 0.1199 - recall: 1.1850 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.8237 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7303 - acc: 0.4889 - precision: 0.1219 - recall: 1.2634 - f1_metric: 0.2185 - val_loss: 0.9305 - val_acc: 0.8103 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8870 - acc: 0.4833 - precision: 0.1216 - recall: 1.2172 - f1_metric: 0.2171 - val_loss: 0.9305 - val_acc: 0.7644 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6981 - acc: 0.4943 - precision: 0.1115 - recall: 1.2440 - f1_metric: 0.2012 - val_loss: 0.9305 - val_acc: 0.7619 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6222 - acc: 0.4986 - precision: 0.1214 - recall: 1.2852 - f1_metric: 0.2174 - val_loss: 0.9305 - val_acc: 0.6952 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1139 - acc: 0.4732 - precision: 0.1183 - recall: 1.1985 - f1_metric: 0.2118 - val_loss: 0.9305 - val_acc: 0.6512 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9616 - acc: 0.4814 - precision: 0.1267 - recall: 1.3058 - f1_metric: 0.2269 - val_loss: 0.9305 - val_acc: 0.7650 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8280 - acc: 0.5075 - precision: 0.1180 - recall: 1.1973 - f1_metric: 0.2109 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7647 - acc: 0.5020 - precision: 0.1271 - recall: 1.2065 - f1_metric: 0.2261 - val_loss: 0.9305 - val_acc: 0.2699 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0236 - acc: 0.4792 - precision: 0.1224 - recall: 1.2113 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.2160 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5765 - acc: 0.4789 - precision: 0.1085 - recall: 1.1510 - f1_metric: 0.1939 - val_loss: 0.9305 - val_acc: 0.1065 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7956 - acc: 0.4745 - precision: 0.1127 - recall: 1.1928 - f1_metric: 0.2026 - val_loss: 0.9305 - val_acc: 0.0728 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7449 - acc: 0.4729 - precision: 0.1222 - recall: 1.2211 - f1_metric: 0.2169 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9592 - acc: 0.4916 - precision: 0.1153 - recall: 1.2458 - f1_metric: 0.2072 - val_loss: 0.9305 - val_acc: 0.9015 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8475 - acc: 0.4956 - precision: 0.1166 - recall: 1.2313 - f1_metric: 0.2098 - val_loss: 0.9305 - val_acc: 0.8905 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7014 - acc: 0.4915 - precision: 0.1206 - recall: 1.2669 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.0967 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.4384 - acc: 0.4978 - precision: 0.1169 - recall: 1.2373 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.7564 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8388 - acc: 0.5033 - precision: 0.1271 - recall: 1.2414 - f1_metric: 0.2256 - val_loss: 0.9305 - val_acc: 0.8574 - val_precision: 0.1311 - val_recall: 1.2991 - val_f1_metric: 0.2307\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7772 - acc: 0.5111 - precision: 0.1239 - recall: 1.2557 - f1_metric: 0.2218 - val_loss: 0.9305 - val_acc: 0.0704 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8638 - acc: 0.4726 - precision: 0.1163 - recall: 1.2415 - f1_metric: 0.2087 - val_loss: 0.9305 - val_acc: 0.0949 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8589 - acc: 0.4730 - precision: 0.1079 - recall: 1.1621 - f1_metric: 0.1937 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9095 - acc: 0.4948 - precision: 0.1202 - recall: 1.2459 - f1_metric: 0.2157 - val_loss: 0.9305 - val_acc: 0.8647 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6593 - acc: 0.4955 - precision: 0.1264 - recall: 1.2398 - f1_metric: 0.2243 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7335 - acc: 0.4712 - precision: 0.1257 - recall: 1.2607 - f1_metric: 0.2241 - val_loss: 0.9305 - val_acc: 0.8758 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8143 - acc: 0.4962 - precision: 0.1176 - recall: 1.2350 - f1_metric: 0.2097 - val_loss: 0.9305 - val_acc: 0.8898 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6470 - acc: 0.5117 - precision: 0.1159 - recall: 1.1795 - f1_metric: 0.2064 - val_loss: 0.9305 - val_acc: 0.0796 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1133 - acc: 0.4967 - precision: 0.1156 - recall: 1.2805 - f1_metric: 0.2076 - val_loss: 0.9305 - val_acc: 0.1126 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0359 - acc: 0.4710 - precision: 0.1101 - recall: 1.1604 - f1_metric: 0.1974 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2690 - acc: 0.4782 - precision: 0.1171 - recall: 1.2488 - f1_metric: 0.2103 - val_loss: 0.9305 - val_acc: 0.1016 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7400 - acc: 0.4778 - precision: 0.1182 - recall: 1.2399 - f1_metric: 0.2115 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0512 - acc: 0.4778 - precision: 0.1074 - recall: 1.2327 - f1_metric: 0.1942 - val_loss: 0.9305 - val_acc: 0.8923 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7124 - acc: 0.5163 - precision: 0.1248 - recall: 1.2354 - f1_metric: 0.2227 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8791 - acc: 0.4667 - precision: 0.1139 - recall: 1.2719 - f1_metric: 0.2042 - val_loss: 0.9305 - val_acc: 0.8996 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7588 - acc: 0.5153 - precision: 0.1174 - recall: 1.2548 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.1004 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0328 - acc: 0.4644 - precision: 0.1179 - recall: 1.2633 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0391 - acc: 0.4944 - precision: 0.1175 - recall: 1.2241 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7762 - acc: 0.4902 - precision: 0.1177 - recall: 1.2758 - f1_metric: 0.2109 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7168 - acc: 0.4759 - precision: 0.1211 - recall: 1.2678 - f1_metric: 0.2174 - val_loss: 0.9305 - val_acc: 0.0722 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8010 - acc: 0.4862 - precision: 0.1271 - recall: 1.2022 - f1_metric: 0.2248 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9098 - acc: 0.4823 - precision: 0.1160 - recall: 1.2035 - f1_metric: 0.2078 - val_loss: 0.9305 - val_acc: 0.8690 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2093 - acc: 0.4946 - precision: 0.1240 - recall: 1.2481 - f1_metric: 0.2217 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7116 - acc: 0.4879 - precision: 0.1197 - recall: 1.2602 - f1_metric: 0.2150 - val_loss: 0.9305 - val_acc: 0.8868 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 12 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.8551 - acc: 0.5018 - precision: 0.1183 - recall: 1.2805 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.1236 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8104 - acc: 0.5063 - precision: 0.1174 - recall: 1.2531 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.8439 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6162 - acc: 0.5179 - precision: 0.1055 - recall: 1.2013 - f1_metric: 0.1903 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8925 - acc: 0.5359 - precision: 0.1216 - recall: 1.2472 - f1_metric: 0.2177 - val_loss: 0.9305 - val_acc: 0.0906 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9729 - acc: 0.4692 - precision: 0.1134 - recall: 1.1868 - f1_metric: 0.2027 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8164 - acc: 0.5144 - precision: 0.1118 - recall: 1.2620 - f1_metric: 0.2017 - val_loss: 0.9305 - val_acc: 0.8678 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7605 - acc: 0.5062 - precision: 0.1185 - recall: 1.2172 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.1891 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9965 - acc: 0.5149 - precision: 0.1115 - recall: 1.2487 - f1_metric: 0.2010 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3672 - acc: 0.5098 - precision: 0.1309 - recall: 1.3084 - f1_metric: 0.2328 - val_loss: 0.9305 - val_acc: 0.8905 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1854 - acc: 0.5312 - precision: 0.1256 - recall: 1.2621 - f1_metric: 0.2244 - val_loss: 0.9305 - val_acc: 0.4045 - val_precision: 0.1312 - val_recall: 1.2847 - val_f1_metric: 0.2308\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6757 - acc: 0.5010 - precision: 0.1150 - recall: 1.2123 - f1_metric: 0.2062 - val_loss: 0.9305 - val_acc: 0.2136 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9343 - acc: 0.4850 - precision: 0.1147 - recall: 1.2293 - f1_metric: 0.2053 - val_loss: 0.9305 - val_acc: 0.0753 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9054 - acc: 0.4865 - precision: 0.1186 - recall: 1.2669 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7940 - acc: 0.4770 - precision: 0.1220 - recall: 1.2756 - f1_metric: 0.2184 - val_loss: 0.9305 - val_acc: 0.7466 - val_precision: 0.1321 - val_recall: 1.3040 - val_f1_metric: 0.2323\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8633 - acc: 0.4898 - precision: 0.1208 - recall: 1.2428 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.7950 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8669 - acc: 0.4806 - precision: 0.1216 - recall: 1.2485 - f1_metric: 0.2173 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7951 - acc: 0.4773 - precision: 0.1177 - recall: 1.2611 - f1_metric: 0.2112 - val_loss: 0.9305 - val_acc: 0.0930 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9970 - acc: 0.4852 - precision: 0.1197 - recall: 1.2382 - f1_metric: 0.2140 - val_loss: 0.9305 - val_acc: 0.1365 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2325\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1025 - acc: 0.4776 - precision: 0.1200 - recall: 1.2858 - f1_metric: 0.2156 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7831 - acc: 0.4920 - precision: 0.1129 - recall: 1.2917 - f1_metric: 0.2034 - val_loss: 0.9305 - val_acc: 0.0863 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9570 - acc: 0.4942 - precision: 0.1199 - recall: 1.2102 - f1_metric: 0.2128 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6826 - acc: 0.4941 - precision: 0.1179 - recall: 1.1774 - f1_metric: 0.2107 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6854 - acc: 0.4815 - precision: 0.1167 - recall: 1.2580 - f1_metric: 0.2089 - val_loss: 0.9305 - val_acc: 0.9002 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0525 - acc: 0.5178 - precision: 0.1166 - recall: 1.2508 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6686 - acc: 0.5004 - precision: 0.1203 - recall: 1.2624 - f1_metric: 0.2153 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1092 - acc: 0.4676 - precision: 0.1164 - recall: 1.2528 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7071 - acc: 0.4681 - precision: 0.1189 - recall: 1.2321 - f1_metric: 0.2129 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7035 - acc: 0.4935 - precision: 0.1152 - recall: 1.1841 - f1_metric: 0.2055 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7813 - acc: 0.4812 - precision: 0.1150 - recall: 1.2519 - f1_metric: 0.2061 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7081 - acc: 0.4728 - precision: 0.1223 - recall: 1.2812 - f1_metric: 0.2181 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8309 - acc: 0.4772 - precision: 0.1250 - recall: 1.2323 - f1_metric: 0.2227 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9402 - acc: 0.5070 - precision: 0.1164 - recall: 1.2742 - f1_metric: 0.2088 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8491 - acc: 0.4985 - precision: 0.1225 - recall: 1.2415 - f1_metric: 0.2189 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6876 - acc: 0.4843 - precision: 0.1159 - recall: 1.2277 - f1_metric: 0.2076 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0858 - acc: 0.4718 - precision: 0.1179 - recall: 1.1712 - f1_metric: 0.2112 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9452 - acc: 0.4799 - precision: 0.1218 - recall: 1.2556 - f1_metric: 0.2186 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7377 - acc: 0.4884 - precision: 0.1178 - recall: 1.2037 - f1_metric: 0.2108 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8794 - acc: 0.4538 - precision: 0.1155 - recall: 1.2793 - f1_metric: 0.2072 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8887 - acc: 0.4733 - precision: 0.1213 - recall: 1.2338 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7972 - acc: 0.4979 - precision: 0.1207 - recall: 1.2197 - f1_metric: 0.2150 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7597 - acc: 0.4885 - precision: 0.1244 - recall: 1.1709 - f1_metric: 0.2207 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9735 - acc: 0.4884 - precision: 0.1242 - recall: 1.1691 - f1_metric: 0.2204 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0299 - acc: 0.4905 - precision: 0.1204 - recall: 1.2814 - f1_metric: 0.2169 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8783 - acc: 0.4895 - precision: 0.1157 - recall: 1.2350 - f1_metric: 0.2075 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0117 - acc: 0.4636 - precision: 0.1200 - recall: 1.2432 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7693 - acc: 0.4907 - precision: 0.1143 - recall: 1.2287 - f1_metric: 0.2060 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7400 - acc: 0.4799 - precision: 0.1242 - recall: 1.2815 - f1_metric: 0.2212 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8293 - acc: 0.4930 - precision: 0.1145 - recall: 1.1938 - f1_metric: 0.2043 - val_loss: 0.9305 - val_acc: 0.3146 - val_precision: 0.1324 - val_recall: 1.3040 - val_f1_metric: 0.2329\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8961 - acc: 0.4868 - precision: 0.1198 - recall: 1.1942 - f1_metric: 0.2139 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8258 - acc: 0.4930 - precision: 0.1161 - recall: 1.1824 - f1_metric: 0.2077 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 13 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.6248 - acc: 0.4683 - precision: 0.1164 - recall: 1.2793 - f1_metric: 0.2091 - val_loss: 0.9305 - val_acc: 0.2613 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6909 - acc: 0.4587 - precision: 0.1132 - recall: 1.2063 - f1_metric: 0.2026 - val_loss: 0.9305 - val_acc: 0.8807 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9344 - acc: 0.4784 - precision: 0.1244 - recall: 1.2541 - f1_metric: 0.2225 - val_loss: 0.9305 - val_acc: 0.7913 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0232 - acc: 0.4978 - precision: 0.1141 - recall: 1.2533 - f1_metric: 0.2041 - val_loss: 0.9305 - val_acc: 0.6603 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7606 - acc: 0.4890 - precision: 0.1169 - recall: 1.2180 - f1_metric: 0.2097 - val_loss: 0.9305 - val_acc: 0.2442 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8589 - acc: 0.4754 - precision: 0.1245 - recall: 1.3056 - f1_metric: 0.2230 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8966 - acc: 0.4821 - precision: 0.1195 - recall: 1.2342 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.8788 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6299 - acc: 0.4857 - precision: 0.1155 - recall: 1.2651 - f1_metric: 0.2068 - val_loss: 0.9305 - val_acc: 0.5196 - val_precision: 0.1321 - val_recall: 1.3040 - val_f1_metric: 0.2324\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7504 - acc: 0.5100 - precision: 0.1194 - recall: 1.2384 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.1371 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5968 - acc: 0.4922 - precision: 0.1179 - recall: 1.2166 - f1_metric: 0.2116 - val_loss: 0.9305 - val_acc: 0.1971 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9240 - acc: 0.4917 - precision: 0.1217 - recall: 1.2621 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.1793 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7870 - acc: 0.4938 - precision: 0.1324 - recall: 1.2280 - f1_metric: 0.2351 - val_loss: 0.9305 - val_acc: 0.8635 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9363 - acc: 0.5179 - precision: 0.1227 - recall: 1.2105 - f1_metric: 0.2186 - val_loss: 0.9305 - val_acc: 0.2099 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9339 - acc: 0.4943 - precision: 0.1187 - recall: 1.2281 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.1671 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0973 - acc: 0.4866 - precision: 0.1165 - recall: 1.2808 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1856 - acc: 0.4984 - precision: 0.1227 - recall: 1.1938 - f1_metric: 0.2175 - val_loss: 0.9305 - val_acc: 0.8953 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8812 - acc: 0.4818 - precision: 0.1300 - recall: 1.2228 - f1_metric: 0.2292 - val_loss: 0.9305 - val_acc: 0.2038 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6887 - acc: 0.4828 - precision: 0.1109 - recall: 1.2246 - f1_metric: 0.1998 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8748 - acc: 0.4763 - precision: 0.1206 - recall: 1.3098 - f1_metric: 0.2166 - val_loss: 0.9305 - val_acc: 0.2515 - val_precision: 0.1310 - val_recall: 1.2943 - val_f1_metric: 0.2305\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9626 - acc: 0.4860 - precision: 0.1266 - recall: 1.2200 - f1_metric: 0.2251 - val_loss: 0.9305 - val_acc: 0.7271 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6718 - acc: 0.4722 - precision: 0.1275 - recall: 1.2532 - f1_metric: 0.2262 - val_loss: 0.9305 - val_acc: 0.8745 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7619 - acc: 0.4940 - precision: 0.1222 - recall: 1.2288 - f1_metric: 0.2173 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1242 - acc: 0.4831 - precision: 0.1202 - recall: 1.2799 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.1824 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8352 - acc: 0.4833 - precision: 0.1162 - recall: 1.2133 - f1_metric: 0.2080 - val_loss: 0.9305 - val_acc: 0.1242 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5983 - acc: 0.4768 - precision: 0.1185 - recall: 1.2677 - f1_metric: 0.2119 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1473 - acc: 0.4769 - precision: 0.1137 - recall: 1.2381 - f1_metric: 0.2034 - val_loss: 0.9305 - val_acc: 0.0685 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7205 - acc: 0.4791 - precision: 0.1210 - recall: 1.2571 - f1_metric: 0.2169 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7694 - acc: 0.4909 - precision: 0.1233 - recall: 1.2364 - f1_metric: 0.2193 - val_loss: 0.9305 - val_acc: 0.5814 - val_precision: 0.1313 - val_recall: 1.2943 - val_f1_metric: 0.2309\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8044 - acc: 0.5113 - precision: 0.1197 - recall: 1.2132 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8568 - acc: 0.4933 - precision: 0.1154 - recall: 1.2254 - f1_metric: 0.2080 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6767 - acc: 0.4858 - precision: 0.1182 - recall: 1.2017 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.1903 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9011 - acc: 0.4791 - precision: 0.1207 - recall: 1.2358 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9368 - acc: 0.4851 - precision: 0.1141 - recall: 1.2450 - f1_metric: 0.2049 - val_loss: 0.9305 - val_acc: 0.3641 - val_precision: 0.1312 - val_recall: 1.2943 - val_f1_metric: 0.2307\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8533 - acc: 0.4932 - precision: 0.1143 - recall: 1.2273 - f1_metric: 0.2045 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9133 - acc: 0.4991 - precision: 0.1217 - recall: 1.2741 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7597 - acc: 0.4789 - precision: 0.1166 - recall: 1.1996 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.8696 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8864 - acc: 0.4909 - precision: 0.1123 - recall: 1.2445 - f1_metric: 0.2017 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8251 - acc: 0.4928 - precision: 0.1152 - recall: 1.2263 - f1_metric: 0.2063 - val_loss: 0.9305 - val_acc: 0.5416 - val_precision: 0.1326 - val_recall: 1.3040 - val_f1_metric: 0.2332\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8716 - acc: 0.4829 - precision: 0.1200 - recall: 1.2863 - f1_metric: 0.2156 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0192 - acc: 0.4850 - precision: 0.1227 - recall: 1.2094 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.1481 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6143 - acc: 0.4961 - precision: 0.1220 - recall: 1.2249 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.0820 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8372 - acc: 0.5051 - precision: 0.1223 - recall: 1.2320 - f1_metric: 0.2185 - val_loss: 0.9305 - val_acc: 0.5490 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2326\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7240 - acc: 0.4844 - precision: 0.1133 - recall: 1.2132 - f1_metric: 0.2032 - val_loss: 0.9305 - val_acc: 0.3978 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7592 - acc: 0.4894 - precision: 0.1228 - recall: 1.2235 - f1_metric: 0.2190 - val_loss: 0.9305 - val_acc: 0.3317 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6680 - acc: 0.4847 - precision: 0.1135 - recall: 1.1757 - f1_metric: 0.2035 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8803 - acc: 0.5111 - precision: 0.1230 - recall: 1.2006 - f1_metric: 0.2192 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0408 - acc: 0.4943 - precision: 0.1219 - recall: 1.2783 - f1_metric: 0.2195 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0370 - acc: 0.4879 - precision: 0.1086 - recall: 1.1468 - f1_metric: 0.1943 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2199 - acc: 0.4678 - precision: 0.1219 - recall: 1.2174 - f1_metric: 0.2167 - val_loss: 0.9305 - val_acc: 0.9015 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1509 - acc: 0.5064 - precision: 0.1161 - recall: 1.2167 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 14 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.9570 - acc: 0.4965 - precision: 0.1279 - recall: 1.2447 - f1_metric: 0.2272 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7823 - acc: 0.4817 - precision: 0.1168 - recall: 1.2461 - f1_metric: 0.2098 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1953 - acc: 0.4831 - precision: 0.1161 - recall: 1.2599 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.8953 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9772 - acc: 0.4825 - precision: 0.1175 - recall: 1.2508 - f1_metric: 0.2098 - val_loss: 0.9305 - val_acc: 0.8856 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1422 - acc: 0.5012 - precision: 0.1189 - recall: 1.2268 - f1_metric: 0.2123 - val_loss: 0.9305 - val_acc: 0.0734 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5887 - acc: 0.4700 - precision: 0.1149 - recall: 1.2246 - f1_metric: 0.2065 - val_loss: 0.9305 - val_acc: 0.0789 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1424 - acc: 0.4603 - precision: 0.1162 - recall: 1.2628 - f1_metric: 0.2080 - val_loss: 0.9305 - val_acc: 0.9015 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7887 - acc: 0.5260 - precision: 0.1139 - recall: 1.2171 - f1_metric: 0.2041 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8874 - acc: 0.4796 - precision: 0.1178 - recall: 1.2399 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8562 - acc: 0.4965 - precision: 0.1220 - recall: 1.2555 - f1_metric: 0.2185 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7769 - acc: 0.4954 - precision: 0.1205 - recall: 1.2365 - f1_metric: 0.2155 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6783 - acc: 0.5050 - precision: 0.1226 - recall: 1.2316 - f1_metric: 0.2182 - val_loss: 0.9305 - val_acc: 0.0924 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0460 - acc: 0.4784 - precision: 0.1281 - recall: 1.2365 - f1_metric: 0.2282 - val_loss: 0.9305 - val_acc: 0.7430 - val_precision: 0.1344 - val_recall: 1.2970 - val_f1_metric: 0.2359\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7829 - acc: 0.5119 - precision: 0.1186 - recall: 1.2267 - f1_metric: 0.2125 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8172 - acc: 0.5143 - precision: 0.1090 - recall: 1.2289 - f1_metric: 0.1956 - val_loss: 0.9305 - val_acc: 0.8947 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9066 - acc: 0.4962 - precision: 0.1211 - recall: 1.2431 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5972 - acc: 0.4599 - precision: 0.1161 - recall: 1.2585 - f1_metric: 0.2087 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9927 - acc: 0.4801 - precision: 0.1216 - recall: 1.2668 - f1_metric: 0.2184 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8038 - acc: 0.4679 - precision: 0.1248 - recall: 1.2177 - f1_metric: 0.2210 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7966 - acc: 0.5181 - precision: 0.1227 - recall: 1.2559 - f1_metric: 0.2197 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7775 - acc: 0.5033 - precision: 0.1238 - recall: 1.2681 - f1_metric: 0.2213 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.6010 - acc: 0.4917 - precision: 0.1233 - recall: 1.3087 - f1_metric: 0.2191 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6992 - acc: 0.4897 - precision: 0.1165 - recall: 1.2319 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9080 - acc: 0.5157 - precision: 0.1203 - recall: 1.2258 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0088 - acc: 0.5357 - precision: 0.1254 - recall: 1.2595 - f1_metric: 0.2243 - val_loss: 0.9305 - val_acc: 0.8237 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8535 - acc: 0.4597 - precision: 0.1199 - recall: 1.2192 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7428 - acc: 0.4807 - precision: 0.1199 - recall: 1.2399 - f1_metric: 0.2143 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8126 - acc: 0.4615 - precision: 0.1126 - recall: 1.2467 - f1_metric: 0.2028 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7768 - acc: 0.5032 - precision: 0.1173 - recall: 1.2738 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7538 - acc: 0.4863 - precision: 0.1220 - recall: 1.2749 - f1_metric: 0.2192 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9404 - acc: 0.4782 - precision: 0.1247 - recall: 1.2465 - f1_metric: 0.2224 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8807 - acc: 0.4877 - precision: 0.1195 - recall: 1.2291 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9471 - acc: 0.4880 - precision: 0.1187 - recall: 1.2047 - f1_metric: 0.2116 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9094 - acc: 0.5000 - precision: 0.1197 - recall: 1.2268 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8069 - acc: 0.5217 - precision: 0.1268 - recall: 1.2503 - f1_metric: 0.2265 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0695 - acc: 0.4839 - precision: 0.1137 - recall: 1.2591 - f1_metric: 0.2047 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1670 - acc: 0.5042 - precision: 0.1208 - recall: 1.2545 - f1_metric: 0.2157 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1099 - acc: 0.4757 - precision: 0.1212 - recall: 1.2026 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6704 - acc: 0.4719 - precision: 0.1096 - recall: 1.1876 - f1_metric: 0.1970 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0987 - acc: 0.4811 - precision: 0.1198 - recall: 1.2358 - f1_metric: 0.2146 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0561 - acc: 0.4958 - precision: 0.1205 - recall: 1.2212 - f1_metric: 0.2145 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8742 - acc: 0.4829 - precision: 0.1144 - recall: 1.2859 - f1_metric: 0.2056 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5177 - acc: 0.4873 - precision: 0.1150 - recall: 1.2616 - f1_metric: 0.2069 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6463 - acc: 0.4780 - precision: 0.1135 - recall: 1.1652 - f1_metric: 0.2031 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1862 - acc: 0.5029 - precision: 0.1170 - recall: 1.2211 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6119 - acc: 0.4992 - precision: 0.1134 - recall: 1.1672 - f1_metric: 0.2025 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8146 - acc: 0.5052 - precision: 0.1181 - recall: 1.2325 - f1_metric: 0.2125 - val_loss: 0.9305 - val_acc: 0.8905 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0201 - acc: 0.4997 - precision: 0.1250 - recall: 1.2994 - f1_metric: 0.2234 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9132 - acc: 0.4992 - precision: 0.1127 - recall: 1.2271 - f1_metric: 0.2028 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9407 - acc: 0.4628 - precision: 0.1108 - recall: 1.1738 - f1_metric: 0.1982 - val_loss: 0.9305 - val_acc: 0.1897 - val_precision: 0.1315 - val_recall: 1.3018 - val_f1_metric: 0.2314\n",
            "Model 15 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 1.0910 - acc: 0.4943 - precision: 0.1241 - recall: 1.2500 - f1_metric: 0.2216 - val_loss: 0.9305 - val_acc: 0.3213 - val_precision: 0.1310 - val_recall: 1.2943 - val_f1_metric: 0.2305\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2675 - acc: 0.4714 - precision: 0.1256 - recall: 1.2250 - f1_metric: 0.2236 - val_loss: 0.9305 - val_acc: 0.0900 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2946 - acc: 0.4768 - precision: 0.1238 - recall: 1.2200 - f1_metric: 0.2205 - val_loss: 0.9305 - val_acc: 0.4559 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8092 - acc: 0.4825 - precision: 0.1196 - recall: 1.2227 - f1_metric: 0.2128 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9105 - acc: 0.4610 - precision: 0.1149 - recall: 1.2187 - f1_metric: 0.2066 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7691 - acc: 0.4589 - precision: 0.1184 - recall: 1.2506 - f1_metric: 0.2124 - val_loss: 0.9305 - val_acc: 0.6126 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2324\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6465 - acc: 0.4657 - precision: 0.1185 - recall: 1.2401 - f1_metric: 0.2116 - val_loss: 0.9305 - val_acc: 0.8519 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9436 - acc: 0.4690 - precision: 0.1186 - recall: 1.2481 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.9002 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7998 - acc: 0.4819 - precision: 0.1200 - recall: 1.2478 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.1163 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6010 - acc: 0.4812 - precision: 0.1169 - recall: 1.1894 - f1_metric: 0.2080 - val_loss: 0.9305 - val_acc: 0.5086 - val_precision: 0.1310 - val_recall: 1.2911 - val_f1_metric: 0.2303\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7942 - acc: 0.4762 - precision: 0.1274 - recall: 1.2700 - f1_metric: 0.2266 - val_loss: 0.9305 - val_acc: 0.0704 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6974 - acc: 0.4834 - precision: 0.1259 - recall: 1.2648 - f1_metric: 0.2247 - val_loss: 0.9305 - val_acc: 0.7931 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7152 - acc: 0.4911 - precision: 0.1162 - recall: 1.0889 - f1_metric: 0.2051 - val_loss: 0.9305 - val_acc: 0.8758 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9034 - acc: 0.4720 - precision: 0.1151 - recall: 1.2449 - f1_metric: 0.2072 - val_loss: 0.9305 - val_acc: 0.0887 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1322 - acc: 0.4777 - precision: 0.1206 - recall: 1.2418 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0845 - acc: 0.4832 - precision: 0.1255 - recall: 1.3016 - f1_metric: 0.2250 - val_loss: 0.9305 - val_acc: 0.5202 - val_precision: 0.1327 - val_recall: 1.3040 - val_f1_metric: 0.2333\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2198 - acc: 0.4651 - precision: 0.1257 - recall: 1.2364 - f1_metric: 0.2241 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0103 - acc: 0.4830 - precision: 0.1228 - recall: 1.2654 - f1_metric: 0.2189 - val_loss: 0.9305 - val_acc: 0.0936 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1153 - acc: 0.4767 - precision: 0.1255 - recall: 1.2735 - f1_metric: 0.2241 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9069 - acc: 0.4903 - precision: 0.1237 - recall: 1.1973 - f1_metric: 0.2205 - val_loss: 0.9305 - val_acc: 0.1787 - val_precision: 0.1319 - val_recall: 1.2879 - val_f1_metric: 0.2318\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9383 - acc: 0.4923 - precision: 0.1197 - recall: 1.2276 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7974 - acc: 0.4687 - precision: 0.1162 - recall: 1.2242 - f1_metric: 0.2091 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8377 - acc: 0.4631 - precision: 0.1210 - recall: 1.2365 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.1083 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7335 - acc: 0.4707 - precision: 0.1200 - recall: 1.2648 - f1_metric: 0.2145 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7088 - acc: 0.4760 - precision: 0.1175 - recall: 1.2107 - f1_metric: 0.2113 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6703 - acc: 0.4978 - precision: 0.1142 - recall: 1.2200 - f1_metric: 0.2045 - val_loss: 0.9305 - val_acc: 0.7160 - val_precision: 0.1344 - val_recall: 1.2756 - val_f1_metric: 0.2357\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8401 - acc: 0.4733 - precision: 0.1145 - recall: 1.2267 - f1_metric: 0.2050 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6360 - acc: 0.4862 - precision: 0.1139 - recall: 1.2290 - f1_metric: 0.2037 - val_loss: 0.9305 - val_acc: 0.8892 - val_precision: 0.1297 - val_recall: 1.2643 - val_f1_metric: 0.2280\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2080 - acc: 0.4948 - precision: 0.1236 - recall: 1.2871 - f1_metric: 0.2218 - val_loss: 0.9305 - val_acc: 0.1016 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8257 - acc: 0.4856 - precision: 0.1273 - recall: 1.2422 - f1_metric: 0.2266 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5964 - acc: 0.5040 - precision: 0.1196 - recall: 1.2510 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0071 - acc: 0.4735 - precision: 0.1161 - recall: 1.2547 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9687 - acc: 0.4859 - precision: 0.1233 - recall: 1.2170 - f1_metric: 0.2202 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5965 - acc: 0.4882 - precision: 0.1133 - recall: 1.2689 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0414 - acc: 0.4753 - precision: 0.1160 - recall: 1.1789 - f1_metric: 0.2071 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7786 - acc: 0.4723 - precision: 0.1238 - recall: 1.2689 - f1_metric: 0.2207 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8172 - acc: 0.4867 - precision: 0.1230 - recall: 1.2845 - f1_metric: 0.2208 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5846 - acc: 0.4971 - precision: 0.1185 - recall: 1.2287 - f1_metric: 0.2117 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9621 - acc: 0.4951 - precision: 0.1097 - recall: 1.2373 - f1_metric: 0.1973 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0251 - acc: 0.4684 - precision: 0.1223 - recall: 1.2130 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5915 - acc: 0.4901 - precision: 0.1190 - recall: 1.2993 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8048 - acc: 0.4814 - precision: 0.1137 - recall: 1.2854 - f1_metric: 0.2045 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7668 - acc: 0.4787 - precision: 0.1229 - recall: 1.2267 - f1_metric: 0.2181 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5781 - acc: 0.4878 - precision: 0.1125 - recall: 1.2338 - f1_metric: 0.2022 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8408 - acc: 0.4731 - precision: 0.1141 - recall: 1.2432 - f1_metric: 0.2059 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9120 - acc: 0.4998 - precision: 0.1103 - recall: 1.1835 - f1_metric: 0.1979 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8159 - acc: 0.5015 - precision: 0.1104 - recall: 1.1809 - f1_metric: 0.1986 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9877 - acc: 0.5046 - precision: 0.1254 - recall: 1.2437 - f1_metric: 0.2239 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9553 - acc: 0.4876 - precision: 0.1209 - recall: 1.2107 - f1_metric: 0.2155 - val_loss: 0.9305 - val_acc: 0.7509 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9522 - acc: 0.4758 - precision: 0.1120 - recall: 1.2181 - f1_metric: 0.2016 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 16 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.7260 - acc: 0.4990 - precision: 0.1230 - recall: 1.2568 - f1_metric: 0.2203 - val_loss: 0.9305 - val_acc: 0.1169 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1690 - acc: 0.5109 - precision: 0.1180 - recall: 1.1652 - f1_metric: 0.2096 - val_loss: 0.9305 - val_acc: 0.0728 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8021 - acc: 0.4772 - precision: 0.1176 - recall: 1.1985 - f1_metric: 0.2105 - val_loss: 0.9305 - val_acc: 0.0716 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.5987 - acc: 0.4569 - precision: 0.1123 - recall: 1.2922 - f1_metric: 0.2025 - val_loss: 0.9305 - val_acc: 0.1187 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0407 - acc: 0.4981 - precision: 0.1166 - recall: 1.2128 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.0728 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0907 - acc: 0.4781 - precision: 0.1181 - recall: 1.1943 - f1_metric: 0.2123 - val_loss: 0.9305 - val_acc: 0.1028 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0727 - acc: 0.4925 - precision: 0.1232 - recall: 1.2742 - f1_metric: 0.2201 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7401 - acc: 0.4909 - precision: 0.1140 - recall: 1.1962 - f1_metric: 0.2048 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7803 - acc: 0.4737 - precision: 0.1164 - recall: 1.2701 - f1_metric: 0.2091 - val_loss: 0.9305 - val_acc: 0.0728 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1492 - acc: 0.4872 - precision: 0.1203 - recall: 1.2432 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.0814 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8146 - acc: 0.4953 - precision: 0.1244 - recall: 1.2474 - f1_metric: 0.2225 - val_loss: 0.9305 - val_acc: 0.1040 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6451 - acc: 0.4814 - precision: 0.1193 - recall: 1.2692 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.0789 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7324 - acc: 0.4819 - precision: 0.1199 - recall: 1.2459 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.0930 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9363 - acc: 0.4832 - precision: 0.1247 - recall: 1.2590 - f1_metric: 0.2226 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8025 - acc: 0.4828 - precision: 0.1140 - recall: 1.2460 - f1_metric: 0.2052 - val_loss: 0.9305 - val_acc: 0.0789 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8308 - acc: 0.4883 - precision: 0.1162 - recall: 1.2459 - f1_metric: 0.2086 - val_loss: 0.9305 - val_acc: 0.0814 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.1603 - acc: 0.4598 - precision: 0.1281 - recall: 1.2135 - f1_metric: 0.2274 - val_loss: 0.9305 - val_acc: 0.0912 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8788 - acc: 0.4868 - precision: 0.1166 - recall: 1.2549 - f1_metric: 0.2095 - val_loss: 0.9305 - val_acc: 0.0734 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9213 - acc: 0.4678 - precision: 0.1213 - recall: 1.1912 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.3109 - val_precision: 0.1312 - val_recall: 1.2975 - val_f1_metric: 0.2308\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0477 - acc: 0.4696 - precision: 0.1293 - recall: 1.2492 - f1_metric: 0.2306 - val_loss: 0.9305 - val_acc: 0.7362 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7664 - acc: 0.4902 - precision: 0.1166 - recall: 1.1855 - f1_metric: 0.2081 - val_loss: 0.9305 - val_acc: 0.7007 - val_precision: 0.1311 - val_recall: 1.2975 - val_f1_metric: 0.2307\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9937 - acc: 0.4825 - precision: 0.1136 - recall: 1.1927 - f1_metric: 0.2036 - val_loss: 0.9305 - val_acc: 0.1597 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7275 - acc: 0.4810 - precision: 0.1261 - recall: 1.2702 - f1_metric: 0.2257 - val_loss: 0.9305 - val_acc: 0.1212 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9171 - acc: 0.4826 - precision: 0.1265 - recall: 1.3161 - f1_metric: 0.2270 - val_loss: 0.9305 - val_acc: 0.8641 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7422 - acc: 0.4625 - precision: 0.1191 - recall: 1.2852 - f1_metric: 0.2133 - val_loss: 0.9305 - val_acc: 0.4002 - val_precision: 0.1327 - val_recall: 1.3040 - val_f1_metric: 0.2333\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8741 - acc: 0.4870 - precision: 0.1252 - recall: 1.3198 - f1_metric: 0.2240 - val_loss: 0.9305 - val_acc: 0.8782 - val_precision: 0.1311 - val_recall: 1.2991 - val_f1_metric: 0.2306\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9118 - acc: 0.4890 - precision: 0.1224 - recall: 1.2637 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.0734 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9678 - acc: 0.4659 - precision: 0.1282 - recall: 1.2458 - f1_metric: 0.2278 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7310 - acc: 0.4649 - precision: 0.1136 - recall: 1.2522 - f1_metric: 0.2051 - val_loss: 0.9305 - val_acc: 0.7381 - val_precision: 0.1311 - val_recall: 1.3012 - val_f1_metric: 0.2308\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.3127 - acc: 0.5005 - precision: 0.1222 - recall: 1.2571 - f1_metric: 0.2181 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9166 - acc: 0.4721 - precision: 0.1218 - recall: 1.2308 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.8953 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7472 - acc: 0.4834 - precision: 0.1095 - recall: 1.2232 - f1_metric: 0.1967 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7824 - acc: 0.4497 - precision: 0.1226 - recall: 1.2842 - f1_metric: 0.2203 - val_loss: 0.9305 - val_acc: 0.9002 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6119 - acc: 0.5007 - precision: 0.1135 - recall: 1.2010 - f1_metric: 0.2036 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7923 - acc: 0.4851 - precision: 0.1193 - recall: 1.1973 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.3397 - val_precision: 0.1308 - val_recall: 1.2895 - val_f1_metric: 0.2300\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7513 - acc: 0.4639 - precision: 0.1242 - recall: 1.2615 - f1_metric: 0.2225 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6161 - acc: 0.4904 - precision: 0.1213 - recall: 1.2135 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0182 - acc: 0.4771 - precision: 0.1218 - recall: 1.2355 - f1_metric: 0.2174 - val_loss: 0.9305 - val_acc: 0.8862 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8040 - acc: 0.4954 - precision: 0.1250 - recall: 1.2954 - f1_metric: 0.2223 - val_loss: 0.9305 - val_acc: 0.3035 - val_precision: 0.1327 - val_recall: 1.3040 - val_f1_metric: 0.2334\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7443 - acc: 0.4915 - precision: 0.1159 - recall: 1.1821 - f1_metric: 0.2069 - val_loss: 0.9305 - val_acc: 0.2466 - val_precision: 0.1327 - val_recall: 1.2991 - val_f1_metric: 0.2334\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8041 - acc: 0.5014 - precision: 0.1090 - recall: 1.2158 - f1_metric: 0.1965 - val_loss: 0.9305 - val_acc: 0.1200 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8994 - acc: 0.4814 - precision: 0.1159 - recall: 1.2622 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.0900 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7697 - acc: 0.4734 - precision: 0.1207 - recall: 1.2271 - f1_metric: 0.2157 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7937 - acc: 0.4667 - precision: 0.1289 - recall: 1.2583 - f1_metric: 0.2295 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5669 - acc: 0.4432 - precision: 0.1198 - recall: 1.2504 - f1_metric: 0.2141 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7418 - acc: 0.4930 - precision: 0.1208 - recall: 1.2286 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9270 - acc: 0.4921 - precision: 0.1208 - recall: 1.2219 - f1_metric: 0.2150 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0436 - acc: 0.4915 - precision: 0.1254 - recall: 1.2457 - f1_metric: 0.2231 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7811 - acc: 0.4608 - precision: 0.1192 - recall: 1.2802 - f1_metric: 0.2147 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6416 - acc: 0.4830 - precision: 0.1185 - recall: 1.2127 - f1_metric: 0.2118 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 17 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 1.1533 - acc: 0.4811 - precision: 0.1155 - recall: 1.2210 - f1_metric: 0.2060 - val_loss: 0.9305 - val_acc: 0.1916 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8781 - acc: 0.4443 - precision: 0.1148 - recall: 1.1931 - f1_metric: 0.2046 - val_loss: 0.9305 - val_acc: 0.2295 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1527 - acc: 0.5074 - precision: 0.1228 - recall: 1.2700 - f1_metric: 0.2200 - val_loss: 0.9305 - val_acc: 0.1726 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8112 - acc: 0.5054 - precision: 0.1228 - recall: 1.2528 - f1_metric: 0.2194 - val_loss: 0.9305 - val_acc: 0.7375 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6887 - acc: 0.5145 - precision: 0.1153 - recall: 1.2428 - f1_metric: 0.2077 - val_loss: 0.9305 - val_acc: 0.6457 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7687 - acc: 0.4994 - precision: 0.1272 - recall: 1.2087 - f1_metric: 0.2258 - val_loss: 0.9305 - val_acc: 0.4302 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9661 - acc: 0.4767 - precision: 0.1106 - recall: 1.2194 - f1_metric: 0.1995 - val_loss: 0.9305 - val_acc: 0.1628 - val_precision: 0.1314 - val_recall: 1.2975 - val_f1_metric: 0.2311\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8595 - acc: 0.4937 - precision: 0.1264 - recall: 1.2279 - f1_metric: 0.2250 - val_loss: 0.9305 - val_acc: 0.8091 - val_precision: 0.1313 - val_recall: 1.3007 - val_f1_metric: 0.2310\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8735 - acc: 0.4901 - precision: 0.1154 - recall: 1.2163 - f1_metric: 0.2064 - val_loss: 0.9305 - val_acc: 0.1224 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9864 - acc: 0.4738 - precision: 0.1222 - recall: 1.2830 - f1_metric: 0.2195 - val_loss: 0.9305 - val_acc: 0.4676 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2323\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6889 - acc: 0.4870 - precision: 0.1242 - recall: 1.2317 - f1_metric: 0.2205 - val_loss: 0.9305 - val_acc: 0.1316 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5601 - acc: 0.4896 - precision: 0.1228 - recall: 1.2319 - f1_metric: 0.2185 - val_loss: 0.9305 - val_acc: 0.2589 - val_precision: 0.1324 - val_recall: 1.3040 - val_f1_metric: 0.2328\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1652 - acc: 0.4874 - precision: 0.1225 - recall: 1.2298 - f1_metric: 0.2183 - val_loss: 0.9305 - val_acc: 0.3452 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2324\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0109 - acc: 0.4876 - precision: 0.1205 - recall: 1.2939 - f1_metric: 0.2156 - val_loss: 0.9305 - val_acc: 0.7546 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6961 - acc: 0.4974 - precision: 0.1091 - recall: 1.1374 - f1_metric: 0.1956 - val_loss: 0.9305 - val_acc: 0.5141 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8444 - acc: 0.4835 - precision: 0.1142 - recall: 1.2039 - f1_metric: 0.2050 - val_loss: 0.9305 - val_acc: 0.1040 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0968 - acc: 0.4935 - precision: 0.1221 - recall: 1.2744 - f1_metric: 0.2192 - val_loss: 0.9305 - val_acc: 0.5239 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7393 - acc: 0.4775 - precision: 0.1284 - recall: 1.2448 - f1_metric: 0.2278 - val_loss: 0.9305 - val_acc: 0.2405 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8564 - acc: 0.4754 - precision: 0.1194 - recall: 1.2305 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.6799 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1049 - acc: 0.5039 - precision: 0.1232 - recall: 1.2403 - f1_metric: 0.2201 - val_loss: 0.9305 - val_acc: 0.1438 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0388 - acc: 0.4975 - precision: 0.1207 - recall: 1.2635 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6795 - acc: 0.4764 - precision: 0.1198 - recall: 1.2370 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7235 - acc: 0.4796 - precision: 0.1240 - recall: 1.2744 - f1_metric: 0.2216 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6340 - acc: 0.5004 - precision: 0.1235 - recall: 1.3279 - f1_metric: 0.2218 - val_loss: 0.9305 - val_acc: 0.0734 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8872 - acc: 0.4422 - precision: 0.1240 - recall: 1.2662 - f1_metric: 0.2218 - val_loss: 0.9305 - val_acc: 0.0826 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7548 - acc: 0.4506 - precision: 0.1193 - recall: 1.2382 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.8984 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2279 - acc: 0.4974 - precision: 0.1218 - recall: 1.2433 - f1_metric: 0.2170 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2397 - acc: 0.5179 - precision: 0.1186 - recall: 1.1932 - f1_metric: 0.2124 - val_loss: 0.9305 - val_acc: 0.8831 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6120 - acc: 0.5130 - precision: 0.1179 - recall: 1.1877 - f1_metric: 0.2110 - val_loss: 0.9305 - val_acc: 0.0783 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2720 - acc: 0.4990 - precision: 0.1265 - recall: 1.3056 - f1_metric: 0.2267 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7178 - acc: 0.4731 - precision: 0.1213 - recall: 1.2708 - f1_metric: 0.2169 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6757 - acc: 0.5031 - precision: 0.1169 - recall: 1.2649 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6468 - acc: 0.4797 - precision: 0.1190 - recall: 1.2754 - f1_metric: 0.2133 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9038 - acc: 0.4839 - precision: 0.1188 - recall: 1.1935 - f1_metric: 0.2117 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8210 - acc: 0.4872 - precision: 0.1153 - recall: 1.2625 - f1_metric: 0.2074 - val_loss: 0.9305 - val_acc: 0.0704 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8464 - acc: 0.5007 - precision: 0.1242 - recall: 1.2660 - f1_metric: 0.2230 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8062 - acc: 0.4851 - precision: 0.1197 - recall: 1.2541 - f1_metric: 0.2143 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8979 - acc: 0.4746 - precision: 0.1178 - recall: 1.2402 - f1_metric: 0.2116 - val_loss: 0.9305 - val_acc: 0.0704 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8797 - acc: 0.4751 - precision: 0.1194 - recall: 1.2550 - f1_metric: 0.2146 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.9887 - acc: 0.5000 - precision: 0.1175 - recall: 1.2829 - f1_metric: 0.2108 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7127 - acc: 0.4715 - precision: 0.1207 - recall: 1.2639 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.1848 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8314 - acc: 0.4884 - precision: 0.1135 - recall: 1.2146 - f1_metric: 0.2032 - val_loss: 0.9305 - val_acc: 0.1022 - val_precision: 0.1376 - val_recall: 1.2980 - val_f1_metric: 0.2407\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7567 - acc: 0.5079 - precision: 0.1168 - recall: 1.2597 - f1_metric: 0.2101 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7607 - acc: 0.4838 - precision: 0.1152 - recall: 1.2482 - f1_metric: 0.2073 - val_loss: 0.9305 - val_acc: 0.0857 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2830 - acc: 0.4915 - precision: 0.1177 - recall: 1.2006 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.2026 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7703 - acc: 0.4776 - precision: 0.1149 - recall: 1.2677 - f1_metric: 0.2066 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0174 - acc: 0.4851 - precision: 0.1186 - recall: 1.2350 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6118 - acc: 0.4914 - precision: 0.1226 - recall: 1.2171 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.2589 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8523 - acc: 0.4980 - precision: 0.1256 - recall: 1.2503 - f1_metric: 0.2246 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9068 - acc: 0.4949 - precision: 1241507.5560 - recall: 1.1999 - f1_metric: 0.2731 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 18 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.6687 - acc: 0.4592 - precision: 0.1126 - recall: 1.1854 - f1_metric: 0.2019 - val_loss: 0.9305 - val_acc: 0.0857 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7451 - acc: 0.4504 - precision: 0.1126 - recall: 1.2084 - f1_metric: 0.2020 - val_loss: 0.9305 - val_acc: 0.0900 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6699 - acc: 0.4565 - precision: 0.1127 - recall: 1.1512 - f1_metric: 0.2010 - val_loss: 0.9305 - val_acc: 0.0820 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8157 - acc: 0.4534 - precision: 0.1184 - recall: 1.2236 - f1_metric: 0.2116 - val_loss: 0.9305 - val_acc: 0.5949 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6698 - acc: 0.4993 - precision: 0.1126 - recall: 1.1918 - f1_metric: 0.2020 - val_loss: 0.9305 - val_acc: 0.6377 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8275 - acc: 0.4908 - precision: 0.1167 - recall: 1.2694 - f1_metric: 0.2100 - val_loss: 0.9305 - val_acc: 0.8849 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3198 - acc: 0.5221 - precision: 0.1102 - recall: 1.2042 - f1_metric: 0.1980 - val_loss: 0.9305 - val_acc: 0.8109 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7328 - acc: 0.4614 - precision: 0.1208 - recall: 1.1848 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.7558 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7345 - acc: 0.4869 - precision: 0.1192 - recall: 1.1945 - f1_metric: 0.2125 - val_loss: 0.9305 - val_acc: 0.7258 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8786 - acc: 0.4766 - precision: 0.1155 - recall: 1.2315 - f1_metric: 0.2072 - val_loss: 0.9305 - val_acc: 0.8048 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8493 - acc: 0.4731 - precision: 0.1155 - recall: 1.1921 - f1_metric: 0.2057 - val_loss: 0.9305 - val_acc: 0.8311 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6778 - acc: 0.4797 - precision: 0.1193 - recall: 1.2392 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.0985 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0044 - acc: 0.4721 - precision: 0.1306 - recall: 1.2840 - f1_metric: 0.2331 - val_loss: 0.9305 - val_acc: 0.0955 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6624 - acc: 0.4601 - precision: 0.1295 - recall: 1.2419 - f1_metric: 0.2301 - val_loss: 0.9305 - val_acc: 0.7271 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8217 - acc: 0.4699 - precision: 0.1233 - recall: 1.1889 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.0832 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9444 - acc: 0.4661 - precision: 0.1179 - recall: 1.2540 - f1_metric: 0.2113 - val_loss: 0.9305 - val_acc: 0.0985 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8684 - acc: 0.4665 - precision: 0.1139 - recall: 1.2462 - f1_metric: 0.2044 - val_loss: 0.9305 - val_acc: 0.7393 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6711 - acc: 0.4663 - precision: 0.1190 - recall: 1.2282 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.7564 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7309 - acc: 0.4843 - precision: 0.1229 - recall: 1.2639 - f1_metric: 0.2190 - val_loss: 0.9305 - val_acc: 0.0851 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6906 - acc: 0.4807 - precision: 0.1202 - recall: 1.2562 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.0826 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7540 - acc: 0.4630 - precision: 0.1120 - recall: 1.2015 - f1_metric: 0.2011 - val_loss: 0.9305 - val_acc: 0.0796 - val_precision: 0.1311 - val_recall: 1.3001 - val_f1_metric: 0.2307\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6824 - acc: 0.4755 - precision: 0.1255 - recall: 1.2738 - f1_metric: 0.2238 - val_loss: 0.9305 - val_acc: 0.0685 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0103 - acc: 0.4786 - precision: 0.1257 - recall: 1.2736 - f1_metric: 0.2244 - val_loss: 0.9305 - val_acc: 0.8892 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7727 - acc: 0.4881 - precision: 0.1145 - recall: 1.2459 - f1_metric: 0.2059 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7471 - acc: 0.4777 - precision: 0.1195 - recall: 1.2181 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.8672 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6981 - acc: 0.4948 - precision: 0.1149 - recall: 1.2000 - f1_metric: 0.2053 - val_loss: 0.9305 - val_acc: 0.0832 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8630 - acc: 0.4717 - precision: 0.1219 - recall: 1.2351 - f1_metric: 0.2170 - val_loss: 0.9305 - val_acc: 0.8868 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.7407 - acc: 0.4935 - precision: 0.1178 - recall: 1.2337 - f1_metric: 0.2108 - val_loss: 0.9305 - val_acc: 0.0802 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8857 - acc: 0.4810 - precision: 0.1255 - recall: 1.2434 - f1_metric: 0.2245 - val_loss: 0.9305 - val_acc: 0.8905 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6833 - acc: 0.4947 - precision: 0.1180 - recall: 1.2080 - f1_metric: 0.2112 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9434 - acc: 0.4675 - precision: 0.1187 - recall: 1.2594 - f1_metric: 0.2128 - val_loss: 0.9305 - val_acc: 0.8586 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8595 - acc: 0.4989 - precision: 0.1210 - recall: 1.2376 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6202 - acc: 0.4948 - precision: 0.1224 - recall: 1.2663 - f1_metric: 0.2202 - val_loss: 0.9305 - val_acc: 0.8605 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5809 - acc: 0.5001 - precision: 0.1152 - recall: 1.1975 - f1_metric: 0.2064 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2325\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7843 - acc: 0.4726 - precision: 0.1156 - recall: 1.2274 - f1_metric: 0.2072 - val_loss: 0.9305 - val_acc: 0.8225 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0576 - acc: 0.4881 - precision: 0.1190 - recall: 1.2786 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.6775 - val_precision: 0.1327 - val_recall: 1.3040 - val_f1_metric: 0.2332\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9590 - acc: 0.4700 - precision: 0.1206 - recall: 1.2292 - f1_metric: 0.2154 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8890 - acc: 0.5023 - precision: 0.1153 - recall: 1.1806 - f1_metric: 0.2055 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9531 - acc: 0.4604 - precision: 0.1131 - recall: 1.2777 - f1_metric: 0.2040 - val_loss: 0.9305 - val_acc: 0.8996 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.3713 - acc: 0.4874 - precision: 0.1227 - recall: 1.2640 - f1_metric: 0.2202 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6399 - acc: 0.4797 - precision: 0.1177 - recall: 1.2386 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7749 - acc: 0.4890 - precision: 0.1202 - recall: 1.2794 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6898 - acc: 0.4916 - precision: 0.1234 - recall: 1.2377 - f1_metric: 0.2201 - val_loss: 0.9305 - val_acc: 0.8782 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8797 - acc: 0.5036 - precision: 0.1248 - recall: 1.2348 - f1_metric: 0.2226 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6945 - acc: 0.4594 - precision: 0.1187 - recall: 1.2521 - f1_metric: 0.2115 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8224 - acc: 0.4944 - precision: 0.1141 - recall: 1.2101 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9566 - acc: 0.4740 - precision: 0.1205 - recall: 1.2140 - f1_metric: 0.2155 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1881 - acc: 0.4838 - precision: 0.1295 - recall: 1.3014 - f1_metric: 0.2311 - val_loss: 0.9305 - val_acc: 0.1242 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.8169 - acc: 0.4595 - precision: 0.1142 - recall: 1.2202 - f1_metric: 0.2043 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7251 - acc: 0.5003 - precision: 0.1183 - recall: 1.2411 - f1_metric: 0.2122 - val_loss: 0.9305 - val_acc: 0.8629 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 19 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.7376 - acc: 0.4643 - precision: 0.1207 - recall: 1.2639 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.8696 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6854 - acc: 0.4652 - precision: 0.1139 - recall: 1.2235 - f1_metric: 0.2042 - val_loss: 0.9305 - val_acc: 0.1083 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6682 - acc: 0.4771 - precision: 0.1141 - recall: 1.2014 - f1_metric: 0.2046 - val_loss: 0.9305 - val_acc: 0.8433 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7849 - acc: 0.4749 - precision: 0.1172 - recall: 1.2174 - f1_metric: 0.2097 - val_loss: 0.9305 - val_acc: 0.1151 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5917 - acc: 0.4735 - precision: 0.1253 - recall: 1.2196 - f1_metric: 0.2221 - val_loss: 0.9305 - val_acc: 0.8984 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7920 - acc: 0.4860 - precision: 0.1164 - recall: 1.2528 - f1_metric: 0.2096 - val_loss: 0.9305 - val_acc: 0.8831 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7595 - acc: 0.4685 - precision: 0.1146 - recall: 1.2374 - f1_metric: 0.2065 - val_loss: 0.9305 - val_acc: 0.6916 - val_precision: 0.1316 - val_recall: 1.3001 - val_f1_metric: 0.2315\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9974 - acc: 0.4729 - precision: 0.1217 - recall: 1.2242 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0112 - acc: 0.4914 - precision: 0.1205 - recall: 1.2573 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.8494 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9158 - acc: 0.4930 - precision: 0.1151 - recall: 1.2088 - f1_metric: 0.2059 - val_loss: 0.9305 - val_acc: 0.5942 - val_precision: 0.1323 - val_recall: 1.3040 - val_f1_metric: 0.2326\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7494 - acc: 0.4870 - precision: 0.1207 - recall: 1.2948 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.8531 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7211 - acc: 0.5042 - precision: 0.1205 - recall: 1.1523 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.0826 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8049 - acc: 0.4938 - precision: 0.1111 - recall: 1.2889 - f1_metric: 0.2007 - val_loss: 0.9305 - val_acc: 0.0918 - val_precision: 0.1312 - val_recall: 1.2975 - val_f1_metric: 0.2308\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6234 - acc: 0.4883 - precision: 0.1199 - recall: 1.2071 - f1_metric: 0.2140 - val_loss: 0.9305 - val_acc: 0.0942 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5138 - acc: 0.4889 - precision: 0.1114 - recall: 1.2409 - f1_metric: 0.2005 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.0761 - acc: 0.5040 - precision: 0.1174 - recall: 1.2618 - f1_metric: 0.2107 - val_loss: 0.9305 - val_acc: 0.7748 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6294 - acc: 0.4849 - precision: 0.1154 - recall: 1.2648 - f1_metric: 0.2075 - val_loss: 0.9305 - val_acc: 0.7987 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 0.6844 - acc: 0.4503 - precision: 0.1226 - recall: 1.2669 - f1_metric: 0.2188 - val_loss: 0.9305 - val_acc: 0.7705 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7042 - acc: 0.4588 - precision: 0.1226 - recall: 1.2418 - f1_metric: 0.2199 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8238 - acc: 0.4663 - precision: 0.1177 - recall: 1.2817 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.7993 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7924 - acc: 0.4720 - precision: 0.1216 - recall: 1.2270 - f1_metric: 0.2174 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7384 - acc: 0.4958 - precision: 0.1156 - recall: 1.2086 - f1_metric: 0.2073 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8496 - acc: 0.5206 - precision: 0.1308 - recall: 1.2826 - f1_metric: 0.2329 - val_loss: 0.9305 - val_acc: 0.3953 - val_precision: 0.1337 - val_recall: 1.2847 - val_f1_metric: 0.2346\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7412 - acc: 0.4722 - precision: 0.1210 - recall: 1.2608 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.1028 - val_precision: 0.1312 - val_recall: 1.3007 - val_f1_metric: 0.2309\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8579 - acc: 0.4817 - precision: 0.1249 - recall: 1.2929 - f1_metric: 0.2233 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9024 - acc: 0.5123 - precision: 0.1204 - recall: 1.2261 - f1_metric: 0.2153 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1316 - acc: 0.4801 - precision: 0.1225 - recall: 1.2452 - f1_metric: 0.2191 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8783 - acc: 0.5143 - precision: 0.1154 - recall: 1.2832 - f1_metric: 0.2081 - val_loss: 0.9305 - val_acc: 0.8813 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8794 - acc: 0.5054 - precision: 0.1247 - recall: 1.2152 - f1_metric: 0.2218 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7446 - acc: 0.4876 - precision: 0.1175 - recall: 1.2386 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8125 - acc: 0.4712 - precision: 0.1248 - recall: 1.2368 - f1_metric: 0.2205 - val_loss: 0.9305 - val_acc: 0.0789 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5823 - acc: 0.4867 - precision: 0.1225 - recall: 1.2297 - f1_metric: 0.2190 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9666 - acc: 0.4910 - precision: 0.1225 - recall: 1.2679 - f1_metric: 0.2191 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8503 - acc: 0.4717 - precision: 0.1178 - recall: 1.2934 - f1_metric: 0.2116 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6030 - acc: 0.4884 - precision: 0.1173 - recall: 1.2332 - f1_metric: 0.2099 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7858 - acc: 0.4640 - precision: 0.1125 - recall: 1.1842 - f1_metric: 0.2008 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6717 - acc: 0.4956 - precision: 0.1112 - recall: 1.1843 - f1_metric: 0.1995 - val_loss: 0.9305 - val_acc: 0.5294 - val_precision: 0.1326 - val_recall: 1.2975 - val_f1_metric: 0.2332\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8228 - acc: 0.4847 - precision: 0.1221 - recall: 1.2507 - f1_metric: 0.2184 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9182 - acc: 0.4905 - precision: 0.1216 - recall: 1.2235 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7859 - acc: 0.4811 - precision: 0.1192 - recall: 1.1814 - f1_metric: 0.2123 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8739 - acc: 0.4914 - precision: 0.1279 - recall: 1.2455 - f1_metric: 0.2272 - val_loss: 0.9305 - val_acc: 0.0881 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8015 - acc: 0.4815 - precision: 0.1163 - recall: 1.2305 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6547 - acc: 0.5078 - precision: 0.1207 - recall: 1.2568 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7073 - acc: 0.4944 - precision: 0.1240 - recall: 1.2237 - f1_metric: 0.2208 - val_loss: 0.9305 - val_acc: 0.8831 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0521 - acc: 0.4815 - precision: 0.1187 - recall: 1.2930 - f1_metric: 0.2131 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1305 - acc: 0.4861 - precision: 0.1193 - recall: 1.2744 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.8115 - val_precision: 0.1302 - val_recall: 1.2527 - val_f1_metric: 0.2284\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7882 - acc: 0.4720 - precision: 0.1188 - recall: 1.2942 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.8788 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8329 - acc: 0.4978 - precision: 0.1232 - recall: 1.2701 - f1_metric: 0.2207 - val_loss: 0.9305 - val_acc: 0.8770 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1483 - acc: 0.4882 - precision: 0.1183 - recall: 1.2694 - f1_metric: 0.2121 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0070 - acc: 0.4912 - precision: 0.1213 - recall: 1.2163 - f1_metric: 0.2151 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 20 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 1.1146 - acc: 0.5013 - precision: 0.1231 - recall: 1.2487 - f1_metric: 0.2202 - val_loss: 0.9305 - val_acc: 0.6873 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1689 - acc: 0.5178 - precision: 0.1198 - recall: 1.2126 - f1_metric: 0.2145 - val_loss: 0.9305 - val_acc: 0.8923 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9178 - acc: 0.4659 - precision: 0.1223 - recall: 1.2737 - f1_metric: 0.2180 - val_loss: 0.9305 - val_acc: 0.1114 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6358 - acc: 0.4872 - precision: 0.1073 - recall: 1.2696 - f1_metric: 0.1939 - val_loss: 0.9305 - val_acc: 0.1047 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6763 - acc: 0.4820 - precision: 0.1133 - recall: 1.1663 - f1_metric: 0.2022 - val_loss: 0.9305 - val_acc: 0.7607 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1156 - acc: 0.4712 - precision: 0.1236 - recall: 1.2803 - f1_metric: 0.2209 - val_loss: 0.9305 - val_acc: 0.8421 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7675 - acc: 0.4960 - precision: 0.1297 - recall: 1.3056 - f1_metric: 0.2311 - val_loss: 0.9305 - val_acc: 0.6860 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8153 - acc: 0.4961 - precision: 0.1177 - recall: 1.2084 - f1_metric: 0.2099 - val_loss: 0.9305 - val_acc: 0.8666 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8073 - acc: 0.4942 - precision: 0.1114 - recall: 1.2553 - f1_metric: 0.2010 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 3ms/step - loss: 1.2629 - acc: 0.5009 - precision: 0.1230 - recall: 1.2262 - f1_metric: 0.2196 - val_loss: 0.9305 - val_acc: 0.4088 - val_precision: 0.1314 - val_recall: 1.2975 - val_f1_metric: 0.2312\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0199 - acc: 0.4980 - precision: 0.1139 - recall: 1.2326 - f1_metric: 0.2054 - val_loss: 0.9305 - val_acc: 0.5900 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8430 - acc: 0.5116 - precision: 0.1147 - recall: 1.1491 - f1_metric: 0.2050 - val_loss: 0.9305 - val_acc: 0.7913 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7354 - acc: 0.4939 - precision: 0.1244 - recall: 1.2911 - f1_metric: 0.2228 - val_loss: 0.9305 - val_acc: 0.7222 - val_precision: 0.1310 - val_recall: 1.2975 - val_f1_metric: 0.2306\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6989 - acc: 0.4865 - precision: 0.1185 - recall: 1.1945 - f1_metric: 0.2115 - val_loss: 0.9305 - val_acc: 0.8892 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0288 - acc: 0.5070 - precision: 0.1252 - recall: 1.2481 - f1_metric: 0.2232 - val_loss: 0.9305 - val_acc: 0.0747 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5376 - acc: 0.4908 - precision: 0.1202 - recall: 1.1924 - f1_metric: 0.2145 - val_loss: 0.9305 - val_acc: 0.3201 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6614 - acc: 0.4918 - precision: 0.1119 - recall: 1.2101 - f1_metric: 0.1999 - val_loss: 0.9305 - val_acc: 0.4920 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6548 - acc: 0.4777 - precision: 0.1183 - recall: 1.2574 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.8195 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0496 - acc: 0.5078 - precision: 0.1176 - recall: 1.2693 - f1_metric: 0.2117 - val_loss: 0.9305 - val_acc: 0.4106 - val_precision: 0.1312 - val_recall: 1.3001 - val_f1_metric: 0.2309\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1279 - acc: 0.4816 - precision: 0.1180 - recall: 1.2404 - f1_metric: 0.2122 - val_loss: 0.9305 - val_acc: 0.6322 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9612 - acc: 0.4796 - precision: 0.1258 - recall: 1.2631 - f1_metric: 0.2250 - val_loss: 0.9305 - val_acc: 0.8978 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9813 - acc: 0.5022 - precision: 0.1142 - recall: 1.2667 - f1_metric: 0.2057 - val_loss: 0.9305 - val_acc: 0.8831 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6632 - acc: 0.4871 - precision: 0.1196 - recall: 1.2457 - f1_metric: 0.2136 - val_loss: 0.9305 - val_acc: 0.9009 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1738 - acc: 0.5093 - precision: 0.1231 - recall: 1.2651 - f1_metric: 0.2210 - val_loss: 0.9305 - val_acc: 0.7785 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9748 - acc: 0.4922 - precision: 0.1114 - recall: 1.2492 - f1_metric: 0.2006 - val_loss: 0.9305 - val_acc: 0.6187 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9733 - acc: 0.4761 - precision: 0.1130 - recall: 1.2248 - f1_metric: 0.2034 - val_loss: 0.9305 - val_acc: 0.6371 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7252 - acc: 0.4952 - precision: 0.1171 - recall: 1.2376 - f1_metric: 0.2103 - val_loss: 0.9305 - val_acc: 0.7185 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7800 - acc: 0.4979 - precision: 0.1183 - recall: 1.2497 - f1_metric: 0.2123 - val_loss: 0.9305 - val_acc: 0.2846 - val_precision: 0.1315 - val_recall: 1.3012 - val_f1_metric: 0.2313\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7566 - acc: 0.4725 - precision: 0.1139 - recall: 1.2556 - f1_metric: 0.2041 - val_loss: 0.9305 - val_acc: 0.0949 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8800 - acc: 0.4868 - precision: 0.1161 - recall: 1.1761 - f1_metric: 0.2073 - val_loss: 0.9305 - val_acc: 0.1193 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7255 - acc: 0.4657 - precision: 0.1178 - recall: 1.2527 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.8696 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8420 - acc: 0.5111 - precision: 0.1146 - recall: 1.1975 - f1_metric: 0.2050 - val_loss: 0.9305 - val_acc: 0.1077 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6248 - acc: 0.4693 - precision: 0.1177 - recall: 1.1939 - f1_metric: 0.2096 - val_loss: 0.9305 - val_acc: 0.8813 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6698 - acc: 0.4920 - precision: 0.1157 - recall: 1.2172 - f1_metric: 0.2075 - val_loss: 0.9305 - val_acc: 0.1377 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6711 - acc: 0.4833 - precision: 0.1170 - recall: 1.1938 - f1_metric: 0.2086 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9903 - acc: 0.4751 - precision: 0.1196 - recall: 1.2624 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.1047 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8755 - acc: 0.4840 - precision: 0.1173 - recall: 1.2469 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7205 - acc: 0.5031 - precision: 0.1229 - recall: 1.2346 - f1_metric: 0.2200 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7429 - acc: 0.4865 - precision: 0.1235 - recall: 1.2815 - f1_metric: 0.2213 - val_loss: 0.9305 - val_acc: 0.0857 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6506 - acc: 0.4772 - precision: 0.1196 - recall: 1.2320 - f1_metric: 0.2133 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8789 - acc: 0.4986 - precision: 0.1183 - recall: 1.1905 - f1_metric: 0.2121 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1905 - acc: 0.4768 - precision: 0.1262 - recall: 1.2832 - f1_metric: 0.2259 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7193 - acc: 0.4650 - precision: 0.1179 - recall: 1.2236 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0658 - acc: 0.5028 - precision: 0.1208 - recall: 1.2333 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9322 - acc: 0.4815 - precision: 0.1162 - recall: 1.2582 - f1_metric: 0.2093 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8252 - acc: 0.4686 - precision: 0.1114 - recall: 1.2328 - f1_metric: 0.2008 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6449 - acc: 0.4782 - precision: 0.1252 - recall: 1.2039 - f1_metric: 0.2223 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2603 - acc: 0.5124 - precision: 0.1244 - recall: 1.3065 - f1_metric: 0.2230 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7771 - acc: 0.4774 - precision: 0.1175 - recall: 1.2428 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6628 - acc: 0.4940 - precision: 0.1124 - recall: 1.2411 - f1_metric: 0.2026 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 21 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 1.0138 - acc: 0.4688 - precision: 0.1210 - recall: 1.2715 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.0949 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8610 - acc: 0.4629 - precision: 0.1187 - recall: 1.2734 - f1_metric: 0.2128 - val_loss: 0.9305 - val_acc: 0.0845 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6121 - acc: 0.4491 - precision: 0.1112 - recall: 1.2382 - f1_metric: 0.2003 - val_loss: 0.9305 - val_acc: 0.1946 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8158 - acc: 0.4661 - precision: 0.1150 - recall: 1.2720 - f1_metric: 0.2068 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9231 - acc: 0.4710 - precision: 0.1115 - recall: 1.2558 - f1_metric: 0.2009 - val_loss: 0.9305 - val_acc: 0.1047 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8633 - acc: 0.4795 - precision: 0.1174 - recall: 1.2569 - f1_metric: 0.2096 - val_loss: 0.9305 - val_acc: 0.1236 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1522 - acc: 0.4722 - precision: 0.1199 - recall: 1.2878 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.1212 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1935 - acc: 0.4713 - precision: 0.1328 - recall: 1.2288 - f1_metric: 0.2349 - val_loss: 0.9305 - val_acc: 0.0985 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6370 - acc: 0.4822 - precision: 0.1175 - recall: 1.2101 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.1236 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3829 - acc: 0.4735 - precision: 0.1230 - recall: 1.2714 - f1_metric: 0.2196 - val_loss: 0.9305 - val_acc: 0.1395 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6830 - acc: 0.4684 - precision: 0.1244 - recall: 1.2843 - f1_metric: 0.2226 - val_loss: 0.9305 - val_acc: 0.1891 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7327 - acc: 0.4873 - precision: 0.1189 - recall: 1.2231 - f1_metric: 0.2128 - val_loss: 0.9305 - val_acc: 0.6928 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8605 - acc: 0.4905 - precision: 0.1233 - recall: 1.2706 - f1_metric: 0.2208 - val_loss: 0.9305 - val_acc: 0.4088 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8906 - acc: 0.5007 - precision: 0.1160 - recall: 1.2053 - f1_metric: 0.2075 - val_loss: 0.9305 - val_acc: 0.1242 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6379 - acc: 0.4744 - precision: 0.1134 - recall: 1.1883 - f1_metric: 0.2032 - val_loss: 0.9305 - val_acc: 0.5685 - val_precision: 0.1309 - val_recall: 1.2895 - val_f1_metric: 0.2302\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7473 - acc: 0.4786 - precision: 0.1106 - recall: 1.2430 - f1_metric: 0.1995 - val_loss: 0.9305 - val_acc: 0.8715 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3691 - acc: 0.4720 - precision: 0.1273 - recall: 1.3064 - f1_metric: 0.2280 - val_loss: 0.9305 - val_acc: 0.2656 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7324 - acc: 0.4854 - precision: 0.1229 - recall: 1.2393 - f1_metric: 0.2181 - val_loss: 0.9305 - val_acc: 0.2050 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6027 - acc: 0.4833 - precision: 0.1214 - recall: 1.2131 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.8825 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7042 - acc: 0.4933 - precision: 0.1227 - recall: 1.2551 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.8164 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8559 - acc: 0.4922 - precision: 0.1276 - recall: 1.2723 - f1_metric: 0.2270 - val_loss: 0.9305 - val_acc: 0.8348 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3464 - acc: 0.4877 - precision: 0.1202 - recall: 1.2185 - f1_metric: 0.2151 - val_loss: 0.9305 - val_acc: 0.0875 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9754 - acc: 0.4856 - precision: 0.1181 - recall: 1.2433 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.1328 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7811 - acc: 0.4798 - precision: 0.1168 - recall: 1.2519 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.9002 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8599 - acc: 0.5008 - precision: 0.1302 - recall: 1.2728 - f1_metric: 0.2307 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7663 - acc: 0.4904 - precision: 0.1206 - recall: 1.2576 - f1_metric: 0.2166 - val_loss: 0.9305 - val_acc: 0.1848 - val_precision: 0.1311 - val_recall: 1.2991 - val_f1_metric: 0.2306\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9784 - acc: 0.4861 - precision: 0.1205 - recall: 1.2643 - f1_metric: 0.2157 - val_loss: 0.9305 - val_acc: 0.1126 - val_precision: 0.1311 - val_recall: 1.2991 - val_f1_metric: 0.2306\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7128 - acc: 0.4873 - precision: 0.1199 - recall: 1.2102 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7463 - acc: 0.5087 - precision: 0.1227 - recall: 1.2076 - f1_metric: 0.2198 - val_loss: 0.9305 - val_acc: 0.1775 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9485 - acc: 0.4926 - precision: 0.1230 - recall: 1.2162 - f1_metric: 0.2190 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8425 - acc: 0.4861 - precision: 0.1255 - recall: 1.2666 - f1_metric: 0.2228 - val_loss: 0.9305 - val_acc: 0.0802 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8077 - acc: 0.4574 - precision: 0.1164 - recall: 1.2138 - f1_metric: 0.2087 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6613 - acc: 0.4848 - precision: 0.1181 - recall: 1.2609 - f1_metric: 0.2119 - val_loss: 0.9305 - val_acc: 0.1077 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6561 - acc: 0.4831 - precision: 0.1173 - recall: 1.2148 - f1_metric: 0.2101 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7736 - acc: 0.4839 - precision: 0.1240 - recall: 1.2529 - f1_metric: 0.2209 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8162 - acc: 0.4774 - precision: 0.1282 - recall: 1.2907 - f1_metric: 0.2291 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2748 - acc: 0.4865 - precision: 0.1233 - recall: 1.2724 - f1_metric: 0.2206 - val_loss: 0.9305 - val_acc: 0.8776 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9020 - acc: 0.4987 - precision: 0.1127 - recall: 1.2742 - f1_metric: 0.2024 - val_loss: 0.9305 - val_acc: 0.0973 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9861 - acc: 0.4711 - precision: 0.1206 - recall: 1.2755 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.8862 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9802 - acc: 0.4880 - precision: 0.1104 - recall: 1.1836 - f1_metric: 0.1981 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7130 - acc: 0.4955 - precision: 0.1143 - recall: 1.2837 - f1_metric: 0.2058 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9609 - acc: 0.4911 - precision: 0.1259 - recall: 1.2289 - f1_metric: 0.2244 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2344 - acc: 0.4993 - precision: 0.1243 - recall: 1.3156 - f1_metric: 0.2225 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7808 - acc: 0.4933 - precision: 0.1232 - recall: 1.3162 - f1_metric: 0.2217 - val_loss: 0.9305 - val_acc: 0.0857 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0468 - acc: 0.4828 - precision: 0.1143 - recall: 1.2574 - f1_metric: 0.2054 - val_loss: 0.9305 - val_acc: 0.0789 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1475 - acc: 0.4852 - precision: 0.1198 - recall: 1.2529 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.8898 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5951 - acc: 0.5000 - precision: 0.1175 - recall: 1.2270 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3520 - acc: 0.4671 - precision: 0.1244 - recall: 1.2544 - f1_metric: 0.2217 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0419 - acc: 0.4836 - precision: 0.1166 - recall: 1.3073 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8735 - acc: 0.4868 - precision: 0.1194 - recall: 1.1537 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 22 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.6682 - acc: 0.4945 - precision: 0.1145 - recall: 1.2604 - f1_metric: 0.2058 - val_loss: 0.9305 - val_acc: 0.1426 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7166 - acc: 0.5007 - precision: 0.1239 - recall: 1.2156 - f1_metric: 0.2196 - val_loss: 0.9305 - val_acc: 0.8905 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9777 - acc: 0.4832 - precision: 0.1163 - recall: 1.2584 - f1_metric: 0.2081 - val_loss: 0.9305 - val_acc: 0.6732 - val_precision: 0.1311 - val_recall: 1.2975 - val_f1_metric: 0.2307\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9705 - acc: 0.4469 - precision: 0.1265 - recall: 1.2162 - f1_metric: 0.2247 - val_loss: 0.9305 - val_acc: 0.6089 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7772 - acc: 0.4595 - precision: 0.1151 - recall: 1.2813 - f1_metric: 0.2071 - val_loss: 0.9305 - val_acc: 0.2264 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7489 - acc: 0.4910 - precision: 0.1154 - recall: 1.2057 - f1_metric: 0.2057 - val_loss: 0.9305 - val_acc: 0.2326 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7159 - acc: 0.4927 - precision: 0.1099 - recall: 1.2688 - f1_metric: 0.1985 - val_loss: 0.9305 - val_acc: 0.6591 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1845 - acc: 0.4869 - precision: 0.1211 - recall: 1.2367 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.4621 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9029 - acc: 0.4902 - precision: 0.1135 - recall: 1.2534 - f1_metric: 0.2037 - val_loss: 0.9305 - val_acc: 0.6224 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6926 - acc: 0.4967 - precision: 0.1195 - recall: 1.2428 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.2497 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8387 - acc: 0.4877 - precision: 0.1222 - recall: 1.2367 - f1_metric: 0.2186 - val_loss: 0.9305 - val_acc: 0.4272 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8628 - acc: 0.4908 - precision: 0.1136 - recall: 1.2546 - f1_metric: 0.2050 - val_loss: 0.9305 - val_acc: 0.6114 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8857 - acc: 0.4756 - precision: 0.1129 - recall: 1.2275 - f1_metric: 0.2031 - val_loss: 0.9305 - val_acc: 0.6934 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8067 - acc: 0.4800 - precision: 0.1165 - recall: 1.2150 - f1_metric: 0.2081 - val_loss: 0.9305 - val_acc: 0.5398 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9084 - acc: 0.4854 - precision: 0.1113 - recall: 1.2591 - f1_metric: 0.2007 - val_loss: 0.9305 - val_acc: 0.2723 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8158 - acc: 0.4857 - precision: 0.1212 - recall: 1.1938 - f1_metric: 0.2161 - val_loss: 0.9305 - val_acc: 0.1071 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9319 - acc: 0.4851 - precision: 0.1185 - recall: 1.2207 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.5673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8126 - acc: 0.4988 - precision: 0.1237 - recall: 1.2688 - f1_metric: 0.2210 - val_loss: 0.9305 - val_acc: 0.3990 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9710 - acc: 0.4702 - precision: 0.1233 - recall: 1.2235 - f1_metric: 0.2204 - val_loss: 0.9305 - val_acc: 0.3727 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9276 - acc: 0.4866 - precision: 0.1147 - recall: 1.2388 - f1_metric: 0.2069 - val_loss: 0.9305 - val_acc: 0.3476 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6859 - acc: 0.4920 - precision: 0.1126 - recall: 1.1871 - f1_metric: 0.2017 - val_loss: 0.9305 - val_acc: 0.5496 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8840 - acc: 0.4859 - precision: 0.1214 - recall: 1.1989 - f1_metric: 0.2166 - val_loss: 0.9305 - val_acc: 0.6151 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8765 - acc: 0.4875 - precision: 0.1206 - recall: 1.3069 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.3543 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1305 - acc: 0.4892 - precision: 0.1273 - recall: 1.2990 - f1_metric: 0.2273 - val_loss: 0.9305 - val_acc: 0.1169 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0509 - acc: 0.4942 - precision: 0.1237 - recall: 1.2310 - f1_metric: 0.2203 - val_loss: 0.9305 - val_acc: 0.1193 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9324 - acc: 0.4911 - precision: 0.1183 - recall: 1.2101 - f1_metric: 0.2119 - val_loss: 0.9305 - val_acc: 0.1285 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7365 - acc: 0.4947 - precision: 0.1149 - recall: 1.2343 - f1_metric: 0.2067 - val_loss: 0.9305 - val_acc: 0.1744 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7501 - acc: 0.4955 - precision: 0.1243 - recall: 1.2163 - f1_metric: 0.2213 - val_loss: 0.9305 - val_acc: 0.4400 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8719 - acc: 0.4915 - precision: 0.1205 - recall: 1.2482 - f1_metric: 0.2167 - val_loss: 0.9305 - val_acc: 0.1365 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2896 - acc: 0.4993 - precision: 0.1272 - recall: 1.1954 - f1_metric: 0.2254 - val_loss: 0.9305 - val_acc: 0.7729 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9343 - acc: 0.4851 - precision: 0.1212 - recall: 1.2612 - f1_metric: 0.2163 - val_loss: 0.9305 - val_acc: 0.2375 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5905 - acc: 0.4908 - precision: 0.1211 - recall: 1.2485 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.2736 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0569 - acc: 0.4756 - precision: 0.1219 - recall: 1.2220 - f1_metric: 0.2176 - val_loss: 0.9305 - val_acc: 0.1469 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7242 - acc: 0.4500 - precision: 0.1157 - recall: 1.2705 - f1_metric: 0.2078 - val_loss: 0.9305 - val_acc: 0.1108 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6992 - acc: 0.4613 - precision: 0.1231 - recall: 1.2684 - f1_metric: 0.2192 - val_loss: 0.9305 - val_acc: 0.1230 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8897 - acc: 0.4658 - precision: 0.1200 - recall: 1.3094 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.1860 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6527 - acc: 0.4810 - precision: 0.1133 - recall: 1.1961 - f1_metric: 0.2037 - val_loss: 0.9305 - val_acc: 0.8990 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6543 - acc: 0.4862 - precision: 0.1222 - recall: 1.2473 - f1_metric: 0.2186 - val_loss: 0.9305 - val_acc: 0.2203 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8260 - acc: 0.4724 - precision: 0.1194 - recall: 1.2257 - f1_metric: 0.2139 - val_loss: 0.9305 - val_acc: 0.8929 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6866 - acc: 0.5007 - precision: 0.1152 - recall: 1.2359 - f1_metric: 0.2067 - val_loss: 0.9305 - val_acc: 0.8874 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8817 - acc: 0.4908 - precision: 0.1185 - recall: 1.1534 - f1_metric: 0.2112 - val_loss: 0.9305 - val_acc: 0.1059 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9026 - acc: 0.4472 - precision: 0.1233 - recall: 1.2636 - f1_metric: 0.2209 - val_loss: 0.9305 - val_acc: 0.1285 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6889 - acc: 0.4937 - precision: 0.1208 - recall: 1.2314 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.1187 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6711 - acc: 0.5033 - precision: 0.1230 - recall: 1.2806 - f1_metric: 0.2204 - val_loss: 0.9305 - val_acc: 0.1738 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6832 - acc: 0.4777 - precision: 0.1188 - recall: 1.2615 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.8935 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7942 - acc: 0.4765 - precision: 0.1249 - recall: 1.2527 - f1_metric: 0.2224 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9524 - acc: 0.4891 - precision: 0.1197 - recall: 1.2058 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.0936 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9846 - acc: 0.4919 - precision: 0.1207 - recall: 1.2541 - f1_metric: 0.2162 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7207 - acc: 0.5307 - precision: 0.1170 - recall: 1.2202 - f1_metric: 0.2090 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2819 - acc: 0.4908 - precision: 0.1242 - recall: 1.1975 - f1_metric: 0.2206 - val_loss: 0.9305 - val_acc: 0.7430 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2325\n",
            "Model 23 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.7330 - acc: 0.5075 - precision: 0.1089 - recall: 1.2129 - f1_metric: 0.1965 - val_loss: 0.9305 - val_acc: 0.4174 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8315 - acc: 0.5115 - precision: 0.1173 - recall: 1.2108 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.8427 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7375 - acc: 0.5412 - precision: 0.1172 - recall: 1.1936 - f1_metric: 0.2084 - val_loss: 0.9305 - val_acc: 0.8568 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6109 - acc: 0.5310 - precision: 0.1212 - recall: 1.2380 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.8556 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8796 - acc: 0.5347 - precision: 0.1209 - recall: 1.2216 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.8617 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3500 - acc: 0.5218 - precision: 0.1177 - recall: 1.2401 - f1_metric: 0.2093 - val_loss: 0.9305 - val_acc: 0.8250 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7985 - acc: 0.5062 - precision: 0.1190 - recall: 1.2450 - f1_metric: 0.2136 - val_loss: 0.9305 - val_acc: 0.8856 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7520 - acc: 0.5261 - precision: 0.1205 - recall: 1.2862 - f1_metric: 0.2153 - val_loss: 0.9305 - val_acc: 0.8213 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7682 - acc: 0.5048 - precision: 0.1231 - recall: 1.2540 - f1_metric: 0.2203 - val_loss: 0.9305 - val_acc: 0.8446 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.4367 - acc: 0.5132 - precision: 0.1286 - recall: 1.2435 - f1_metric: 0.2266 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9189 - acc: 0.4991 - precision: 0.1170 - recall: 1.2421 - f1_metric: 0.2093 - val_loss: 0.9305 - val_acc: 0.7999 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5508 - acc: 0.5306 - precision: 0.1129 - recall: 1.2398 - f1_metric: 0.2029 - val_loss: 0.9305 - val_acc: 0.8953 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7486 - acc: 0.5007 - precision: 0.1129 - recall: 1.2104 - f1_metric: 0.2028 - val_loss: 0.9305 - val_acc: 0.3886 - val_precision: 0.1313 - val_recall: 1.2751 - val_f1_metric: 0.2308\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0982 - acc: 0.4760 - precision: 0.1205 - recall: 1.2276 - f1_metric: 0.2154 - val_loss: 0.9305 - val_acc: 0.3170 - val_precision: 0.1313 - val_recall: 1.2927 - val_f1_metric: 0.2309\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7494 - acc: 0.4914 - precision: 0.1254 - recall: 1.2469 - f1_metric: 0.2229 - val_loss: 0.9305 - val_acc: 0.8378 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6939 - acc: 0.4965 - precision: 0.1205 - recall: 1.2291 - f1_metric: 0.2150 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8066 - acc: 0.4846 - precision: 0.1229 - recall: 1.2585 - f1_metric: 0.2193 - val_loss: 0.9305 - val_acc: 0.6206 - val_precision: 0.1323 - val_recall: 1.3040 - val_f1_metric: 0.2327\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8301 - acc: 0.4858 - precision: 0.1225 - recall: 1.2427 - f1_metric: 0.2193 - val_loss: 0.9305 - val_acc: 0.0796 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2560 - acc: 0.4573 - precision: 0.1260 - recall: 1.2302 - f1_metric: 0.2246 - val_loss: 0.9305 - val_acc: 0.9015 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7609 - acc: 0.4838 - precision: 0.1180 - recall: 1.2455 - f1_metric: 0.2113 - val_loss: 0.9305 - val_acc: 0.8819 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9462 - acc: 0.4920 - precision: 0.1191 - recall: 1.2019 - f1_metric: 0.2121 - val_loss: 0.9305 - val_acc: 0.4890 - val_precision: 0.1335 - val_recall: 1.3012 - val_f1_metric: 0.2345\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8505 - acc: 0.4738 - precision: 0.1246 - recall: 1.2574 - f1_metric: 0.2225 - val_loss: 0.9305 - val_acc: 0.8678 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8064 - acc: 0.4923 - precision: 0.1292 - recall: 1.2281 - f1_metric: 0.2293 - val_loss: 0.9305 - val_acc: 0.8470 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0933 - acc: 0.4784 - precision: 0.1205 - recall: 1.2536 - f1_metric: 0.2166 - val_loss: 0.9305 - val_acc: 0.9009 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1102 - acc: 0.4996 - precision: 0.1185 - recall: 1.2331 - f1_metric: 0.2122 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8416 - acc: 0.4924 - precision: 0.1121 - recall: 1.2102 - f1_metric: 0.2012 - val_loss: 0.9305 - val_acc: 0.8911 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7392 - acc: 0.5018 - precision: 0.1232 - recall: 1.2409 - f1_metric: 0.2183 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9100 - acc: 0.4782 - precision: 0.1250 - recall: 1.2266 - f1_metric: 0.2222 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7494 - acc: 0.5030 - precision: 0.1292 - recall: 1.2678 - f1_metric: 0.2300 - val_loss: 0.9305 - val_acc: 0.3415 - val_precision: 0.1321 - val_recall: 1.2927 - val_f1_metric: 0.2322\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8274 - acc: 0.4766 - precision: 0.1185 - recall: 1.2163 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7235 - acc: 0.4866 - precision: 0.1266 - recall: 1.2701 - f1_metric: 0.2266 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9026 - acc: 0.4958 - precision: 0.1123 - recall: 1.2530 - f1_metric: 0.2028 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7964 - acc: 0.5106 - precision: 0.1249 - recall: 1.2326 - f1_metric: 0.2227 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6965 - acc: 0.4955 - precision: 0.1139 - recall: 1.2129 - f1_metric: 0.2046 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5534 - acc: 0.4693 - precision: 0.1114 - recall: 1.2522 - f1_metric: 0.2001 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8477 - acc: 0.4750 - precision: 0.1130 - recall: 1.2154 - f1_metric: 0.2032 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7540 - acc: 0.4931 - precision: 0.1227 - recall: 1.2736 - f1_metric: 0.2191 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7882 - acc: 0.4830 - precision: 0.1283 - recall: 1.2801 - f1_metric: 0.2290 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9601 - acc: 0.5071 - precision: 0.1194 - recall: 1.2367 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1180 - acc: 0.4838 - precision: 0.1238 - recall: 1.2799 - f1_metric: 0.2218 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1399 - acc: 0.4643 - precision: 0.1237 - recall: 1.2929 - f1_metric: 0.2214 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7863 - acc: 0.4725 - precision: 0.1210 - recall: 1.2973 - f1_metric: 0.2164 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9210 - acc: 0.4909 - precision: 0.1203 - recall: 1.2713 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7093 - acc: 0.5172 - precision: 0.1161 - recall: 1.1685 - f1_metric: 0.2077 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9894 - acc: 0.4833 - precision: 0.1123 - recall: 1.2493 - f1_metric: 0.2024 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3137 - acc: 0.4810 - precision: 0.1280 - recall: 1.2445 - f1_metric: 0.2264 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6857 - acc: 0.4873 - precision: 0.1149 - recall: 1.2166 - f1_metric: 0.2065 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9624 - acc: 0.5047 - precision: 0.1186 - recall: 1.2708 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.2252 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8214 - acc: 0.4817 - precision: 0.1144 - recall: 1.2527 - f1_metric: 0.2054 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6727 - acc: 0.4860 - precision: 0.1246 - recall: 1.2242 - f1_metric: 0.2212 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 24 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.9259 - acc: 0.4227 - precision: 0.1200 - recall: 1.2116 - f1_metric: 0.2131 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9832 - acc: 0.4634 - precision: 0.1271 - recall: 1.2131 - f1_metric: 0.2263 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8790 - acc: 0.4481 - precision: 0.1172 - recall: 1.1634 - f1_metric: 0.2080 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6817 - acc: 0.4684 - precision: 0.1135 - recall: 1.2139 - f1_metric: 0.2038 - val_loss: 0.9305 - val_acc: 0.0783 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9840 - acc: 0.4589 - precision: 0.1089 - recall: 1.2787 - f1_metric: 0.1966 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8546 - acc: 0.4791 - precision: 0.1157 - recall: 1.2178 - f1_metric: 0.2073 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8279 - acc: 0.4691 - precision: 0.1238 - recall: 1.2578 - f1_metric: 0.2209 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9496 - acc: 0.4524 - precision: 0.1173 - recall: 1.2196 - f1_metric: 0.2097 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9784 - acc: 0.4685 - precision: 0.1249 - recall: 1.2512 - f1_metric: 0.2221 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6354 - acc: 0.4763 - precision: 0.1195 - recall: 1.2559 - f1_metric: 0.2131 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0236 - acc: 0.4663 - precision: 0.1257 - recall: 1.2445 - f1_metric: 0.2240 - val_loss: 0.9305 - val_acc: 0.0734 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7026 - acc: 0.4495 - precision: 0.1196 - recall: 1.2760 - f1_metric: 0.2146 - val_loss: 0.9305 - val_acc: 0.0814 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8863 - acc: 0.4739 - precision: 0.1117 - recall: 1.2449 - f1_metric: 0.2013 - val_loss: 0.9305 - val_acc: 0.0900 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7472 - acc: 0.4643 - precision: 0.1264 - recall: 1.3123 - f1_metric: 0.2252 - val_loss: 0.9305 - val_acc: 0.1016 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7551 - acc: 0.4819 - precision: 0.1201 - recall: 1.2027 - f1_metric: 0.2142 - val_loss: 0.9305 - val_acc: 0.0949 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9777 - acc: 0.4880 - precision: 0.1229 - recall: 1.2181 - f1_metric: 0.2195 - val_loss: 0.9305 - val_acc: 0.1322 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7326 - acc: 0.4902 - precision: 0.1210 - recall: 1.2697 - f1_metric: 0.2171 - val_loss: 0.9305 - val_acc: 0.1224 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1203 - acc: 0.4752 - precision: 0.1204 - recall: 1.1844 - f1_metric: 0.2143 - val_loss: 0.9305 - val_acc: 0.1047 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6483 - acc: 0.4855 - precision: 0.1158 - recall: 1.2532 - f1_metric: 0.2078 - val_loss: 0.9305 - val_acc: 0.0759 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8226 - acc: 0.4786 - precision: 0.1043 - recall: 1.1705 - f1_metric: 0.1879 - val_loss: 0.9305 - val_acc: 0.1316 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6250 - acc: 0.4877 - precision: 0.1118 - recall: 1.2125 - f1_metric: 0.2000 - val_loss: 0.9305 - val_acc: 0.1095 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8640 - acc: 0.4990 - precision: 0.1190 - recall: 1.2502 - f1_metric: 0.2125 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1396 - acc: 0.4908 - precision: 0.1239 - recall: 1.2883 - f1_metric: 0.2220 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8131 - acc: 0.4829 - precision: 0.1186 - recall: 1.2089 - f1_metric: 0.2119 - val_loss: 0.9305 - val_acc: 0.0759 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7388 - acc: 0.4709 - precision: 0.1134 - recall: 1.1647 - f1_metric: 0.2030 - val_loss: 0.9305 - val_acc: 0.1377 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0702 - acc: 0.4833 - precision: 0.1380 - recall: 1.3200 - f1_metric: 0.2439 - val_loss: 0.9305 - val_acc: 0.1047 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6243 - acc: 0.4950 - precision: 0.1196 - recall: 1.2501 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7559 - acc: 0.4786 - precision: 0.1133 - recall: 1.2612 - f1_metric: 0.2038 - val_loss: 0.9305 - val_acc: 0.1799 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2325\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9100 - acc: 0.5062 - precision: 0.1163 - recall: 1.2477 - f1_metric: 0.2080 - val_loss: 0.9305 - val_acc: 0.1903 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7172 - acc: 0.4912 - precision: 0.1201 - recall: 1.2160 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.0900 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7108 - acc: 0.4809 - precision: 0.1207 - recall: 1.1873 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.1983 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1213 - acc: 0.4940 - precision: 0.1160 - recall: 1.1658 - f1_metric: 0.2067 - val_loss: 0.9305 - val_acc: 0.0771 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8835 - acc: 0.5024 - precision: 0.1200 - recall: 1.2002 - f1_metric: 0.2142 - val_loss: 0.9305 - val_acc: 0.1603 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6849 - acc: 0.5025 - precision: 0.1186 - recall: 1.2353 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.0814 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7068 - acc: 0.4742 - precision: 0.1135 - recall: 1.2022 - f1_metric: 0.2035 - val_loss: 0.9305 - val_acc: 0.1542 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8615 - acc: 0.5013 - precision: 0.1205 - recall: 1.2653 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6023 - acc: 0.4646 - precision: 0.1233 - recall: 1.2800 - f1_metric: 0.2211 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7648 - acc: 0.5027 - precision: 0.1094 - recall: 1.2003 - f1_metric: 0.1964 - val_loss: 0.9305 - val_acc: 0.0973 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8461 - acc: 0.4944 - precision: 0.1164 - recall: 1.2214 - f1_metric: 0.2086 - val_loss: 0.9305 - val_acc: 0.8972 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9107 - acc: 0.5116 - precision: 0.1138 - recall: 1.1536 - f1_metric: 0.2036 - val_loss: 0.9305 - val_acc: 0.0808 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7740 - acc: 0.4707 - precision: 0.1146 - recall: 1.2439 - f1_metric: 0.2066 - val_loss: 0.9305 - val_acc: 0.0832 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0507 - acc: 0.4778 - precision: 0.1259 - recall: 1.2563 - f1_metric: 0.2243 - val_loss: 0.9305 - val_acc: 0.1089 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0423 - acc: 0.4798 - precision: 0.1149 - recall: 1.2014 - f1_metric: 0.2053 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8063 - acc: 0.4857 - precision: 0.1224 - recall: 1.2233 - f1_metric: 0.2174 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7544 - acc: 0.4820 - precision: 0.1231 - recall: 1.2667 - f1_metric: 0.2203 - val_loss: 0.9305 - val_acc: 0.0777 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9356 - acc: 0.4800 - precision: 0.1188 - recall: 1.2552 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1613 - acc: 0.4881 - precision: 0.1299 - recall: 1.2005 - f1_metric: 0.2295 - val_loss: 0.9305 - val_acc: 0.0704 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6808 - acc: 0.4658 - precision: 0.1168 - recall: 1.2341 - f1_metric: 0.2091 - val_loss: 0.9305 - val_acc: 0.1867 - val_precision: 0.1325 - val_recall: 1.3040 - val_f1_metric: 0.2331\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6123 - acc: 0.4862 - precision: 0.1111 - recall: 1.1739 - f1_metric: 0.1986 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7116 - acc: 0.4747 - precision: 0.1159 - recall: 1.2156 - f1_metric: 0.2081 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 25 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.7644 - acc: 0.4709 - precision: 0.1247 - recall: 1.2399 - f1_metric: 0.2223 - val_loss: 0.9305 - val_acc: 0.8898 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6871 - acc: 0.5007 - precision: 0.1132 - recall: 1.2168 - f1_metric: 0.2029 - val_loss: 0.9305 - val_acc: 0.8898 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9131 - acc: 0.5257 - precision: 0.1159 - recall: 1.1975 - f1_metric: 0.2080 - val_loss: 0.9305 - val_acc: 0.8990 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8431 - acc: 0.5049 - precision: 0.1205 - recall: 1.2586 - f1_metric: 0.2150 - val_loss: 0.9305 - val_acc: 0.8825 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6583 - acc: 0.4914 - precision: 0.1215 - recall: 1.2338 - f1_metric: 0.2170 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7443 - acc: 0.4931 - precision: 0.1199 - recall: 1.2274 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.7717 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.4948 - acc: 0.4822 - precision: 0.1151 - recall: 1.2164 - f1_metric: 0.2062 - val_loss: 0.9305 - val_acc: 0.8341 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3453 - acc: 0.4994 - precision: 0.1232 - recall: 1.2247 - f1_metric: 0.2194 - val_loss: 0.9305 - val_acc: 0.3727 - val_precision: 0.1313 - val_recall: 1.2943 - val_f1_metric: 0.2310\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7311 - acc: 0.4679 - precision: 0.1145 - recall: 1.1879 - f1_metric: 0.2049 - val_loss: 0.9305 - val_acc: 0.2791 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9081 - acc: 0.4675 - precision: 0.1088 - recall: 1.2272 - f1_metric: 0.1956 - val_loss: 0.9305 - val_acc: 0.6175 - val_precision: 0.1315 - val_recall: 1.2975 - val_f1_metric: 0.2311\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8030 - acc: 0.4900 - precision: 0.1228 - recall: 1.2372 - f1_metric: 0.2193 - val_loss: 0.9305 - val_acc: 0.2173 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8733 - acc: 0.4831 - precision: 0.1189 - recall: 1.2371 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.8923 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0483 - acc: 0.4995 - precision: 0.1159 - recall: 1.2191 - f1_metric: 0.2074 - val_loss: 0.9305 - val_acc: 0.8354 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6483 - acc: 0.4784 - precision: 0.1199 - recall: 1.2431 - f1_metric: 0.2154 - val_loss: 0.9305 - val_acc: 0.8415 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2777 - acc: 0.5034 - precision: 0.1210 - recall: 1.2954 - f1_metric: 0.2176 - val_loss: 0.9305 - val_acc: 0.2264 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1094 - acc: 0.4928 - precision: 0.1153 - recall: 1.1870 - f1_metric: 0.2063 - val_loss: 0.9305 - val_acc: 0.1554 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7425 - acc: 0.4730 - precision: 0.1179 - recall: 1.2612 - f1_metric: 0.2109 - val_loss: 0.9305 - val_acc: 0.1193 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8407 - acc: 0.4929 - precision: 0.1231 - recall: 1.2446 - f1_metric: 0.2200 - val_loss: 0.9305 - val_acc: 0.4871 - val_precision: 0.1312 - val_recall: 1.2943 - val_f1_metric: 0.2308\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8891 - acc: 0.4952 - precision: 0.1191 - recall: 1.2313 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2017 - acc: 0.4762 - precision: 0.1239 - recall: 1.2417 - f1_metric: 0.2206 - val_loss: 0.9305 - val_acc: 0.0832 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7034 - acc: 0.4891 - precision: 0.1128 - recall: 1.2066 - f1_metric: 0.2020 - val_loss: 0.9305 - val_acc: 0.0961 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8839 - acc: 0.4920 - precision: 0.1201 - recall: 1.2286 - f1_metric: 0.2151 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7053 - acc: 0.4663 - precision: 0.1173 - recall: 1.2018 - f1_metric: 0.2098 - val_loss: 0.9305 - val_acc: 0.2124 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0213 - acc: 0.4854 - precision: 0.1225 - recall: 1.1962 - f1_metric: 0.2176 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6401 - acc: 0.4908 - precision: 0.1186 - recall: 1.2522 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0317 - acc: 0.4656 - precision: 0.1218 - recall: 1.2690 - f1_metric: 0.2177 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9270 - acc: 0.4839 - precision: 0.1212 - recall: 1.2673 - f1_metric: 0.2171 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0624 - acc: 0.4648 - precision: 0.1182 - recall: 1.2281 - f1_metric: 0.2110 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8484 - acc: 0.4767 - precision: 0.1256 - recall: 1.2060 - f1_metric: 0.2228 - val_loss: 0.9305 - val_acc: 0.0869 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7110 - acc: 0.4732 - precision: 0.1175 - recall: 1.2583 - f1_metric: 0.2112 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0692 - acc: 0.4751 - precision: 0.1133 - recall: 1.2044 - f1_metric: 0.2044 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6819 - acc: 0.5015 - precision: 0.1166 - recall: 1.2675 - f1_metric: 0.2090 - val_loss: 0.9305 - val_acc: 0.0832 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0877 - acc: 0.4833 - precision: 0.1202 - recall: 1.2541 - f1_metric: 0.2151 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9502 - acc: 0.4868 - precision: 0.1199 - recall: 1.2681 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.4851 - acc: 0.4936 - precision: 0.1212 - recall: 1.2235 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.0716 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9668 - acc: 0.4840 - precision: 0.1224 - recall: 1.2618 - f1_metric: 0.2178 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5964 - acc: 0.4803 - precision: 0.1188 - recall: 1.2531 - f1_metric: 0.2115 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7499 - acc: 0.4925 - precision: 0.1208 - recall: 1.2245 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.3188 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7271 - acc: 0.4828 - precision: 0.1180 - recall: 1.2245 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7593 - acc: 0.5104 - precision: 0.1118 - recall: 1.2004 - f1_metric: 0.2009 - val_loss: 0.9305 - val_acc: 0.1457 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7586 - acc: 0.4787 - precision: 0.1185 - recall: 1.2062 - f1_metric: 0.2121 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8676 - acc: 0.4966 - precision: 0.1200 - recall: 1.2409 - f1_metric: 0.2141 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6739 - acc: 0.4608 - precision: 0.1159 - recall: 1.2123 - f1_metric: 0.2073 - val_loss: 0.9305 - val_acc: 0.0741 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2651 - acc: 0.4807 - precision: 0.1274 - recall: 1.2631 - f1_metric: 0.2262 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1447 - acc: 0.4774 - precision: 0.1251 - recall: 1.3101 - f1_metric: 0.2241 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7711 - acc: 0.4812 - precision: 0.1125 - recall: 1.2196 - f1_metric: 0.2029 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1110 - acc: 0.4885 - precision: 0.1212 - recall: 1.2678 - f1_metric: 0.2174 - val_loss: 0.9305 - val_acc: 0.0722 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8006 - acc: 0.4760 - precision: 0.1163 - recall: 1.1991 - f1_metric: 0.2079 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8987 - acc: 0.4819 - precision: 0.1277 - recall: 1.2523 - f1_metric: 0.2277 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7736 - acc: 0.4925 - precision: 0.1205 - recall: 1.2253 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 26 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 5ms/step - loss: 0.6500 - acc: 0.5138 - precision: 0.1168 - recall: 1.2330 - f1_metric: 0.2095 - val_loss: 0.9305 - val_acc: 0.8911 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7976 - acc: 0.4949 - precision: 0.1227 - recall: 1.2759 - f1_metric: 0.2194 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8925 - acc: 0.5119 - precision: 0.1110 - recall: 1.2181 - f1_metric: 0.2000 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3789 - acc: 0.5286 - precision: 0.1253 - recall: 1.2994 - f1_metric: 0.2243 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7263 - acc: 0.5010 - precision: 0.1196 - recall: 1.2018 - f1_metric: 0.2133 - val_loss: 0.9305 - val_acc: 0.8709 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8981 - acc: 0.4783 - precision: 0.1170 - recall: 1.2498 - f1_metric: 0.2105 - val_loss: 0.9305 - val_acc: 0.5679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3404 - acc: 0.4627 - precision: 0.1175 - recall: 1.2679 - f1_metric: 0.2115 - val_loss: 0.9305 - val_acc: 0.8574 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9148 - acc: 0.4439 - precision: 0.1188 - recall: 1.1944 - f1_metric: 0.2121 - val_loss: 0.9305 - val_acc: 0.6371 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6836 - acc: 0.4404 - precision: 0.1223 - recall: 1.2832 - f1_metric: 0.2196 - val_loss: 0.9305 - val_acc: 0.2821 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9386 - acc: 0.4292 - precision: 0.1188 - recall: 1.1856 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.6634 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8985 - acc: 0.4377 - precision: 0.1201 - recall: 1.2753 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.6218 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8526 - acc: 0.4580 - precision: 0.1209 - recall: 1.2555 - f1_metric: 0.2163 - val_loss: 0.9305 - val_acc: 0.6089 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0487 - acc: 0.4624 - precision: 0.1173 - recall: 1.3148 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.0936 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0272 - acc: 0.4697 - precision: 0.1206 - recall: 1.2502 - f1_metric: 0.2158 - val_loss: 0.9305 - val_acc: 0.7521 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7469 - acc: 0.4932 - precision: 0.1250 - recall: 1.2129 - f1_metric: 0.2222 - val_loss: 0.9305 - val_acc: 0.6818 - val_precision: 0.1311 - val_recall: 1.2943 - val_f1_metric: 0.2306\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7000 - acc: 0.4686 - precision: 0.1177 - recall: 1.2500 - f1_metric: 0.2099 - val_loss: 0.9305 - val_acc: 0.6304 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9945 - acc: 0.4571 - precision: 0.1185 - recall: 1.1946 - f1_metric: 0.2124 - val_loss: 0.9305 - val_acc: 0.5447 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7658 - acc: 0.4751 - precision: 0.1243 - recall: 1.2889 - f1_metric: 0.2226 - val_loss: 0.9305 - val_acc: 0.7491 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9565 - acc: 0.4704 - precision: 0.1190 - recall: 1.2888 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.7209 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7638 - acc: 0.4614 - precision: 0.1205 - recall: 1.1699 - f1_metric: 0.2141 - val_loss: 0.9305 - val_acc: 0.1144 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6919 - acc: 0.4661 - precision: 0.1199 - recall: 1.2270 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.3911 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7547 - acc: 0.4777 - precision: 0.1116 - recall: 1.2279 - f1_metric: 0.2009 - val_loss: 0.9305 - val_acc: 0.5991 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7835 - acc: 0.4554 - precision: 0.1164 - recall: 1.2901 - f1_metric: 0.2097 - val_loss: 0.9305 - val_acc: 0.5869 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9145 - acc: 0.4551 - precision: 0.1212 - recall: 1.2591 - f1_metric: 0.2166 - val_loss: 0.9305 - val_acc: 0.0900 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7988 - acc: 0.4678 - precision: 0.1203 - recall: 1.2173 - f1_metric: 0.2138 - val_loss: 0.9305 - val_acc: 0.4547 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2320\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8211 - acc: 0.4564 - precision: 0.1201 - recall: 1.2037 - f1_metric: 0.2141 - val_loss: 0.9305 - val_acc: 0.2778 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9263 - acc: 0.4599 - precision: 0.1258 - recall: 1.2579 - f1_metric: 0.2249 - val_loss: 0.9305 - val_acc: 0.3562 - val_precision: 0.1321 - val_recall: 1.3040 - val_f1_metric: 0.2324\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6591 - acc: 0.4699 - precision: 0.1126 - recall: 1.1938 - f1_metric: 0.2019 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8901 - acc: 0.4539 - precision: 0.1179 - recall: 1.2554 - f1_metric: 0.2110 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7796 - acc: 0.4490 - precision: 0.1157 - recall: 1.2911 - f1_metric: 0.2077 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2360 - acc: 0.4530 - precision: 0.1183 - recall: 1.2415 - f1_metric: 0.2129 - val_loss: 0.9305 - val_acc: 0.5985 - val_precision: 0.1324 - val_recall: 1.3040 - val_f1_metric: 0.2328\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7362 - acc: 0.4720 - precision: 0.1106 - recall: 1.1819 - f1_metric: 0.1983 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7386 - acc: 0.4760 - precision: 0.1235 - recall: 1.2332 - f1_metric: 0.2207 - val_loss: 0.9305 - val_acc: 0.1506 - val_precision: 0.1312 - val_recall: 1.2991 - val_f1_metric: 0.2308\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8629 - acc: 0.4680 - precision: 0.1170 - recall: 1.2521 - f1_metric: 0.2084 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7696 - acc: 0.4684 - precision: 0.1146 - recall: 1.2228 - f1_metric: 0.2060 - val_loss: 0.9305 - val_acc: 0.8488 - val_precision: 0.1323 - val_recall: 1.3040 - val_f1_metric: 0.2326\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7137 - acc: 0.4805 - precision: 0.1223 - recall: 1.2103 - f1_metric: 0.2183 - val_loss: 0.9305 - val_acc: 0.8807 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7246 - acc: 0.4940 - precision: 0.1092 - recall: 1.2022 - f1_metric: 0.1967 - val_loss: 0.9305 - val_acc: 0.8813 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6854 - acc: 0.5093 - precision: 0.1224 - recall: 1.2422 - f1_metric: 0.2193 - val_loss: 0.9305 - val_acc: 0.0698 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8342 - acc: 0.4744 - precision: 0.1164 - recall: 1.2027 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9345 - acc: 0.4778 - precision: 0.1095 - recall: 1.2043 - f1_metric: 0.1972 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7708 - acc: 0.4658 - precision: 0.1194 - recall: 1.2621 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9279 - acc: 0.4881 - precision: 0.1233 - recall: 1.2200 - f1_metric: 0.2194 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3493 - acc: 0.4560 - precision: 0.1141 - recall: 1.2959 - f1_metric: 0.2052 - val_loss: 0.9305 - val_acc: 0.1573 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7504 - acc: 0.4919 - precision: 0.1142 - recall: 1.1742 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8908 - acc: 0.4976 - precision: 0.1177 - recall: 1.2514 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.1022 - val_precision: 0.1312 - val_recall: 1.2991 - val_f1_metric: 0.2308\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7430 - acc: 0.4689 - precision: 0.1206 - recall: 1.2343 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0741 - acc: 0.4850 - precision: 0.1176 - recall: 1.2124 - f1_metric: 0.2107 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6952 - acc: 0.4951 - precision: 0.1166 - recall: 1.2245 - f1_metric: 0.2096 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8063 - acc: 0.4907 - precision: 0.1189 - recall: 1.2198 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8502 - acc: 0.4935 - precision: 0.1212 - recall: 1.2379 - f1_metric: 0.2172 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 27 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.9556 - acc: 0.4946 - precision: 0.1146 - recall: 1.2418 - f1_metric: 0.2066 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9048 - acc: 0.5231 - precision: 0.1172 - recall: 1.2621 - f1_metric: 0.2106 - val_loss: 0.9305 - val_acc: 0.8807 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7134 - acc: 0.5259 - precision: 0.1133 - recall: 1.1926 - f1_metric: 0.2034 - val_loss: 0.9305 - val_acc: 0.8935 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8167 - acc: 0.5179 - precision: 0.1217 - recall: 1.2591 - f1_metric: 0.2178 - val_loss: 0.9305 - val_acc: 0.9009 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8941 - acc: 0.5342 - precision: 0.1151 - recall: 1.2393 - f1_metric: 0.2065 - val_loss: 0.9305 - val_acc: 0.8715 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2817 - acc: 0.4778 - precision: 0.1293 - recall: 1.2312 - f1_metric: 0.2285 - val_loss: 0.9305 - val_acc: 0.8556 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7669 - acc: 0.4893 - precision: 0.1217 - recall: 1.2217 - f1_metric: 0.2177 - val_loss: 0.9305 - val_acc: 0.8905 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9379 - acc: 0.5068 - precision: 0.1189 - recall: 1.2375 - f1_metric: 0.2132 - val_loss: 0.9305 - val_acc: 0.8758 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7496 - acc: 0.4943 - precision: 0.1202 - recall: 1.2527 - f1_metric: 0.2142 - val_loss: 0.9305 - val_acc: 0.8403 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9103 - acc: 0.4995 - precision: 0.1220 - recall: 1.2947 - f1_metric: 0.2198 - val_loss: 0.9305 - val_acc: 0.8244 - val_precision: 0.1311 - val_recall: 1.2991 - val_f1_metric: 0.2306\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7467 - acc: 0.4921 - precision: 0.1229 - recall: 1.2602 - f1_metric: 0.2188 - val_loss: 0.9305 - val_acc: 0.7564 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2193 - acc: 0.4887 - precision: 0.1181 - recall: 1.2467 - f1_metric: 0.2123 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6914 - acc: 0.5071 - precision: 0.1180 - recall: 1.2493 - f1_metric: 0.2117 - val_loss: 0.9305 - val_acc: 0.8874 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9059 - acc: 0.4913 - precision: 0.1199 - recall: 1.2254 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.4070 - val_precision: 0.1317 - val_recall: 1.2911 - val_f1_metric: 0.2315\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7785 - acc: 0.4774 - precision: 0.1213 - recall: 1.2654 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.8121 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9025 - acc: 0.4641 - precision: 0.1222 - recall: 1.2132 - f1_metric: 0.2178 - val_loss: 0.9305 - val_acc: 0.1475 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2924 - acc: 0.4858 - precision: 0.1216 - recall: 1.2392 - f1_metric: 0.2171 - val_loss: 0.9305 - val_acc: 0.5330 - val_precision: 0.1323 - val_recall: 1.3040 - val_f1_metric: 0.2327\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6199 - acc: 0.4740 - precision: 0.1213 - recall: 1.2039 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.8770 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8118 - acc: 0.4841 - precision: 0.1247 - recall: 1.2783 - f1_metric: 0.2233 - val_loss: 0.9305 - val_acc: 0.1579 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1504 - acc: 0.4690 - precision: 0.1254 - recall: 1.2762 - f1_metric: 0.2234 - val_loss: 0.9305 - val_acc: 0.8831 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9905 - acc: 0.4876 - precision: 0.1257 - recall: 1.2619 - f1_metric: 0.2243 - val_loss: 0.9305 - val_acc: 0.8874 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8388 - acc: 0.4934 - precision: 0.1252 - recall: 1.2861 - f1_metric: 0.2245 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9598 - acc: 0.4835 - precision: 0.1264 - recall: 1.2291 - f1_metric: 0.2240 - val_loss: 0.9305 - val_acc: 0.1151 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0690 - acc: 0.4631 - precision: 0.1242 - recall: 1.2682 - f1_metric: 0.2212 - val_loss: 0.9305 - val_acc: 0.8121 - val_precision: 0.1332 - val_recall: 1.3040 - val_f1_metric: 0.2342\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0090 - acc: 0.5042 - precision: 0.1174 - recall: 1.2734 - f1_metric: 0.2111 - val_loss: 0.9305 - val_acc: 0.1083 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7890 - acc: 0.4835 - precision: 0.1152 - recall: 1.2000 - f1_metric: 0.2068 - val_loss: 0.9305 - val_acc: 0.1083 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7057 - acc: 0.4799 - precision: 0.1141 - recall: 1.2290 - f1_metric: 0.2049 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7506 - acc: 0.5053 - precision: 0.1172 - recall: 1.2379 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8216 - acc: 0.5034 - precision: 0.1242 - recall: 1.2516 - f1_metric: 0.2206 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8891 - acc: 0.4957 - precision: 0.1196 - recall: 1.2097 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6043 - acc: 0.5347 - precision: 0.1249 - recall: 1.2006 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1434 - acc: 0.4880 - precision: 0.1197 - recall: 1.2264 - f1_metric: 0.2068 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7871 - acc: 0.5148 - precision: 0.1297 - recall: 1.1925 - f1_metric: 0.2208 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6164 - acc: 0.5426 - precision: 0.1547 - recall: 1.2107 - f1_metric: 0.2143 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7219 - acc: 0.5274 - precision: 0.1245 - recall: 1.1940 - f1_metric: 0.2209 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9011 - acc: 0.5079 - precision: 0.1200 - recall: 1.2177 - f1_metric: 0.2141 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9184 - acc: 0.5123 - precision: 0.1152 - recall: 1.1170 - f1_metric: 0.2036 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6557 - acc: 0.4792 - precision: 0.1167 - recall: 1.2897 - f1_metric: 0.2091 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8414 - acc: 0.4769 - precision: 467581.8195 - recall: 1.2345 - f1_metric: 0.2438 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3990 - acc: 0.5641 - precision: 5004653.7484 - recall: 1.1203 - f1_metric: 0.5904 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 18461538.0000 - val_recall: 0.6079 - val_f1_metric: 1.2158\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9220 - acc: 0.6005 - precision: 13050337.9660 - recall: 0.8458 - f1_metric: 0.9511 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9875 - acc: 0.5552 - precision: 8355433.8144 - recall: 0.9046 - f1_metric: 0.6031 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 18461538.0000 - val_recall: 0.6079 - val_f1_metric: 1.2158\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7311 - acc: 0.5665 - precision: 6075131.3981 - recall: 0.8961 - f1_metric: 0.6169 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8215 - acc: 0.5108 - precision: 1621210.2075 - recall: 1.1448 - f1_metric: 0.2863 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9637 - acc: 0.4911 - precision: 83254.2059 - recall: 1.1819 - f1_metric: 0.2239 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7220 - acc: 0.4712 - precision: 138254.9419 - recall: 1.1733 - f1_metric: 0.2187 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8296 - acc: 0.4552 - precision: 612208.3602 - recall: 1.2146 - f1_metric: 0.2460 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7008 - acc: 0.4768 - precision: 276577.0790 - recall: 1.2002 - f1_metric: 0.2196 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0025 - acc: 0.4977 - precision: 1113720.3837 - recall: 1.2255 - f1_metric: 0.2318 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0500 - acc: 0.5010 - precision: 650531.6430 - recall: 1.2903 - f1_metric: 0.2728 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 28 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 1.1059 - acc: 0.4773 - precision: 0.1216 - recall: 1.2689 - f1_metric: 0.2175 - val_loss: 0.9305 - val_acc: 0.4994 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.4363 - acc: 0.4891 - precision: 0.1177 - recall: 1.2192 - f1_metric: 0.2114 - val_loss: 0.9305 - val_acc: 0.8819 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8648 - acc: 0.4802 - precision: 0.1170 - recall: 1.2513 - f1_metric: 0.2091 - val_loss: 0.9305 - val_acc: 0.8452 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9201 - acc: 0.4673 - precision: 0.1197 - recall: 1.2217 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.7778 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9994 - acc: 0.5015 - precision: 0.1185 - recall: 1.2459 - f1_metric: 0.2129 - val_loss: 0.9305 - val_acc: 0.8507 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8067 - acc: 0.4758 - precision: 0.1200 - recall: 1.2282 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.8464 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8105 - acc: 0.4858 - precision: 0.1159 - recall: 1.2274 - f1_metric: 0.2077 - val_loss: 0.9305 - val_acc: 0.8856 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8681 - acc: 0.4885 - precision: 0.1211 - recall: 1.2554 - f1_metric: 0.2176 - val_loss: 0.9305 - val_acc: 0.8917 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1277 - acc: 0.4828 - precision: 0.1219 - recall: 1.2122 - f1_metric: 0.2171 - val_loss: 0.9305 - val_acc: 0.8623 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6298 - acc: 0.4910 - precision: 0.1080 - recall: 1.1811 - f1_metric: 0.1936 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8574 - acc: 0.4954 - precision: 0.1265 - recall: 1.2570 - f1_metric: 0.2261 - val_loss: 0.9305 - val_acc: 0.8623 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6617 - acc: 0.4782 - precision: 0.1179 - recall: 1.2284 - f1_metric: 0.2107 - val_loss: 0.9305 - val_acc: 0.8599 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7319 - acc: 0.4844 - precision: 0.1131 - recall: 1.2246 - f1_metric: 0.2030 - val_loss: 0.9305 - val_acc: 0.8091 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7532 - acc: 0.4741 - precision: 0.1205 - recall: 1.2013 - f1_metric: 0.2152 - val_loss: 0.9305 - val_acc: 0.7105 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9704 - acc: 0.4758 - precision: 0.1207 - recall: 1.2176 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.8482 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1688 - acc: 0.4874 - precision: 0.1188 - recall: 1.2240 - f1_metric: 0.2127 - val_loss: 0.9305 - val_acc: 0.8360 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.4018 - acc: 0.4912 - precision: 0.1113 - recall: 1.1756 - f1_metric: 0.1994 - val_loss: 0.9305 - val_acc: 0.8703 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8662 - acc: 0.4786 - precision: 0.1200 - recall: 1.2792 - f1_metric: 0.2156 - val_loss: 0.9305 - val_acc: 0.8072 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6441 - acc: 0.4651 - precision: 0.1227 - recall: 1.2433 - f1_metric: 0.2173 - val_loss: 0.9305 - val_acc: 0.7827 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7236 - acc: 0.4882 - precision: 0.1203 - recall: 1.2440 - f1_metric: 0.2153 - val_loss: 0.9305 - val_acc: 0.8341 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8981 - acc: 0.4879 - precision: 0.1132 - recall: 1.2419 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.8439 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8838 - acc: 0.4885 - precision: 0.1215 - recall: 1.2279 - f1_metric: 0.2165 - val_loss: 0.9305 - val_acc: 0.8703 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7184 - acc: 0.4940 - precision: 0.1105 - recall: 1.2070 - f1_metric: 0.1977 - val_loss: 0.9305 - val_acc: 0.8782 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8169 - acc: 0.4834 - precision: 0.1260 - recall: 1.2464 - f1_metric: 0.2239 - val_loss: 0.9305 - val_acc: 0.4253 - val_precision: 0.1320 - val_recall: 1.3040 - val_f1_metric: 0.2322\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6801 - acc: 0.4654 - precision: 0.1174 - recall: 1.2776 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.8543 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7188 - acc: 0.4793 - precision: 0.1123 - recall: 1.2692 - f1_metric: 0.2023 - val_loss: 0.9305 - val_acc: 0.8880 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9853 - acc: 0.4724 - precision: 0.1215 - recall: 1.2904 - f1_metric: 0.2163 - val_loss: 0.9305 - val_acc: 0.8770 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7183 - acc: 0.4712 - precision: 0.1191 - recall: 1.1980 - f1_metric: 0.2136 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8595 - acc: 0.5067 - precision: 0.1158 - recall: 1.2806 - f1_metric: 0.2085 - val_loss: 0.9305 - val_acc: 0.0838 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0958 - acc: 0.4848 - precision: 0.1152 - recall: 1.1914 - f1_metric: 0.2055 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9188 - acc: 0.5064 - precision: 0.1086 - recall: 1.2017 - f1_metric: 0.1957 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8620 - acc: 0.4864 - precision: 0.1228 - recall: 1.2553 - f1_metric: 0.2202 - val_loss: 0.9305 - val_acc: 0.9015 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9755 - acc: 0.5040 - precision: 0.1131 - recall: 1.2215 - f1_metric: 0.2022 - val_loss: 0.9305 - val_acc: 0.8562 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0634 - acc: 0.4828 - precision: 0.1245 - recall: 1.2244 - f1_metric: 0.2215 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7693 - acc: 0.4807 - precision: 0.1192 - recall: 1.2886 - f1_metric: 0.2146 - val_loss: 0.9305 - val_acc: 0.3176 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7651 - acc: 0.4682 - precision: 0.1117 - recall: 1.1852 - f1_metric: 0.2004 - val_loss: 0.9305 - val_acc: 0.7711 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8689 - acc: 0.4869 - precision: 0.1076 - recall: 1.2952 - f1_metric: 0.1948 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6307 - acc: 0.4715 - precision: 0.1190 - recall: 1.2304 - f1_metric: 0.2129 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9200 - acc: 0.4888 - precision: 0.1179 - recall: 1.1591 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6989 - acc: 0.4866 - precision: 0.1216 - recall: 1.2294 - f1_metric: 0.2178 - val_loss: 0.9305 - val_acc: 0.8335 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6226 - acc: 0.4816 - precision: 0.1284 - recall: 1.2888 - f1_metric: 0.2288 - val_loss: 0.9305 - val_acc: 0.1004 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6690 - acc: 0.4922 - precision: 0.1210 - recall: 1.2476 - f1_metric: 0.2160 - val_loss: 0.9305 - val_acc: 0.8996 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8462 - acc: 0.4960 - precision: 0.1169 - recall: 1.2474 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.8947 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8649 - acc: 0.4772 - precision: 0.1182 - recall: 1.2770 - f1_metric: 0.2124 - val_loss: 0.9305 - val_acc: 0.9033 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7524 - acc: 0.4823 - precision: 0.1213 - recall: 1.1946 - f1_metric: 0.2151 - val_loss: 0.9305 - val_acc: 0.8421 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1894 - acc: 0.4834 - precision: 0.1246 - recall: 1.2206 - f1_metric: 0.2223 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7202 - acc: 0.4993 - precision: 0.1204 - recall: 1.2296 - f1_metric: 0.2154 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7126 - acc: 0.5155 - precision: 0.1121 - recall: 1.1713 - f1_metric: 0.1998 - val_loss: 0.9305 - val_acc: 0.0826 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9607 - acc: 0.4815 - precision: 0.1237 - recall: 1.2915 - f1_metric: 0.2219 - val_loss: 0.9305 - val_acc: 0.8947 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1101 - acc: 0.4888 - precision: 0.1241 - recall: 1.2172 - f1_metric: 0.2207 - val_loss: 0.9305 - val_acc: 0.1120 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 29 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.9822 - acc: 0.4806 - precision: 0.1246 - recall: 1.2309 - f1_metric: 0.2215 - val_loss: 0.9305 - val_acc: 0.1928 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6535 - acc: 0.4773 - precision: 0.1099 - recall: 1.2222 - f1_metric: 0.1980 - val_loss: 0.9305 - val_acc: 0.1010 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1996 - acc: 0.5095 - precision: 0.1199 - recall: 1.2488 - f1_metric: 0.2142 - val_loss: 0.9305 - val_acc: 0.1353 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7282 - acc: 0.5016 - precision: 0.1166 - recall: 1.2454 - f1_metric: 0.2088 - val_loss: 0.9305 - val_acc: 0.1236 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0607 - acc: 0.5150 - precision: 0.1210 - recall: 1.2556 - f1_metric: 0.2172 - val_loss: 0.9305 - val_acc: 0.2032 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7759 - acc: 0.5187 - precision: 0.1184 - recall: 1.2373 - f1_metric: 0.2117 - val_loss: 0.9305 - val_acc: 0.1469 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7301 - acc: 0.5183 - precision: 0.1190 - recall: 1.2611 - f1_metric: 0.2137 - val_loss: 0.9305 - val_acc: 0.3072 - val_precision: 0.1322 - val_recall: 1.3040 - val_f1_metric: 0.2326\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.4565 - acc: 0.4915 - precision: 0.1205 - recall: 1.2049 - f1_metric: 0.2149 - val_loss: 0.9305 - val_acc: 0.9027 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7224 - acc: 0.4891 - precision: 0.1197 - recall: 1.2224 - f1_metric: 0.2140 - val_loss: 0.9305 - val_acc: 0.7583 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6967 - acc: 0.5045 - precision: 0.1178 - recall: 1.2089 - f1_metric: 0.2110 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8960 - acc: 0.4619 - precision: 0.1191 - recall: 1.2667 - f1_metric: 0.2135 - val_loss: 0.9305 - val_acc: 0.0747 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8711 - acc: 0.4780 - precision: 0.1198 - recall: 1.2286 - f1_metric: 0.2140 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7512 - acc: 0.4889 - precision: 0.1199 - recall: 1.2477 - f1_metric: 0.2150 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1342 - acc: 0.4854 - precision: 0.1301 - recall: 1.2506 - f1_metric: 0.2310 - val_loss: 0.9305 - val_acc: 0.9009 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0816 - acc: 0.4776 - precision: 0.1122 - recall: 1.2116 - f1_metric: 0.2018 - val_loss: 0.9305 - val_acc: 0.1218 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6618 - acc: 0.4891 - precision: 0.1162 - recall: 1.2263 - f1_metric: 0.2080 - val_loss: 0.9305 - val_acc: 0.1230 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7108 - acc: 0.4847 - precision: 0.1159 - recall: 1.2097 - f1_metric: 0.2079 - val_loss: 0.9305 - val_acc: 0.0887 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1194 - acc: 0.4786 - precision: 0.1178 - recall: 1.2042 - f1_metric: 0.2104 - val_loss: 0.9305 - val_acc: 0.0704 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8450 - acc: 0.4899 - precision: 0.1160 - recall: 1.2390 - f1_metric: 0.2077 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6545 - acc: 0.4982 - precision: 0.1248 - recall: 1.2742 - f1_metric: 0.2226 - val_loss: 0.9305 - val_acc: 0.8531 - val_precision: 0.1314 - val_recall: 1.3012 - val_f1_metric: 0.2311\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7378 - acc: 0.4596 - precision: 0.1224 - recall: 1.2450 - f1_metric: 0.2191 - val_loss: 0.9305 - val_acc: 0.0912 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3019 - acc: 0.4740 - precision: 0.1235 - recall: 1.2652 - f1_metric: 0.2208 - val_loss: 0.9305 - val_acc: 0.1010 - val_precision: 0.1318 - val_recall: 1.3040 - val_f1_metric: 0.2319\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9552 - acc: 0.4809 - precision: 0.1145 - recall: 1.2569 - f1_metric: 0.2067 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6725 - acc: 0.4981 - precision: 0.1189 - recall: 1.2253 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.0845 - val_precision: 0.1311 - val_recall: 1.3001 - val_f1_metric: 0.2307\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9759 - acc: 0.5002 - precision: 0.1184 - recall: 1.2598 - f1_metric: 0.2128 - val_loss: 0.9305 - val_acc: 0.0998 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9881 - acc: 0.4894 - precision: 0.1172 - recall: 1.2672 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.9015 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8229 - acc: 0.4907 - precision: 0.1187 - recall: 1.2636 - f1_metric: 0.2132 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8503 - acc: 0.4761 - precision: 0.1111 - recall: 1.1896 - f1_metric: 0.1998 - val_loss: 0.9305 - val_acc: 0.8990 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0565 - acc: 0.4980 - precision: 0.1125 - recall: 1.2228 - f1_metric: 0.2009 - val_loss: 0.9305 - val_acc: 0.9039 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7101 - acc: 0.5068 - precision: 0.1216 - recall: 1.2180 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3206 - acc: 0.4935 - precision: 0.1133 - recall: 1.2617 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9160 - acc: 0.4843 - precision: 0.1070 - recall: 1.2160 - f1_metric: 0.1934 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8209 - acc: 0.4951 - precision: 0.1189 - recall: 1.2712 - f1_metric: 0.2139 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0591 - acc: 0.4836 - precision: 0.1261 - recall: 1.2595 - f1_metric: 0.2248 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8519 - acc: 0.4694 - precision: 0.1169 - recall: 1.2403 - f1_metric: 0.2093 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5564 - acc: 0.4726 - precision: 0.1167 - recall: 1.2008 - f1_metric: 0.2084 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6674 - acc: 0.4732 - precision: 0.1110 - recall: 1.1655 - f1_metric: 0.1990 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8285 - acc: 0.4792 - precision: 0.1226 - recall: 1.2278 - f1_metric: 0.2188 - val_loss: 0.9305 - val_acc: 0.0710 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9381 - acc: 0.4816 - precision: 0.1266 - recall: 1.2749 - f1_metric: 0.2256 - val_loss: 0.9305 - val_acc: 0.8788 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6791 - acc: 0.5036 - precision: 0.1145 - recall: 1.2549 - f1_metric: 0.2057 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5716 - acc: 0.4595 - precision: 0.1187 - recall: 1.2385 - f1_metric: 0.2120 - val_loss: 0.9305 - val_acc: 0.0967 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9180 - acc: 0.4648 - precision: 0.1243 - recall: 1.2951 - f1_metric: 0.2221 - val_loss: 0.9305 - val_acc: 0.0802 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7618 - acc: 0.4863 - precision: 0.1130 - recall: 1.2748 - f1_metric: 0.2046 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7863 - acc: 0.4934 - precision: 0.1127 - recall: 1.2666 - f1_metric: 0.2021 - val_loss: 0.9305 - val_acc: 0.1016 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6739 - acc: 0.4823 - precision: 0.1188 - recall: 1.2566 - f1_metric: 0.2134 - val_loss: 0.9305 - val_acc: 0.0875 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1870 - acc: 0.4913 - precision: 0.1206 - recall: 1.2471 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.9045 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6140 - acc: 0.5077 - precision: 0.1149 - recall: 1.1940 - f1_metric: 0.2060 - val_loss: 0.9305 - val_acc: 0.9021 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6505 - acc: 0.4712 - precision: 0.1140 - recall: 1.2569 - f1_metric: 0.2051 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9647 - acc: 0.4779 - precision: 0.1161 - recall: 1.2663 - f1_metric: 0.2079 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7494 - acc: 0.4857 - precision: 0.1052 - recall: 1.1579 - f1_metric: 0.1888 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Model 30 Fitting\n",
            "Epoch 1/50\n",
            "205/205 [==============================] - 2s 6ms/step - loss: 0.7190 - acc: 0.4672 - precision: 0.1230 - recall: 1.2496 - f1_metric: 0.2200 - val_loss: 0.9305 - val_acc: 0.0783 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 2/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6565 - acc: 0.4683 - precision: 0.1216 - recall: 1.2128 - f1_metric: 0.2167 - val_loss: 0.9305 - val_acc: 0.0728 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 3/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7904 - acc: 0.4848 - precision: 0.1173 - recall: 1.2635 - f1_metric: 0.2102 - val_loss: 0.9305 - val_acc: 0.2601 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 4/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9548 - acc: 0.4977 - precision: 0.1221 - recall: 1.1925 - f1_metric: 0.2162 - val_loss: 0.9305 - val_acc: 0.0741 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 5/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6172 - acc: 0.4920 - precision: 0.1199 - recall: 1.2010 - f1_metric: 0.2132 - val_loss: 0.9305 - val_acc: 0.0826 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 6/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8541 - acc: 0.4936 - precision: 0.1207 - recall: 1.1438 - f1_metric: 0.2146 - val_loss: 0.9305 - val_acc: 0.0796 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 7/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0155 - acc: 0.4859 - precision: 0.1144 - recall: 1.2142 - f1_metric: 0.2053 - val_loss: 0.9305 - val_acc: 0.1261 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 8/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8489 - acc: 0.4948 - precision: 0.1266 - recall: 1.2537 - f1_metric: 0.2245 - val_loss: 0.9305 - val_acc: 0.1457 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 9/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9576 - acc: 0.5040 - precision: 0.1135 - recall: 1.2570 - f1_metric: 0.2039 - val_loss: 0.9305 - val_acc: 0.4572 - val_precision: 0.1314 - val_recall: 1.2991 - val_f1_metric: 0.2312\n",
            "Epoch 10/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8048 - acc: 0.5095 - precision: 0.1203 - recall: 1.2557 - f1_metric: 0.2145 - val_loss: 0.9305 - val_acc: 0.1034 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 11/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0476 - acc: 0.5053 - precision: 0.1236 - recall: 1.2645 - f1_metric: 0.2210 - val_loss: 0.9305 - val_acc: 0.1805 - val_precision: 0.1319 - val_recall: 1.3040 - val_f1_metric: 0.2321\n",
            "Epoch 12/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6182 - acc: 0.4968 - precision: 0.1281 - recall: 1.2218 - f1_metric: 0.2274 - val_loss: 0.9305 - val_acc: 0.0991 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 13/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7207 - acc: 0.5083 - precision: 0.1206 - recall: 1.2201 - f1_metric: 0.2154 - val_loss: 0.9305 - val_acc: 0.0692 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 14/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8957 - acc: 0.4822 - precision: 0.1228 - recall: 1.2834 - f1_metric: 0.2201 - val_loss: 0.9305 - val_acc: 0.2760 - val_precision: 0.1309 - val_recall: 1.2879 - val_f1_metric: 0.2302\n",
            "Epoch 15/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7423 - acc: 0.5117 - precision: 0.1186 - recall: 1.2462 - f1_metric: 0.2130 - val_loss: 0.9305 - val_acc: 0.8384 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 16/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9197 - acc: 0.5099 - precision: 0.1199 - recall: 1.2424 - f1_metric: 0.2140 - val_loss: 0.9305 - val_acc: 0.0875 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 17/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8556 - acc: 0.5099 - precision: 0.1200 - recall: 1.2336 - f1_metric: 0.2145 - val_loss: 0.9305 - val_acc: 0.8941 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 18/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7197 - acc: 0.5017 - precision: 0.1205 - recall: 1.2126 - f1_metric: 0.2159 - val_loss: 0.9305 - val_acc: 0.1157 - val_precision: 0.1311 - val_recall: 1.2991 - val_f1_metric: 0.2306\n",
            "Epoch 19/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7380 - acc: 0.4846 - precision: 0.1187 - recall: 1.2491 - f1_metric: 0.2132 - val_loss: 0.9305 - val_acc: 0.0679 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 20/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6891 - acc: 0.4782 - precision: 0.1185 - recall: 1.2658 - f1_metric: 0.2133 - val_loss: 0.9305 - val_acc: 0.0894 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 21/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7091 - acc: 0.5084 - precision: 0.1217 - recall: 1.2645 - f1_metric: 0.2173 - val_loss: 0.9305 - val_acc: 0.0783 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 22/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0754 - acc: 0.5043 - precision: 0.1195 - recall: 1.2789 - f1_metric: 0.2148 - val_loss: 0.9305 - val_acc: 0.5171 - val_precision: 0.1329 - val_recall: 1.3040 - val_f1_metric: 0.2336\n",
            "Epoch 23/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7719 - acc: 0.4926 - precision: 0.1175 - recall: 1.2170 - f1_metric: 0.2093 - val_loss: 0.9305 - val_acc: 0.0789 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 24/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8303 - acc: 0.4789 - precision: 0.1172 - recall: 1.2293 - f1_metric: 0.2105 - val_loss: 0.9305 - val_acc: 0.8629 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 25/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.0354 - acc: 0.4920 - precision: 0.1185 - recall: 1.2449 - f1_metric: 0.2126 - val_loss: 0.9305 - val_acc: 0.0673 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 26/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7540 - acc: 0.4801 - precision: 0.1335 - recall: 1.2552 - f1_metric: 0.2364 - val_loss: 0.9305 - val_acc: 0.0741 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 27/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7537 - acc: 0.4771 - precision: 0.1163 - recall: 1.2418 - f1_metric: 0.2084 - val_loss: 0.9305 - val_acc: 0.8800 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 28/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7230 - acc: 0.4897 - precision: 0.1169 - recall: 1.2332 - f1_metric: 0.2098 - val_loss: 0.9305 - val_acc: 0.8274 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 29/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8067 - acc: 0.4966 - precision: 0.1204 - recall: 1.2202 - f1_metric: 0.2155 - val_loss: 0.9305 - val_acc: 0.0826 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 30/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8758 - acc: 0.4717 - precision: 0.1172 - recall: 1.2436 - f1_metric: 0.2103 - val_loss: 0.9305 - val_acc: 0.0820 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2318\n",
            "Epoch 31/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6615 - acc: 0.4947 - precision: 0.1175 - recall: 1.2263 - f1_metric: 0.2094 - val_loss: 0.9305 - val_acc: 0.8531 - val_precision: 0.1317 - val_recall: 1.3040 - val_f1_metric: 0.2317\n",
            "Epoch 32/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.1488 - acc: 0.4896 - precision: 0.1221 - recall: 1.2119 - f1_metric: 0.2179 - val_loss: 0.9305 - val_acc: 0.6867 - val_precision: 0.1324 - val_recall: 1.3040 - val_f1_metric: 0.2329\n",
            "Epoch 33/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.5947 - acc: 0.4848 - precision: 0.1123 - recall: 1.1604 - f1_metric: 0.2017 - val_loss: 0.9305 - val_acc: 0.1322 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 34/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2777 - acc: 0.4864 - precision: 0.1217 - recall: 1.2269 - f1_metric: 0.2173 - val_loss: 0.9305 - val_acc: 0.8880 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 35/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8390 - acc: 0.4644 - precision: 0.1207 - recall: 1.2040 - f1_metric: 0.2150 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 36/50\n",
            "205/205 [==============================] - 1s 5ms/step - loss: 1.0613 - acc: 0.4823 - precision: 0.1146 - recall: 1.1775 - f1_metric: 0.2053 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 37/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9959 - acc: 0.4824 - precision: 0.1223 - recall: 1.2286 - f1_metric: 0.2179 - val_loss: 0.9305 - val_acc: 0.8984 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 38/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6220 - acc: 0.4736 - precision: 0.1144 - recall: 1.2725 - f1_metric: 0.2053 - val_loss: 0.9305 - val_acc: 0.0765 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 39/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.3998 - acc: 0.4857 - precision: 0.1169 - recall: 1.2190 - f1_metric: 0.2096 - val_loss: 0.9305 - val_acc: 0.8880 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 40/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8650 - acc: 0.4909 - precision: 0.1124 - recall: 1.2305 - f1_metric: 0.2021 - val_loss: 0.9305 - val_acc: 0.8953 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 41/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2161 - acc: 0.4995 - precision: 0.1257 - recall: 1.2769 - f1_metric: 0.2239 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 42/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7942 - acc: 0.4758 - precision: 0.1208 - recall: 1.2651 - f1_metric: 0.2168 - val_loss: 0.9305 - val_acc: 0.8886 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 43/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 1.2924 - acc: 0.4963 - precision: 0.1206 - recall: 1.2727 - f1_metric: 0.2151 - val_loss: 0.9305 - val_acc: 0.0667 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 44/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8877 - acc: 0.4888 - precision: 0.1157 - recall: 1.2477 - f1_metric: 0.2077 - val_loss: 0.9305 - val_acc: 0.1689 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 45/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7138 - acc: 0.4944 - precision: 0.1203 - recall: 1.2140 - f1_metric: 0.2144 - val_loss: 0.9305 - val_acc: 0.0838 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 46/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.8518 - acc: 0.4895 - precision: 0.1131 - recall: 1.2315 - f1_metric: 0.2032 - val_loss: 0.9305 - val_acc: 0.0704 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 47/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.7009 - acc: 0.4885 - precision: 0.1154 - recall: 1.2704 - f1_metric: 0.2068 - val_loss: 0.9305 - val_acc: 0.4259 - val_precision: 0.1317 - val_recall: 1.2799 - val_f1_metric: 0.2314\n",
            "Epoch 48/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.9146 - acc: 0.4978 - precision: 0.1152 - recall: 1.2334 - f1_metric: 0.2058 - val_loss: 0.9305 - val_acc: 0.1040 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 49/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6301 - acc: 0.4816 - precision: 0.1100 - recall: 1.1696 - f1_metric: 0.1971 - val_loss: 0.9305 - val_acc: 0.8886 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n",
            "Epoch 50/50\n",
            "205/205 [==============================] - 1s 4ms/step - loss: 0.6421 - acc: 0.5051 - precision: 0.1175 - recall: 1.2325 - f1_metric: 0.2109 - val_loss: 0.9305 - val_acc: 0.0661 - val_precision: 0.1316 - val_recall: 1.3040 - val_f1_metric: 0.2316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GfHp9ww0FWvB",
        "outputId": "7bb10c48-db45-4928-8d2c-bece4da80dee"
      },
      "source": [
        "#Run model predictions\n",
        "\n",
        "# Model 1 \n",
        "prediction_features_1 = model1.predict(features_test)\n",
        "performance1 = model1.evaluate(features_test, labels_test)\n",
        "print(performance1)\n",
        "# Model 2 \n",
        "prediction_features_2 = model2.predict(features_test)\n",
        "performance2 = model2.evaluate(features_test, labels_test)\n",
        "print(performance1)\n",
        "# Model 3 \n",
        "prediction_features_3 = model3.predict(features_test)\n",
        "performance3 = model3.evaluate(features_test, labels_test)\n",
        "print(performance1)\n",
        "# Model 4 \n",
        "prediction_features_4 = model4.predict(features_test)\n",
        "performance4 = model4.evaluate(features_test, labels_test)\n",
        "print(performance4)\n",
        "# Model 5 \n",
        "prediction_features_5 = model5.predict(features_test)\n",
        "performance5 = model5.evaluate(features_test, labels_test)\n",
        "print(performance5)\n",
        "\n",
        "# Model 6 \n",
        "prediction_features_6 = model6.predict(features_test)\n",
        "performance6 = model6.evaluate(features_test, labels_test)\n",
        "print(performance6)\n",
        "# Model 7\n",
        "prediction_features_7 = model7.predict(features_test)\n",
        "performance7 = model7.evaluate(features_test, labels_test)\n",
        "print(performance7)\n",
        "# Model 8 \n",
        "prediction_features_8 = model8.predict(features_test)\n",
        "performance8 = model8.evaluate(features_test, labels_test)\n",
        "print(performance8)\n",
        "# Model 9\n",
        "prediction_features_9 = model9.predict(features_test)\n",
        "performance9 = model9.evaluate(features_test, labels_test)\n",
        "print(performance9)\n",
        "# Model 10\n",
        "prediction_features_10 = model10.predict(features_test)\n",
        "performance10 = model10.evaluate(features_test, labels_test)\n",
        "print(performance10)\n",
        "\n",
        "# Model 11 \n",
        "prediction_features_11 = model11.predict(features_test)\n",
        "performance11 = model11.evaluate(features_test, labels_test)\n",
        "print(performance11)\n",
        "# Model 12 \n",
        "prediction_features_12 = model12.predict(features_test)\n",
        "performance12 = model12.evaluate(features_test, labels_test)\n",
        "print(performance12)\n",
        "# Model 13 \n",
        "prediction_features_13 = model13.predict(features_test)\n",
        "performance13 = model13.evaluate(features_test, labels_test)\n",
        "print(performance13)\n",
        "# Model 14 \n",
        "prediction_features_14 = model14.predict(features_test)\n",
        "performance14 = model14.evaluate(features_test, labels_test)\n",
        "print(performance14)\n",
        "# Model 15 \n",
        "prediction_features_15 = model15.predict(features_test)\n",
        "performance15 = model15.evaluate(features_test, labels_test)\n",
        "print(performance15)\n",
        "\n",
        "# Model 16\n",
        "prediction_features_16 = model16.predict(features_test)\n",
        "performance16 = model16.evaluate(features_test, labels_test)\n",
        "print(performance16)\n",
        "# Model 17\n",
        "prediction_features_17 = model17.predict(features_test)\n",
        "performance17 = model17.evaluate(features_test, labels_test)\n",
        "print(performance17)\n",
        "# Model 18 \n",
        "prediction_features_18 = model18.predict(features_test)\n",
        "performance18 = model18.evaluate(features_test, labels_test)\n",
        "print(performance18)\n",
        "# Model 19\n",
        "prediction_features_19 = model19.predict(features_test)\n",
        "performance19 = model19.evaluate(features_test, labels_test)\n",
        "print(performance19)\n",
        "# Model 20 \n",
        "prediction_features_20 = model20.predict(features_test)\n",
        "performance20 = model20.evaluate(features_test, labels_test)\n",
        "print(performance20)\n",
        "\n",
        "# Model 21\n",
        "prediction_features_21 = model21.predict(features_test)\n",
        "performance21 = model21.evaluate(features_test, labels_test)\n",
        "print(performance21)\n",
        "# Model 22\n",
        "prediction_features_22 = model22.predict(features_test)\n",
        "performance22 = model22.evaluate(features_test, labels_test)\n",
        "print(performance22)\n",
        "# Model 23\n",
        "prediction_features_23 = model23.predict(features_test)\n",
        "performance23 = model23.evaluate(features_test, labels_test)\n",
        "print(performance23)\n",
        "# Model 24 \n",
        "prediction_features_24 = model24.predict(features_test)\n",
        "performance24 = model24.evaluate(features_test, labels_test)\n",
        "print(performance24)\n",
        "# Model 25\n",
        "prediction_features_25 = model25.predict(features_test)\n",
        "performance25 = model25.evaluate(features_test, labels_test)\n",
        "print(performance25)\n",
        "\n",
        "# Model 26 \n",
        "prediction_features_26 = model26.predict(features_test)\n",
        "performance26 = model26.evaluate(features_test, labels_test)\n",
        "print(performance26)\n",
        "# Model 27 \n",
        "prediction_features_27 = model27.predict(features_test)\n",
        "performance27 = model27.evaluate(features_test, labels_test)\n",
        "print(performance27)\n",
        "# Model 28 \n",
        "prediction_features_28 = model28.predict(features_test)\n",
        "performance28 = model28.evaluate(features_test, labels_test)\n",
        "print(performance28)\n",
        "# Model 29\n",
        "prediction_features_29 = model29.predict(features_test)\n",
        "performance29 = model29.evaluate(features_test, labels_test)\n",
        "print(performance29)\n",
        "# Model 30 \n",
        "prediction_features_30 = model30.predict(features_test)\n",
        "performance30 = model30.evaluate(features_test, labels_test)\n",
        "print(performance30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.8982 - precision: 0.1153 - recall: 1.2522 - f1_metric: 0.2073\n",
            "[0.7365394234657288, 0.8981889486312866, 0.11529270559549332, 1.252213716506958, 0.20732875168323517]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0622 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.8981889486312866, 0.11529270559549332, 1.252213716506958, 0.20732875168323517]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.1043 - precision: 0.1160 - recall: 1.2522 - f1_metric: 0.2084\n",
            "[0.7365394234657288, 0.8981889486312866, 0.11529270559549332, 1.252213716506958, 0.20732875168323517]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0627 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06265296041965485, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9109 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.9109153151512146, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9041 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.9040626287460327, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0622 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06216348335146904, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.8776 - precision: 0.1153 - recall: 1.2522 - f1_metric: 0.2074\n",
            "[0.7365394234657288, 0.8776309490203857, 0.11532420665025711, 1.252213716506958, 0.20739798247814178]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0622 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06216348335146904, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.4244 - precision: 0.1160 - recall: 1.2522 - f1_metric: 0.2084\n",
            "[0.7365394234657288, 0.4243759214878082, 0.11598574370145798, 1.252213716506958, 0.2084304839372635]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9011 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.9011257886886597, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9114 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.9114047884941101, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9099 - precision: 0.1153 - recall: 1.2522 - f1_metric: 0.2073\n",
            "[0.7365394234657288, 0.9099363684654236, 0.11529270559549332, 1.252213716506958, 0.20732875168323517]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.1811 - precision: 0.1153 - recall: 1.2522 - f1_metric: 0.2073\n",
            "[0.7365394234657288, 0.18110620975494385, 0.11526120454072952, 1.252213716506958, 0.20728500187397003]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9109 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.9109153151512146, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9109 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.9109153151512146, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0622 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06216348335146904, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.8737 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.8737151026725769, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0627 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06265296041965485, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9080 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.9079784750938416, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0627 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06265296041965485, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.7430 - precision: 0.1163 - recall: 1.2522 - f1_metric: 0.2089\n",
            "[0.7365394234657288, 0.7430249452590942, 0.11629131436347961, 1.252213716506958, 0.20893113315105438]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0631 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06314244121313095, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0627 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06265296041965485, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.9114 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.9114047884941101, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0627 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06265296041965485, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0622 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06216348335146904, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.1067 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.10670582205057144, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0627 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06265296041965485, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n",
            "64/64 [==============================] - 0s 2ms/step - loss: 0.7365 - acc: 0.0627 - precision: 0.1152 - recall: 1.2522 - f1_metric: 0.2072\n",
            "[0.7365394234657288, 0.06265296041965485, 0.11519820243120193, 1.252213716506958, 0.2071799486875534]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-kn2xq4Ts7s-",
        "outputId": "3ce7123d-0ec2-4008-bd0c-62786889861d"
      },
      "source": [
        "# Averages\n",
        "\n",
        "# Loss\n",
        "loss_avg = (performance1[0] + performance2[0] + performance3[0] + performance4[0] + performance5[0] + performance6[0] + performance7[0] + performance8[0] + performance9[0] + performance10[0]\n",
        "            + performance11[0] + performance12[0] + performance13[0] + performance14[0] + performance15[0] + performance16[0] + performance17[0] + performance18[0] + performance19[0] + performance20[0]\n",
        "            + performance21[0] + performance22[0] + performance23[0] + performance24[0] + performance25[0] + performance26[0] + performance27[0] + performance28[0] + performance29[0] + performance30[0])/30\n",
        "print(\"Loss Average: \", loss_avg)\n",
        "\n",
        "# Accuracy\n",
        "acc_avg =(performance1[1] + performance2[1] + performance3[1] + performance4[1] + performance5[1] + performance6[1] + performance7[1] + performance8[1] + performance9[1] + performance10[1]\n",
        "            + performance11[1] + performance12[1] + performance13[1] + performance14[1] + performance15[1] + performance16[1] + performance17[1] + performance18[1] + performance19[1] + performance20[1]\n",
        "            + performance21[1] + performance22[1] + performance23[1] + performance24[1] + performance25[1] + performance26[1] + performance27[1] + performance28[1] + performance29[1] + performance30[1])/30\n",
        "print(\"Accuraccy Average: \", acc_avg)\n",
        "\n",
        "# Precision\n",
        "precision_avg = (performance1[2] + performance2[2] + performance3[2] + performance4[2] + performance5[2] + performance6[2] + performance7[2] + performance8[2] + performance9[2] + performance10[2]\n",
        "            + performance11[2] + performance12[2] + performance13[2] + performance14[2] + performance15[2] + performance16[2] + performance17[2] + performance18[2] + performance19[2] + performance20[2]\n",
        "            + performance21[2] + performance22[2] + performance23[2] + performance24[2] + performance25[2] + performance26[2] + performance27[2] + performance28[2] + performance29[2] + performance30[2])/30\n",
        "print(\"Precision Average: \", precision_avg)\n",
        "\n",
        "# Recall\n",
        "recall_avg = (performance1[3] + performance2[3] + performance3[3] + performance4[3] + performance5[3] + performance6[3] + performance7[3] + performance8[3] + performance9[3] + performance10[3]\n",
        "            + performance11[3] + performance12[3] + performance13[3] + performance14[3] + performance15[3] + performance16[3] + performance17[3] + performance18[3] + performance19[3] + performance20[3]\n",
        "            + performance21[3] + performance22[3] + performance23[3] + performance24[3] + performance25[3] + performance26[3] + performance27[3] + performance28[3] + performance29[3] + performance30[3])/30\n",
        "print(\"Recall Average: \", recall_avg)\n",
        "\n",
        "# f1_metric\n",
        "f1_avg = (performance1[4] + performance2[4] + performance3[4] + performance4[4] + performance5[4] + performance6[4] + performance7[4] + performance8[4] + performance9[4] + performance10[4]\n",
        "            + performance11[4] + performance12[4] + performance13[4] + performance14[4] + performance15[4] + performance16[4] + performance17[4] + performance18[4] + performance19[4] + performance20[4]\n",
        "            + performance21[4] + performance22[4] + performance23[4] + performance24[4] + performance25[4] + performance26[4] + performance27[4] + performance28[4] + performance29[4] + performance30[4])/30\n",
        "print(\"F1 Average: \", f1_avg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss Average:  0.7365394234657288\n",
            "Accuraccy Average:  0.4400065234551827\n",
            "Precision Average:  0.11529946277538936\n",
            "Recall Average:  1.252213716506958\n",
            "F1 Average:  0.20734166006247204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XuwAUg5_KOz4",
        "outputId": "07f27aed-ed08-4c5e-a42f-ea011890f267"
      },
      "source": [
        "#Take the standard deviation of the model samples\n",
        "\n",
        "#Loss SE\n",
        "Loss_SE = statistics.stdev([performance1[0], performance2[0], performance3[0], performance4[0], performance5[0],\n",
        "                  performance6[0], performance7[0], performance8[0], performance9[0], performance10[0],\n",
        "                  performance11[0], performance12[0], performance13[0], performance14[0], performance15[0],\n",
        "                  performance16[0], performance17[0], performance18[0], performance19[0], performance20[0], \n",
        "                  performance21[0], performance22[0], performance23[0], performance24[0], performance25[0],\n",
        "                  performance26[0], performance27[0], performance28[0], performance29[0], performance30[0]])\n",
        "print(\"Loss SE:\", Loss_SE)\n",
        "\n",
        "#Accuracy SE\n",
        "Acc_SE = statistics.stdev([performance1[1], performance2[1], performance3[1], performance4[1], performance5[1],\n",
        "                  performance6[1], performance7[1], performance8[1], performance9[1], performance10[1],\n",
        "                  performance11[1], performance12[1], performance13[1], performance14[1], performance15[1],\n",
        "                  performance16[1], performance17[1], performance18[1], performance19[1], performance20[1], \n",
        "                  performance21[1], performance22[1], performance23[1], performance24[1], performance25[1],\n",
        "                  performance26[1], performance27[1], performance28[1], performance29[1], performance30[1]])\n",
        "print(\"Accuraccy SE: \", Acc_SE)\n",
        "\n",
        "#Precision SE\n",
        "precision_SE = statistics.stdev([performance1[2], performance2[2], performance3[2], performance4[2], performance5[2],\n",
        "                  performance6[2], performance7[2], performance8[2], performance9[2], performance10[2],\n",
        "                  performance11[2], performance12[2], performance13[2], performance14[2], performance15[2],\n",
        "                  performance16[2], performance17[2], performance18[2], performance19[2], performance20[2], \n",
        "                  performance21[2], performance22[2], performance23[2], performance24[2], performance25[2],\n",
        "                  performance26[2], performance27[2], performance28[2], performance29[2], performance30[2]])\n",
        "print(\"Precision SE: \", precision_SE)\n",
        "\n",
        "#Recall \n",
        "Recall_SE = statistics.stdev([performance1[3], performance2[3], performance3[3], performance4[3], performance5[3],\n",
        "                  performance6[3], performance7[3], performance8[3], performance9[3], performance10[3],\n",
        "                  performance11[3], performance12[3], performance13[3], performance14[3], performance15[3],\n",
        "                  performance16[3], performance17[3], performance18[3], performance19[3], performance20[3], \n",
        "                  performance21[3], performance22[3], performance23[3], performance24[3], performance25[3],\n",
        "                  performance26[3], performance27[3], performance28[3], performance29[3], performance30[3]])\n",
        "print(\"Recall SE: \", Recall_SE)\n",
        "\n",
        "#F1 Score\n",
        "F1_Score_SE = statistics.stdev([performance1[4], performance2[4], performance3[4], performance4[4], performance5[4],\n",
        "                  performance6[4], performance7[4], performance8[4], performance9[4], performance10[4],\n",
        "                  performance11[4], performance12[4], performance13[4], performance14[4], performance15[4],\n",
        "                  performance16[4], performance17[4], performance18[4], performance19[4], performance20[4], \n",
        "                  performance21[4], performance22[4], performance23[4], performance24[4], performance25[4],\n",
        "                  performance26[4], performance27[4], performance28[4], performance29[4], performance30[4]])\n",
        "print(\"F1_Score_SE: \", F1_Score_SE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss SE: 0.0\n",
            "Accuraccy SE:  0.406947912130386\n",
            "Precision SE:  0.00027242710848082203\n",
            "Recall SE:  0.0\n",
            "F1_Score_SE:  0.00043387163848523924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aMpLKJ4OKt-X",
        "outputId": "9c350a4d-3aa3-4d56-f6b0-6fb4246a9ca9"
      },
      "source": [
        "#Tensorflow Graphics\n",
        "\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}